ccopy_reg
_reconstructor
p0
(cdigits.model.images.classification.job
ImageClassificationModelJob
p1
c__builtin__
object
p2
Ntp3
Rp4
(dp5
S'username'
p6
Vazh2
p7
sS'_notes'
p8
NsS'tasks'
p9
(lp10
g0
(cdigits.model.tasks.caffe_train
CaffeTrainTask
p11
g2
Ntp12
Rp13
(dp14
S'shuffle'
p15
I01
sS'snapshot_interval'
p16
F1.0
sS'caffe_version'
p17
S'0.16.4'
p18
sS'train_outputs'
p19
ccollections
OrderedDict
p20
((lp21
(lp22
S'epoch'
p23
ag0
(cdigits.model.tasks.train
NetworkOutput
p24
c__builtin__
tuple
p25
(S'Epoch'
p26
(lp27
I0
aF0.00019062142584826535
aF0.0003812428516965307
aF0.015059092642012962
aF0.030118185284025923
aF0.045177277926038886
aF0.060236370568051846
aF0.0752954632100648
aF0.09035455585207777
aF0.10541364849409074
aF0.12047274113610369
aF0.13553183377811667
aF0.1505909264201296
aF0.16565001906214258
aF0.18070911170415555
aF0.1957682043461685
aF0.21082729698818148
aF0.22588638963019445
aF0.24094548227220738
aF0.25600457491422035
aF0.27106366755623335
aF0.2861227601982463
aF0.3011818528402592
aF0.3162409454822722
aF0.33130003812428516
aF0.34635913076629815
aF0.3614182234083111
aF0.37647731605032403
aF0.391536408692337
aF0.40659550133434996
aF0.42165459397636296
aF0.4367136866183759
aF0.4517727792603889
aF0.46683187190240183
aF0.48189096454441477
aF0.49695005718642776
aF0.5120091498284407
aF0.5270682424704537
aF0.5421273351124667
aF0.5571864277544796
aF0.5722455203964926
aF0.5873046130385056
aF0.6023637056805184
aF0.6174227983225314
aF0.6324818909645444
aF0.6475409836065574
aF0.6626000762485703
aF0.6776591688905833
aF0.6927182615325963
aF0.7077773541746092
aF0.7228364468166222
aF0.7378955394586352
aF0.7529546321006481
aF0.768013724742661
aF0.783072817384674
aF0.798131910026687
aF0.8131910026686999
aF0.8282500953107129
aF0.8433091879527259
aF0.8583682805947388
aF0.8734273732367518
aF0.8884864658787648
aF0.9035455585207778
aF0.9186046511627907
aF0.9336637438048037
aF0.9487228364468167
aF0.9637819290888295
aF0.9788410217308425
aF0.9939001143728555
aF1.0089592070148685
aF1.0240182996568814
aF1.0390773922988943
aF1.0541364849409074
aF1.0691955775829203
aF1.0842546702249334
aF1.0993137628669463
aF1.1143728555089591
aF1.1294319481509723
aF1.1444910407929851
aF1.159550133434998
aF1.1746092260770111
aF1.189668318719024
aF1.204727411361037
aF1.21978650400305
aF1.2348455966450629
aF1.2499046892870758
aF1.2649637819290889
aF1.2800228745711018
aF1.2950819672131149
aF1.3101410598551277
aF1.3252001524971406
aF1.3402592451391537
aF1.3553183377811666
aF1.3703774304231795
aF1.3854365230651926
aF1.4004956157072055
aF1.4155547083492184
aF1.4306138009912315
aF1.4456728936332444
aF1.4607319862752572
aF1.4757910789172703
aF1.4908501715592832
aF1.5059092642012961
aF1.5209683568433092
aF1.536027449485322
aF1.5510865421273352
aF1.566145634769348
aF1.581204727411361
aF1.596263820053374
aF1.611322912695387
aF1.6263820053373998
aF1.641441097979413
aF1.6565001906214258
aF1.6715592832634387
aF1.6866183759054518
aF1.7016774685474647
aF1.7167365611894776
aF1.7317956538314907
aF1.7468547464735036
aF1.7619138391155167
aF1.7769729317575296
aF1.7920320243995425
aF1.8070911170415556
aF1.8221502096835684
aF1.8372093023255813
aF1.8522683949675944
aF1.8673274876096073
aF1.8823865802516202
aF1.8974456728936333
aF1.9125047655356462
aF1.927563858177659
aF1.9426229508196722
aF1.957682043461685
aF1.972741136103698
aF1.987800228745711
aF2.002859321387724
aF2.017918414029737
aF2.0329775066717497
aF2.048036599313763
aF2.063095691955776
aF2.0781547845977886
aF2.0932138772398017
aF2.108272969881815
aF2.123332062523828
aF2.1383911551658406
aF2.1534502478078537
aF2.1685093404498668
aF2.1835684330918794
aF2.1986275257338925
aF2.2136866183759056
aF2.2287457110179183
aF2.2438048036599314
aF2.2588638963019445
aF2.273922988943957
aF2.2889820815859703
aF2.3040411742279834
aF2.319100266869996
aF2.334159359512009
aF2.3492184521540223
aF2.364277544796035
aF2.379336637438048
aF2.394395730080061
aF2.409454822722074
aF2.424513915364087
aF2.4395730080061
aF2.4546321006481127
aF2.4696911932901258
aF2.484750285932139
aF2.4998093785741515
aF2.5148684712161646
aF2.5299275638581777
aF2.5449866565001904
aF2.5600457491422035
aF2.5751048417842166
aF2.5901639344262297
aF2.6052230270682424
aF2.6202821197102555
aF2.6353412123522686
aF2.6504003049942813
aF2.6654593976362944
aF2.6805184902783075
aF2.69557758292032
aF2.7106366755623332
aF2.7256957682043463
aF2.740754860846359
aF2.755813953488372
aF2.770873046130385
aF2.785932138772398
aF2.800991231414411
aF2.816050324056424
aF2.8311094166984367
aF2.84616850934045
aF2.861227601982463
aF2.8762866946244756
aF2.8913457872664887
aF2.906404879908502
aF2.9214639725505145
aF2.9365230651925276
aF2.9515821578345407
aF2.9666412504765534
aF2.9817003431185665
aF2.9967594357605796
aF3.0118185284025922
aF3.0268776210446053
aF3.0419367136866184
aF3.0569958063286315
aF3.072054898970644
aF3.0871139916126573
aF3.1021730842546704
aF3.117232176896683
aF3.132291269538696
aF3.1473503621807093
aF3.162409454822722
aF3.177468547464735
aF3.192527640106748
aF3.207586732748761
aF3.222645825390774
aF3.237704918032787
aF3.2527640106747997
aF3.267823103316813
aF3.282882195958826
aF3.2979412886008386
aF3.3130003812428517
aF3.328059473884865
aF3.3431185665268774
aF3.3581776591688905
aF3.3732367518109037
aF3.3882958444529163
aF3.4033549370949294
aF3.4184140297369425
aF3.433473122378955
aF3.4485322150209683
aF3.4635913076629814
aF3.478650400304994
aF3.493709492947007
aF3.5087685855890203
aF3.5238276782310334
aF3.538886770873046
aF3.553945863515059
aF3.5690049561570723
aF3.584064048799085
aF3.599123141441098
aF3.614182234083111
aF3.629241326725124
aF3.644300419367137
aF3.65935951200915
aF3.6744186046511627
aF3.6894776972931758
aF3.704536789935189
aF3.7195958825772015
aF3.7346549752192146
aF3.7497140678612277
aF3.7647731605032404
aF3.7798322531452535
aF3.7948913457872666
aF3.8099504384292793
aF3.8250095310712924
aF3.8400686237133055
aF3.855127716355318
aF3.8701868089973313
aF3.8852459016393444
aF3.900304994281357
aF3.91536408692337
aF3.9304231795653832
aF3.945482272207396
aF3.960541364849409
aF3.975600457491422
aF3.990659550133435
aF4.005718642775448
aF4.020777735417461
aF4.035836828059474
aF4.050895920701487
aF4.065955013343499
aF4.0810141059855125
aF4.096073198627526
aF4.111132291269539
aF4.126191383911552
aF4.141250476553565
aF4.156309569195577
aF4.17136866183759
aF4.186427754479603
aF4.2014868471216165
aF4.21654593976363
aF4.231605032405643
aF4.246664125047656
aF4.261723217689668
aF4.276782310331681
aF4.291841402973694
aF4.306900495615707
aF4.32195958825772
aF4.3370186808997335
aF4.352077773541746
aF4.367136866183759
aF4.382195958825772
aF4.397255051467785
aF4.412314144109798
aF4.427373236751811
aF4.4424323293938235
aF4.457491422035837
aF4.47255051467785
aF4.487609607319863
aF4.502668699961876
aF4.517727792603889
aF4.532786885245901
aF4.547845977887914
aF4.562905070529927
aF4.5779641631719405
aF4.593023255813954
aF4.608082348455967
aF4.623141441097979
aF4.638200533739992
aF4.653259626382005
aF4.668318719024018
aF4.683377811666031
aF4.6984369043080445
aF4.713495996950058
aF4.72855508959207
aF4.743614182234083
aF4.758673274876096
aF4.773732367518109
aF4.788791460160122
aF4.803850552802135
aF4.818909645444148
aF4.833968738086161
aF4.849027830728174
aF4.864086923370187
aF4.8791460160122
aF4.894205108654213
aF4.909264201296225
aF4.924323293938238
aF4.9393823865802515
aF4.954441479222265
aF4.969500571864278
aF4.984559664506291
aF4.999618757148303
aF5.014677849790316
aF5.029736942432329
aF5.044796035074342
aF5.0598551277163555
aF5.074914220358369
aF5.089973313000381
aF5.105032405642394
aF5.120091498284407
aF5.13515059092642
aF5.150209683568433
aF5.165268776210446
aF5.180327868852459
aF5.195386961494472
aF5.210446054136485
aF5.225505146778498
aF5.240564239420511
aF5.255623332062524
aF5.270682424704537
aF5.285741517346549
aF5.3008006099885625
aF5.315859702630576
aF5.330918795272589
aF5.345977887914602
aF5.361036980556615
aF5.376096073198627
aF5.39115516584064
aF5.406214258482653
aF5.4212733511246665
aF5.43633244376668
aF5.451391536408693
aF5.466450629050705
aF5.481509721692718
aF5.496568814334731
aF5.511627906976744
aF5.526686999618757
aF5.54174609226077
aF5.556805184902783
aF5.571864277544796
aF5.586923370186809
aF5.601982462828822
aF5.617041555470835
aF5.632100648112848
aF5.647159740754861
aF5.6622188333968735
aF5.677277926038887
aF5.6923370186809
aF5.707396111322913
aF5.722455203964926
aF5.737514296606939
aF5.752573389248951
aF5.767632481890964
aF5.782691574532977
aF5.7977506671749905
aF5.812809759817004
aF5.827868852459017
aF5.842927945101029
aF5.857987037743042
aF5.873046130385055
aF5.888105223027068
aF5.903164315669081
aF5.9182234083110945
aF5.933282500953107
aF5.94834159359512
aF5.963400686237133
aF5.978459778879146
aF5.993518871521159
aF6.008577964163172
aF6.0236370568051845
aF6.038696149447198
aF6.053755242089211
aF6.068814334731224
aF6.083873427373237
aF6.09893252001525
aF6.113991612657263
aF6.129050705299275
aF6.144109797941288
aF6.1591688905833015
aF6.174227983225315
aF6.189287075867328
aF6.204346168509341
aF6.219405261151353
aF6.234464353793366
aF6.249523446435379
aF6.264582539077392
aF6.2796416317194055
aF6.294700724361419
aF6.309759817003431
aF6.324818909645444
aF6.339878002287457
aF6.35493709492947
aF6.369996187571483
aF6.385055280213496
aF6.4001143728555085
aF6.415173465497522
aF6.430232558139535
aF6.445291650781548
aF6.460350743423561
aF6.475409836065574
aF6.490468928707586
aF6.505528021349599
aF6.5205871139916125
aF6.535646206633626
aF6.550705299275639
aF6.565764391917652
aF6.580823484559665
aF6.595882577201677
aF6.61094166984369
aF6.626000762485703
aF6.6410598551277165
aF6.65611894776973
aF6.671178040411743
aF6.686237133053755
aF6.701296225695768
aF6.716355318337781
aF6.731414410979794
aF6.746473503621807
aF6.76153259626382
aF6.776591688905833
aF6.791650781547846
aF6.806709874189859
aF6.821768966831872
aF6.836828059473885
aF6.851887152115898
aF6.86694624475791
aF6.8820053373999235
aF6.897064430041937
aF6.91212352268395
aF6.927182615325963
aF6.942241707967976
aF6.957300800609988
aF6.972359893252001
aF6.987418985894014
aF7.002478078536027
aF7.0175371711780405
aF7.032596263820054
aF7.047655356462067
aF7.062714449104079
aF7.077773541746092
aF7.092832634388105
aF7.107891727030118
aF7.122950819672131
aF7.1380099123141445
aF7.153069004956157
aF7.16812809759817
aF7.183187190240183
aF7.198246282882196
aF7.213305375524209
aF7.228364468166222
aF7.2434235608082345
aF7.258482653450248
aF7.273541746092261
aF7.288600838734274
aF7.303659931376287
aF7.3187190240183
aF7.333778116660312
aF7.348837209302325
aF7.363896301944338
aF7.3789553945863515
aF7.394014487228365
aF7.409073579870378
aF7.42413267251239
aF7.439191765154403
aF7.454250857796416
aF7.469309950438429
aF7.484369043080442
aF7.4994281357224555
aF7.514487228364469
aF7.529546321006481
aF7.544605413648494
aF7.559664506290507
aF7.57472359893252
aF7.589782691574533
aF7.604841784216546
aF7.6199008768585585
aF7.634959969500572
aF7.650019062142585
aF7.665078154784598
aF7.680137247426611
aF7.695196340068624
aF7.710255432710636
aF7.725314525352649
aF7.7403736179946625
aF7.755432710636676
aF7.770491803278689
aF7.785550895920702
aF7.800609988562714
aF7.815669081204727
aF7.83072817384674
aF7.845787266488753
aF7.8608463591307665
aF7.87590545177278
aF7.890964544414792
aF7.906023637056805
aF7.921082729698818
aF7.936141822340831
aF7.951200914982844
aF7.966260007624857
aF7.98131910026687
aF7.996378192908883
aF8.011437285550896
aF8.02649637819291
aF8.041555470834922
aF8.056614563476934
aF8.071673656118948
aF8.08673274876096
aF8.101791841402974
aF8.116850934044987
aF8.131910026686999
aF8.146969119329013
aF8.162028211971025
aF8.177087304613039
aF8.192146397255051
aF8.207205489897065
aF8.222264582539077
aF8.23732367518109
aF8.252382767823104
aF8.267441860465116
aF8.28250095310713
aF8.297560045749142
aF8.312619138391154
aF8.327678231033168
aF8.34273732367518
aF8.357796416317194
aF8.372855508959207
aF8.38791460160122
aF8.402973694243233
aF8.418032786885245
aF8.43309187952726
aF8.448150972169271
aF8.463210064811285
aF8.478269157453298
aF8.493328250095312
aF8.508387342737324
aF8.523446435379336
aF8.53850552802135
aF8.553564620663362
aF8.568623713305376
aF8.583682805947388
aF8.5987418985894
aF8.613800991231415
aF8.628860083873427
aF8.64391917651544
aF8.658978269157453
aF8.674037361799467
aF8.68909645444148
aF8.704155547083491
aF8.719214639725505
aF8.734273732367518
aF8.749332825009532
aF8.764391917651544
aF8.779451010293556
aF8.79451010293557
aF8.809569195577582
aF8.824628288219596
aF8.839687380861609
aF8.854746473503623
aF8.869805566145635
aF8.884864658787647
aF8.899923751429661
aF8.914982844071673
aF8.930041936713687
aF8.9451010293557
aF8.960160121997713
aF8.975219214639726
aF8.990278307281738
aF9.005337399923752
aF9.020396492565764
aF9.035455585207778
aF9.05051467784979
aF9.065573770491802
aF9.080632863133816
aF9.095691955775829
aF9.110751048417843
aF9.125810141059855
aF9.140869233701869
aF9.155928326343881
aF9.170987418985893
aF9.186046511627907
aF9.20110560426992
aF9.216164696911934
aF9.231223789553946
aF9.246282882195958
aF9.261341974837972
aF9.276401067479984
aF9.291460160121998
aF9.30651925276401
aF9.321578345406024
aF9.336637438048037
aF9.351696530690049
aF9.366755623332063
aF9.381814715974075
aF9.396873808616089
aF9.411932901258101
aF9.426991993900115
aF9.442051086542127
aF9.45711017918414
aF9.472169271826154
aF9.487228364468166
aF9.50228745711018
aF9.517346549752192
aF9.532405642394204
aF9.547464735036218
aF9.56252382767823
aF9.577582920320244
aF9.592642012962257
aF9.60770110560427
aF9.622760198246283
aF9.637819290888295
aF9.65287838353031
aF9.667937476172321
aF9.682996568814335
aF9.698055661456348
aF9.71311475409836
aF9.728173846740374
aF9.743232939382386
aF9.7582920320244
aF9.773351124666412
aF9.788410217308426
aF9.803469309950438
aF9.81852840259245
aF9.833587495234465
aF9.848646587876477
aF9.86370568051849
aF9.878764773160503
aF9.893823865802517
aF9.90888295844453
aF9.923942051086541
aF9.939001143728555
aF9.954060236370568
aF9.969119329012582
aF9.984178421654594
aF9.999237514296606
aF10.01429660693862
aF10.029355699580632
aF10.044414792222646
aF10.059473884864659
aF10.074532977506673
aF10.089592070148685
aF10.104651162790697
aF10.119710255432711
aF10.134769348074723
aF10.149828440716737
aF10.16488753335875
aF10.179946626000762
aF10.195005718642776
aF10.210064811284788
aF10.225123903926802
aF10.240182996568814
aF10.255242089210828
aF10.27030118185284
aF10.285360274494852
aF10.300419367136866
aF10.315478459778879
aF10.330537552420893
aF10.345596645062905
aF10.360655737704919
aF10.375714830346931
aF10.390773922988943
aF10.405833015630957
aF10.42089210827297
aF10.435951200914984
aF10.451010293556996
aF10.466069386199008
aF10.481128478841022
aF10.496187571483034
aF10.511246664125048
aF10.52630575676706
aF10.541364849409074
aF10.556423942051087
aF10.571483034693099
aF10.586542127335113
aF10.601601219977125
aF10.616660312619139
aF10.631719405261151
aF10.646778497903163
aF10.661837590545177
aF10.67689668318719
aF10.691955775829204
aF10.707014868471216
aF10.72207396111323
aF10.737133053755242
aF10.752192146397254
aF10.767251239039268
aF10.78231033168128
aF10.797369424323294
aF10.812428516965307
aF10.82748760960732
aF10.842546702249333
aF10.857605794891345
aF10.87266488753336
aF10.887723980175371
aF10.902783072817385
aF10.917842165459398
aF10.93290125810141
aF10.947960350743424
aF10.963019443385436
aF10.97807853602745
aF10.993137628669462
aF11.008196721311476
aF11.023255813953488
aF11.0383149065955
aF11.053373999237515
aF11.068433091879527
aF11.08349218452154
aF11.098551277163553
aF11.113610369805565
aF11.12866946244758
aF11.143728555089591
aF11.158787647731605
aF11.173846740373618
aF11.188905833015632
aF11.203964925657644
aF11.219024018299656
aF11.23408311094167
aF11.249142203583682
aF11.264201296225696
aF11.279260388867709
aF11.294319481509723
aF11.309378574151735
aF11.324437666793747
aF11.339496759435761
aF11.354555852077773
aF11.369614944719787
aF11.3846740373618
aF11.399733130003812
aF11.414792222645826
aF11.429851315287838
aF11.444910407929852
aF11.459969500571864
aF11.475028593213878
aF11.49008768585589
aF11.505146778497902
aF11.520205871139916
aF11.535264963781929
aF11.550324056423943
aF11.565383149065955
aF11.580442241707967
aF11.595501334349981
aF11.610560426991993
aF11.625619519634007
aF11.64067861227602
aF11.655737704918034
aF11.670796797560046
aF11.685855890202058
aF11.700914982844072
aF11.715974075486084
aF11.731033168128098
aF11.74609226077011
aF11.761151353412124
aF11.776210446054137
aF11.791269538696149
aF11.806328631338163
aF11.821387723980175
aF11.836446816622189
aF11.851505909264201
aF11.866565001906213
aF11.881624094548227
aF11.89668318719024
aF11.911742279832254
aF11.926801372474266
aF11.94186046511628
aF11.956919557758292
aF11.971978650400304
aF11.987037743042318
aF12.00209683568433
aF12.017155928326344
aF12.032215020968357
aF12.047274113610369
aF12.062333206252383
aF12.077392298894395
aF12.09245139153641
aF12.107510484178421
aF12.122569576820435
aF12.137628669462448
aF12.15268776210446
aF12.167746854746474
aF12.182805947388486
aF12.1978650400305
aF12.212924132672512
aF12.227983225314526
aF12.243042317956538
aF12.25810141059855
aF12.273160503240565
aF12.288219595882577
aF12.30327868852459
aF12.318337781166603
aF12.333396873808615
aF12.34845596645063
aF12.363515059092641
aF12.378574151734655
aF12.393633244376668
aF12.408692337018682
aF12.423751429660694
aF12.438810522302706
aF12.45386961494472
aF12.468928707586732
aF12.483987800228746
aF12.499046892870759
aF12.51410598551277
aF12.529165078154785
aF12.544224170796797
aF12.559283263438811
aF12.574342356080823
aF12.589401448722837
aF12.60446054136485
aF12.619519634006862
aF12.634578726648876
aF12.649637819290888
aF12.664696911932902
aF12.679756004574914
aF12.694815097216928
aF12.70987418985894
aF12.724933282500952
aF12.739992375142966
aF12.755051467784979
aF12.770110560426993
aF12.785169653069005
aF12.800228745711017
aF12.815287838353031
aF12.830346930995043
aF12.845406023637057
aF12.86046511627907
aF12.875524208921084
aF12.890583301563096
aF12.905642394205108
aF12.920701486847122
aF12.935760579489134
aF12.950819672131148
aF12.96587876477316
aF12.980937857415173
aF12.995996950057187
aF13.011056042699199
aF13.026115135341213
aF13.041174227983225
aF13.056233320625239
aF13.071292413267251
aF13.086351505909263
aF13.101410598551277
aF13.11646969119329
aF13.131528783835304
aF13.146587876477316
aF13.16164696911933
aF13.176706061761342
aF13.191765154403354
aF13.206824247045368
aF13.22188333968738
aF13.236942432329394
aF13.252001524971407
aF13.267060617613419
aF13.282119710255433
aF13.297178802897445
aF13.31223789553946
aF13.327296988181471
aF13.342356080823485
aF13.357415173465498
aF13.37247426610751
aF13.387533358749524
aF13.402592451391536
aF13.41765154403355
aF13.432710636675562
aF13.447769729317574
aF13.462828821959588
aF13.4778879146016
aF13.492947007243615
aF13.508006099885627
aF13.52306519252764
aF13.538124285169653
aF13.553183377811665
aF13.56824247045368
aF13.583301563095691
aF13.598360655737705
aF13.613419748379718
aF13.628478841021732
aF13.643537933663744
aF13.658597026305756
aF13.67365611894777
aF13.688715211589782
aF13.703774304231796
aF13.718833396873809
aF13.73389248951582
aF13.748951582157835
aF13.764010674799847
aF13.779069767441861
aF13.794128860083873
aF13.809187952725887
aF13.8242470453679
aF13.839306138009912
aF13.854365230651926
aF13.869424323293938
aF13.884483415935952
aF13.899542508577964
aF13.914601601219976
aF13.92966069386199
aF13.944719786504002
aF13.959778879146016
aF13.974837971788029
aF13.989897064430043
aF14.004956157072055
aF14.020015249714067
aF14.035074342356081
aF14.050133434998093
aF14.065192527640107
aF14.08025162028212
aF14.095310712924134
aF14.110369805566146
aF14.125428898208158
aF14.140487990850172
aF14.155547083492184
aF14.170606176134198
aF14.18566526877621
aF14.200724361418223
aF14.215783454060237
aF14.230842546702249
aF14.245901639344263
aF14.260960731986275
aF14.276019824628289
aF14.291078917270301
aF14.306138009912313
aF14.321197102554327
aF14.33625619519634
aF14.351315287838354
aF14.366374380480366
aF14.381433473122378
aF14.396492565764392
aF14.411551658406404
aF14.426610751048418
aF14.44166984369043
aF14.456728936332444
aF14.471788028974457
aF14.486847121616469
aF14.501906214258483
aF14.516965306900495
aF14.53202439954251
aF14.547083492184521
aF14.562142584826535
aF14.577201677468548
aF14.59226077011056
aF14.607319862752574
aF14.622378955394586
aF14.6374380480366
aF14.652497140678612
aF14.667556233320624
aF14.682615325962638
aF14.69767441860465
aF14.712733511246665
aF14.727792603888677
aF14.74285169653069
aF14.757910789172703
aF14.772969881814715
aF14.78802897445673
aF14.803088067098741
aF14.818147159740755
aF14.833206252382768
aF14.84826534502478
aF14.863324437666794
aF14.878383530308806
aF14.89344262295082
aF14.908501715592832
aF14.923560808234846
aF14.938619900876859
aF14.95367899351887
aF14.968738086160885
aF14.983797178802897
aF14.998856271444911
aF15.013915364086923
aF15.028974456728937
aF15.04403354937095
aF15.059092642012962
aF15.074151734654976
aF15.089210827296988
aF15.104269919939002
aF15.119329012581014
aF15.134388105223026
aF15.14944719786504
aF15.164506290507052
aF15.179565383149066
aF15.194624475791079
aF15.209683568433093
aF15.224742661075105
aF15.239801753717117
aF15.254860846359131
aF15.269919939001143
aF15.284979031643157
aF15.30003812428517
aF15.315097216927182
aF15.330156309569196
aF15.345215402211208
aF15.360274494853222
aF15.375333587495234
aF15.390392680137248
aF15.40545177277926
aF15.420510865421273
aF15.435569958063287
aF15.450629050705299
aF15.465688143347313
aF15.480747235989325
aF15.495806328631339
aF15.510865421273351
aF15.525924513915363
aF15.540983606557377
aF15.55604269919939
aF15.571101791841404
aF15.586160884483416
aF15.601219977125428
aF15.616279069767442
aF15.631338162409454
aF15.646397255051468
aF15.66145634769348
aF15.676515440335494
aF15.691574532977507
aF15.706633625619519
aF15.721692718261533
aF15.736751810903545
aF15.75181090354556
aF15.766869996187571
aF15.781929088829584
aF15.796988181471598
aF15.81204727411361
aF15.827106366755624
aF15.842165459397636
aF15.85722455203965
aF15.872283644681662
aF15.887342737323674
aF15.902401829965688
aF15.9174609226077
aF15.932520015249715
aF15.947579107891727
aF15.96263820053374
aF15.977697293175753
aF15.992756385817765
aF16.00781547845978
aF16.02287457110179
aF16.037933663743804
aF16.05299275638582
aF16.06805184902783
aF16.083110941669844
aF16.098170034311856
aF16.11322912695387
aF16.128288219595884
aF16.143347312237896
aF16.15840640487991
aF16.17346549752192
aF16.188524590163933
aF16.20358368280595
aF16.21864277544796
aF16.233701868089973
aF16.248760960731985
aF16.263820053373998
aF16.278879146016013
aF16.293938238658026
aF16.308997331300038
aF16.32405642394205
aF16.339115516584066
aF16.354174609226078
aF16.36923370186809
aF16.384292794510102
aF16.399351887152115
aF16.41441097979413
aF16.429470072436143
aF16.444529165078155
aF16.459588257720167
aF16.47464735036218
aF16.489706443004195
aF16.504765535646207
aF16.51982462828822
aF16.53488372093023
aF16.549942813572244
aF16.56500190621426
aF16.580060998856272
aF16.595120091498284
aF16.610179184140296
aF16.62523827678231
aF16.640297369424324
aF16.655356462066337
aF16.67041555470835
aF16.68547464735036
aF16.700533739992377
aF16.71559283263439
aF16.7306519252764
aF16.745711017918413
aF16.760770110560426
aF16.77582920320244
aF16.790888295844454
aF16.805947388486466
aF16.821006481128478
aF16.83606557377049
aF16.851124666412506
aF16.86618375905452
aF16.88124285169653
aF16.896301944338543
aF16.911361036980555
aF16.92642012962257
aF16.941479222264583
aF16.956538314906595
aF16.971597407548607
aF16.986656500190623
aF17.001715592832635
aF17.016774685474648
aF17.03183377811666
aF17.046892870758672
aF17.061951963400688
aF17.0770110560427
aF17.092070148684712
aF17.107129241326724
aF17.122188333968737
aF17.137247426610752
aF17.152306519252765
aF17.167365611894777
aF17.18242470453679
aF17.1974837971788
aF17.212542889820817
aF17.22760198246283
aF17.24266107510484
aF17.257720167746854
aF17.27277926038887
aF17.28783835303088
aF17.302897445672894
aF17.317956538314906
aF17.33301563095692
aF17.348074723598934
aF17.363133816240946
aF17.37819290888296
aF17.39325200152497
aF17.408311094166983
aF17.423370186809
aF17.43842927945101
aF17.453488372093023
aF17.468547464735035
aF17.483606557377048
aF17.498665650019063
aF17.513724742661076
aF17.528783835303088
aF17.5438429279451
aF17.558902020587112
aF17.573961113229128
aF17.58902020587114
aF17.604079298513152
aF17.619138391155165
aF17.63419748379718
aF17.649256576439193
aF17.664315669081205
aF17.679374761723217
aF17.69443385436523
aF17.709492947007245
aF17.724552039649257
aF17.73961113229127
aF17.75467022493328
aF17.769729317575294
aF17.78478841021731
aF17.799847502859322
aF17.814906595501334
aF17.829965688143346
aF17.84502478078536
aF17.860083873427374
aF17.875142966069387
aF17.8902020587114
aF17.90526115135341
aF17.920320243995427
aF17.93537933663744
aF17.95043842927945
aF17.965497521921463
aF17.980556614563476
aF17.99561570720549
aF18.010674799847504
aF18.025733892489516
aF18.040792985131528
aF18.05585207777354
aF18.070911170415556
aF18.08597026305757
aF18.10102935569958
aF18.116088448341593
aF18.131147540983605
aF18.14620663362562
aF18.161265726267633
aF18.176324818909645
aF18.191383911551657
aF18.206443004193673
aF18.221502096835685
aF18.236561189477698
aF18.25162028211971
aF18.266679374761722
aF18.281738467403738
aF18.29679756004575
aF18.311856652687762
aF18.326915745329774
aF18.341974837971787
aF18.357033930613802
aF18.372093023255815
aF18.387152115897827
aF18.40221120853984
aF18.41727030118185
aF18.432329393823867
aF18.44738848646588
aF18.46244757910789
aF18.477506671749904
aF18.492565764391916
aF18.50762485703393
aF18.522683949675944
aF18.537743042317956
aF18.55280213495997
aF18.567861227601984
aF18.582920320243996
aF18.59797941288601
aF18.61303850552802
aF18.628097598170033
aF18.64315669081205
aF18.65821578345406
aF18.673274876096073
aF18.688333968738085
aF18.703393061380098
aF18.718452154022113
aF18.733511246664126
aF18.748570339306138
aF18.76362943194815
aF18.778688524590162
aF18.793747617232178
aF18.80880670987419
aF18.823865802516202
aF18.838924895158215
aF18.85398398780023
aF18.869043080442243
aF18.884102173084255
aF18.899161265726267
aF18.91422035836828
aF18.929279451010295
aF18.944338543652307
aF18.95939763629432
aF18.97445672893633
aF18.989515821578344
aF19.00457491422036
aF19.019634006862372
aF19.034693099504384
aF19.049752192146396
aF19.06481128478841
aF19.079870377430424
aF19.094929470072437
aF19.10998856271445
aF19.12504765535646
aF19.140106747998477
aF19.15516584064049
aF19.1702249332825
aF19.185284025924513
aF19.200343118566526
aF19.21540221120854
aF19.230461303850554
aF19.245520396492566
aF19.260579489134578
aF19.27563858177659
aF19.290697674418606
aF19.30575676706062
aF19.32081585970263
aF19.335874952344643
aF19.350934044986655
aF19.36599313762867
aF19.381052230270683
aF19.396111322912695
aF19.411170415554707
aF19.42622950819672
aF19.441288600838735
aF19.456347693480748
aF19.47140678612276
aF19.486465878764772
aF19.501524971406788
aF19.5165840640488
aF19.531643156690812
aF19.546702249332824
aF19.561761341974837
aF19.576820434616852
aF19.591879527258865
aF19.606938619900877
aF19.62199771254289
aF19.6370568051849
aF19.652115897826917
aF19.66717499046893
aF19.68223408311094
aF19.697293175752954
aF19.712352268394966
aF19.72741136103698
aF19.742470453678994
aF19.757529546321006
aF19.77258863896302
aF19.787647731605034
aF19.802706824247046
aF19.81776591688906
aF19.83282500953107
aF19.847884102173083
aF19.8629431948151
aF19.87800228745711
aF19.893061380099123
aF19.908120472741135
aF19.923179565383148
aF19.938238658025163
aF19.953297750667176
aF19.968356843309188
aF19.9834159359512
aF19.998475028593212
aF20.013534121235228
aF20.02859321387724
aF20.043652306519252
aF20.058711399161265
aF20.07377049180328
aF20.088829584445293
aF20.103888677087305
aF20.118947769729317
aF20.13400686237133
aF20.149065955013345
aF20.164125047655357
aF20.17918414029737
aF20.19424323293938
aF20.209302325581394
aF20.22436141822341
aF20.239420510865422
aF20.254479603507434
aF20.269538696149446
aF20.28459778879146
aF20.299656881433474
aF20.314715974075487
aF20.3297750667175
aF20.34483415935951
aF20.359893252001523
aF20.37495234464354
aF20.39001143728555
aF20.405070529927563
aF20.420129622569576
aF20.43518871521159
aF20.450247807853604
aF20.465306900495616
aF20.480365993137628
aF20.49542508577964
aF20.510484178421656
aF20.52554327106367
aF20.54060236370568
aF20.555661456347693
aF20.570720548989705
aF20.58577964163172
aF20.600838734273733
aF20.615897826915745
aF20.630956919557757
aF20.64601601219977
aF20.661075104841785
aF20.676134197483798
aF20.69119329012581
aF20.706252382767822
aF20.721311475409838
aF20.73637056805185
aF20.751429660693862
aF20.766488753335874
aF20.781547845977887
aF20.796606938619902
aF20.811666031261915
aF20.826725123903927
aF20.84178421654594
aF20.85684330918795
aF20.871902401829967
aF20.88696149447198
aF20.90202058711399
aF20.917079679756004
aF20.932138772398016
aF20.94719786504003
aF20.962256957682044
aF20.977316050324056
aF20.99237514296607
aF21.007434235608084
aF21.022493328250096
aF21.03755242089211
aF21.05261151353412
aF21.067670606176133
aF21.08272969881815
aF21.09778879146016
aF21.112847884102173
aF21.127906976744185
aF21.142966069386198
aF21.158025162028213
aF21.173084254670226
aF21.188143347312238
aF21.20320243995425
aF21.218261532596262
aF21.233320625238278
aF21.24837971788029
aF21.263438810522302
aF21.278497903164315
aF21.293556995806327
aF21.308616088448343
aF21.323675181090355
aF21.338734273732367
aF21.35379336637438
aF21.368852459016395
aF21.383911551658407
aF21.39897064430042
aF21.41402973694243
aF21.429088829584444
aF21.44414792222646
aF21.459207014868472
aF21.474266107510484
aF21.489325200152496
aF21.50438429279451
aF21.519443385436524
aF21.534502478078537
aF21.54956157072055
aF21.56462066336256
aF21.579679756004573
aF21.59473884864659
aF21.6097979412886
aF21.624857033930613
aF21.639916126572626
aF21.65497521921464
aF21.670034311856654
aF21.685093404498666
aF21.700152497140678
aF21.71521158978269
aF21.730270682424706
aF21.74532977506672
aF21.76038886770873
aF21.775447960350743
aF21.790507052992755
aF21.80556614563477
aF21.820625238276783
aF21.835684330918795
aF21.850743423560807
aF21.86580251620282
aF21.880861608844835
aF21.895920701486848
aF21.91097979412886
aF21.926038886770872
aF21.941097979412888
aF21.9561570720549
aF21.971216164696912
aF21.986275257338924
aF22.001334349980937
aF22.016393442622952
aF22.031452535264965
aF22.046511627906977
aF22.06157072054899
aF22.076629813191
aF22.091688905833017
aF22.10674799847503
aF22.12180709111704
aF22.136866183759054
aF22.151925276401066
aF22.16698436904308
aF22.182043461685094
aF22.197102554327106
aF22.21216164696912
aF22.22722073961113
aF22.242279832253146
aF22.25733892489516
aF22.27239801753717
aF22.287457110179183
aF22.3025162028212
aF22.31757529546321
aF22.332634388105223
aF22.347693480747235
aF22.362752573389248
aF22.377811666031263
aF22.392870758673276
aF22.407929851315288
aF22.4229889439573
aF22.438048036599312
aF22.453107129241328
aF22.46816622188334
aF22.483225314525352
aF22.498284407167365
aF22.513343499809377
aF22.528402592451393
aF22.543461685093405
aF22.558520777735417
aF22.57357987037743
aF22.588638963019445
aF22.603698055661457
aF22.61875714830347
aF22.63381624094548
aF22.648875333587494
aF22.66393442622951
aF22.678993518871522
aF22.694052611513534
aF22.709111704155546
aF22.72417079679756
aF22.739229889439574
aF22.754288982081587
aF22.7693480747236
aF22.78440716736561
aF22.799466260007623
aF22.81452535264964
aF22.82958444529165
aF22.844643537933663
aF22.859702630575676
aF22.87476172321769
aF22.889820815859704
aF22.904879908501716
aF22.919939001143728
aF22.93499809378574
aF22.950057186427756
aF22.96511627906977
aF22.98017537171178
aF22.995234464353793
aF23.010293556995805
aF23.02535264963782
aF23.040411742279833
aF23.055470834921845
aF23.070529927563857
aF23.08558902020587
aF23.100648112847885
aF23.115707205489898
aF23.13076629813191
aF23.145825390773922
aF23.160884483415934
aF23.17594357605795
aF23.191002668699962
aF23.206061761341974
aF23.221120853983987
aF23.236179946626002
aF23.251239039268015
aF23.266298131910027
aF23.28135722455204
aF23.29641631719405
aF23.311475409836067
aF23.32653450247808
aF23.34159359512009
aF23.356652687762104
aF23.371711780404116
aF23.38677087304613
aF23.401829965688144
aF23.416889058330156
aF23.43194815097217
aF23.44700724361418
aF23.462066336256196
aF23.47712542889821
aF23.49218452154022
aF23.507243614182233
aF23.52230270682425
aF23.53736179946626
aF23.552420892108273
aF23.567479984750285
aF23.582539077392298
aF23.597598170034313
aF23.612657262676326
aF23.627716355318338
aF23.64277544796035
aF23.657834540602362
aF23.672893633244378
aF23.68795272588639
aF23.703011818528402
aF23.718070911170415
aF23.733130003812427
aF23.748189096454443
aF23.763248189096455
aF23.778307281738467
aF23.79336637438048
aF23.808425467022495
aF23.823484559664507
aF23.83854365230652
aF23.85360274494853
aF23.868661837590544
aF23.88372093023256
aF23.898780022874572
aF23.913839115516584
aF23.928898208158596
aF23.94395730080061
aF23.959016393442624
aF23.974075486084637
aF23.98913457872665
aF24.00419367136866
aF24.019252764010673
aF24.03431185665269
aF24.0493709492947
aF24.064430041936713
aF24.079489134578726
aF24.094548227220738
aF24.109607319862754
aF24.124666412504766
aF24.139725505146778
aF24.15478459778879
aF24.169843690430806
aF24.18490278307282
aF24.19996187571483
aF24.215020968356843
aF24.230080060998855
aF24.24513915364087
aF24.260198246282883
aF24.275257338924895
aF24.290316431566907
aF24.30537552420892
aF24.320434616850935
aF24.335493709492948
aF24.35055280213496
aF24.365611894776972
aF24.380670987418984
aF24.395730080061
aF24.410789172703012
aF24.425848265345024
aF24.440907357987037
aF24.455966450629052
aF24.471025543271065
aF24.486084635913077
aF24.50114372855509
aF24.5162028211971
aF24.531261913839117
aF24.54632100648113
aF24.56138009912314
aF24.576439191765154
aF24.591498284407166
aF24.60655737704918
aF24.621616469691194
aF24.636675562333206
aF24.65173465497522
aF24.66679374761723
aF24.681852840259246
aF24.69691193290126
aF24.71197102554327
aF24.727030118185283
aF24.7420892108273
aF24.75714830346931
aF24.772207396111323
aF24.787266488753335
aF24.802325581395348
aF24.817384674037363
aF24.832443766679376
aF24.847502859321388
aF24.8625619519634
aF24.877621044605412
aF24.892680137247428
aF24.90773922988944
aF24.922798322531452
aF24.937857415173465
aF24.952916507815477
aF24.967975600457493
aF24.983034693099505
aF24.998093785741517
aF25.01315287838353
aF25.02821197102554
aF25.043271063667557
aF25.05833015630957
aF25.07338924895158
aF25.088448341593594
aF25.10350743423561
aF25.118566526877622
aF25.133625619519634
aF25.148684712161646
aF25.16374380480366
aF25.178802897445674
aF25.193861990087687
aF25.2089210827297
aF25.22398017537171
aF25.239039268013723
aF25.25409836065574
aF25.26915745329775
aF25.284216545939763
aF25.299275638581776
aF25.314334731223788
aF25.329393823865804
aF25.344452916507816
aF25.359512009149828
aF25.37457110179184
aF25.389630194433856
aF25.40468928707587
aF25.41974837971788
aF25.434807472359893
aF25.449866565001905
aF25.46492565764392
aF25.479984750285933
aF25.495043842927945
aF25.510102935569957
aF25.52516202821197
aF25.540221120853985
aF25.555280213495998
aF25.57033930613801
aF25.585398398780022
aF25.600457491422034
aF25.61551658406405
aF25.630575676706062
aF25.645634769348074
aF25.660693861990087
aF25.675752954632102
aF25.690812047274115
aF25.705871139916127
aF25.72093023255814
aF25.73598932520015
aF25.751048417842167
aF25.76610751048418
aF25.78116660312619
aF25.796225695768204
aF25.811284788410216
aF25.82634388105223
aF25.841402973694244
aF25.856462066336256
aF25.87152115897827
aF25.88658025162028
aF25.901639344262296
aF25.91669843690431
aF25.93175752954632
aF25.946816622188333
aF25.961875714830345
aF25.97693480747236
aF25.991993900114373
aF26.007052992756385
aF26.022112085398398
aF26.037171178040413
aF26.052230270682426
aF26.067289363324438
aF26.08234845596645
aF26.097407548608462
aF26.112466641250478
aF26.12752573389249
aF26.142584826534502
aF26.157643919176515
aF26.172703011818527
aF26.187762104460543
aF26.202821197102555
aF26.217880289744567
aF26.23293938238658
aF26.24799847502859
aF26.263057567670607
aF26.27811666031262
aF26.29317575295463
aF26.308234845596644
aF26.32329393823866
aF26.338353030880672
aF26.353412123522684
aF26.368471216164696
aF26.38353030880671
aF26.398589401448724
aF26.413648494090737
aF26.42870758673275
aF26.44376667937476
aF26.458825772016773
aF26.47388486465879
aF26.4889439573008
aF26.504003049942813
aF26.519062142584826
aF26.534121235226838
aF26.549180327868854
aF26.564239420510866
aF26.579298513152878
aF26.59435760579489
aF26.609416698436906
aF26.62447579107892
aF26.63953488372093
aF26.654593976362943
aF26.669653069004955
aF26.68471216164697
aF26.699771254288983
aF26.714830346930995
aF26.729889439573007
aF26.74494853221502
aF26.760007624857035
aF26.775066717499048
aF26.79012581014106
aF26.805184902783072
aF26.820243995425084
aF26.8353030880671
aF26.850362180709112
aF26.865421273351124
aF26.880480365993137
aF26.89553945863515
aF26.910598551277165
aF26.925657643919177
aF26.94071673656119
aF26.9557758292032
aF26.970834921845217
aF26.98589401448723
aF27.00095310712924
aF27.016012199771254
aF27.031071292413266
aF27.04613038505528
aF27.061189477697294
aF27.076248570339306
aF27.09130766298132
aF27.10636675562333
aF27.121425848265346
aF27.13648494090736
aF27.15154403354937
aF27.166603126191383
aF27.181662218833395
aF27.19672131147541
aF27.211780404117423
aF27.226839496759435
aF27.241898589401448
aF27.256957682043463
aF27.272016774685476
aF27.287075867327488
aF27.3021349599695
aF27.317194052611512
aF27.332253145253528
aF27.34731223789554
aF27.362371330537552
aF27.377430423179565
aF27.392489515821577
aF27.407548608463593
aF27.422607701105605
aF27.437666793747617
aF27.45272588638963
aF27.46778497903164
aF27.482844071673657
aF27.49790316431567
aF27.51296225695768
aF27.528021349599694
aF27.54308044224171
aF27.558139534883722
aF27.573198627525734
aF27.588257720167746
aF27.60331681280976
aF27.618375905451774
aF27.633434998093787
aF27.6484940907358
aF27.66355318337781
aF27.678612276019823
aF27.69367136866184
aF27.70873046130385
aF27.723789553945863
aF27.738848646587876
aF27.753907739229888
aF27.768966831871904
aF27.784025924513916
aF27.799085017155928
aF27.81414410979794
aF27.829203202439952
aF27.84426229508197
aF27.85932138772398
aF27.874380480365993
aF27.889439573008005
aF27.90449866565002
aF27.919557758292033
aF27.934616850934045
aF27.949675943576057
aF27.96473503621807
aF27.979794128860085
aF27.994853221502098
aF28.00991231414411
aF28.024971406786122
aF28.040030499428134
aF28.05508959207015
aF28.070148684712162
aF28.085207777354174
aF28.100266869996187
aF28.1153259626382
aF28.130385055280215
aF28.145444147922227
aF28.16050324056424
aF28.17556233320625
aF28.190621425848267
aF28.20568051849028
aF28.22073961113229
aF28.235798703774304
aF28.250857796416316
aF28.26591688905833
aF28.280975981700344
aF28.296035074342356
aF28.31109416698437
aF28.32615325962638
aF28.341212352268396
aF28.35627144491041
aF28.37133053755242
aF28.386389630194433
aF28.401448722836445
aF28.41650781547846
aF28.431566908120473
aF28.446626000762485
aF28.461685093404498
aF28.476744186046513
aF28.491803278688526
aF28.506862371330538
aF28.52192146397255
aF28.536980556614562
aF28.552039649256578
aF28.56709874189859
aF28.582157834540602
aF28.597216927182615
aF28.612276019824627
aF28.627335112466643
aF28.642394205108655
aF28.657453297750667
aF28.67251239039268
aF28.68757148303469
aF28.702630575676707
aF28.71768966831872
aF28.73274876096073
aF28.747807853602744
aF28.762866946244756
aF28.777926038886772
aF28.792985131528784
aF28.808044224170796
aF28.82310331681281
aF28.838162409454824
aF28.853221502096837
aF28.86828059473885
aF28.88333968738086
aF28.898398780022873
aF28.91345787266489
aF28.9285169653069
aF28.943576057948913
aF28.958635150590926
aF28.973694243232938
aF28.988753335874954
aF29.003812428516966
aF29.018871521158978
aF29.03393061380099
aF29.048989706443002
aF29.06404879908502
aF29.07910789172703
aF29.094166984369043
aF29.109226077011055
aF29.12428516965307
aF29.139344262295083
aF29.154403354937095
aF29.169462447579107
aF29.18452154022112
aF29.199580632863135
aF29.214639725505148
aF29.22969881814716
aF29.244757910789172
aF29.259817003431184
aF29.2748760960732
aF29.289935188715212
aF29.304994281357224
aF29.320053373999237
aF29.33511246664125
aF29.350171559283265
aF29.365230651925277
aF29.38028974456729
aF29.3953488372093
aF29.410407929851317
aF29.42546702249333
aF29.44052611513534
aF29.455585207777354
aF29.470644300419366
aF29.48570339306138
aF29.500762485703394
aF29.515821578345406
aF29.53088067098742
aF29.54593976362943
aF29.560998856271446
aF29.57605794891346
aF29.59111704155547
aF29.606176134197483
aF29.621235226839495
aF29.63629431948151
aF29.651353412123523
aF29.666412504765535
aF29.681471597407548
aF29.69653069004956
aF29.711589782691576
aF29.726648875333588
aF29.7417079679756
aF29.756767060617612
aF29.771826153259628
aF29.78688524590164
aF29.801944338543652
aF29.817003431185665
aF29.832062523827677
aF29.847121616469693
aF29.862180709111705
aF29.877239801753717
aF29.89229889439573
aF29.90735798703774
aF29.922417079679757
aF29.93747617232177
aF29.95253526496378
aF29.967594357605794
aF29.982653450247806
aF29.997712542889822
aF29.999809378574152
atp28
tp29
Rp30
aa(lp31
S'accuracy'
p32
ag0
(g24
g25
(S'Accuracy'
p33
(lp34
F0.0
aF0.0
aF0.375
aF0.375
aF0.5
aF0.125
aF0.75
aF0.5
aF0.5
aF0.875
aF0.625
aF0.875
aF0.375
aF0.75
aF0.75
aF0.625
aF0.75
aF0.75
aF1.0
aF0.875
aF0.75
aF1.0
aF0.75
aF0.75
aF0.875
aF0.75
aF0.25
aF0.625
aF0.75
aF0.75
aF0.875
aF0.625
aF0.875
aF0.75
aF1.0
aF0.875
aF0.75
aF0.625
aF0.75
aF1.0
aF0.75
aF0.5
aF0.75
aF0.875
aF1.0
aF0.875
aF0.75
aF0.625
aF1.0
aF0.625
aF0.875
aF0.75
aF0.875
aF0.875
aF0.875
aF1.0
aF0.75
aF0.75
aF1.0
aF0.75
aF0.75
aF0.625
aF0.875
aF1.0
aF0.625
aF1.0
aF0.625
aF0.75
aF0.75
aF0.625
aF0.5
aF0.875
aF0.875
aF0.875
aF1.0
aF0.75
aF0.75
aF0.875
aF1.0
aF0.875
aF1.0
aF1.0
aF0.75
aF1.0
aF0.75
aF0.625
aF1.0
aF0.5
aF0.75
aF1.0
aF0.75
aF0.875
aF0.875
aF0.875
aF1.0
aF0.625
aF1.0
aF0.875
aF0.75
aF0.875
aF0.875
aF0.625
aF0.75
aF0.75
aF0.375
aF0.625
aF0.75
aF0.5
aF0.875
aF0.75
aF0.75
aF1.0
aF0.875
aF0.875
aF1.0
aF1.0
aF0.75
aF0.875
aF1.0
aF1.0
aF0.875
aF0.875
aF0.75
aF0.75
aF0.875
aF0.875
aF1.0
aF0.875
aF0.875
aF0.875
aF1.0
aF0.875
aF0.75
aF0.875
aF0.875
aF0.625
aF0.875
aF0.875
aF0.875
aF1.0
aF0.875
aF1.0
aF0.875
aF0.625
aF0.75
aF0.75
aF0.875
aF0.875
aF0.75
aF0.75
aF0.875
aF0.75
aF0.875
aF0.875
aF0.875
aF0.5
aF0.875
aF0.75
aF0.875
aF0.75
aF1.0
aF1.0
aF0.875
aF1.0
aF0.75
aF1.0
aF0.75
aF0.875
aF1.0
aF0.875
aF0.625
aF0.75
aF1.0
aF0.625
aF0.875
aF0.875
aF0.5
aF0.75
aF1.0
aF0.875
aF0.875
aF0.75
aF0.625
aF0.875
aF1.0
aF0.75
aF0.875
aF0.875
aF1.0
aF0.75
aF0.625
aF1.0
aF0.625
aF0.875
aF0.5
aF1.0
aF0.625
aF1.0
aF1.0
aF0.875
aF1.0
aF0.625
aF0.75
aF0.875
aF0.75
aF1.0
aF0.625
aF1.0
aF1.0
aF0.875
aF0.625
aF0.875
aF0.75
aF0.75
aF0.625
aF0.75
aF0.875
aF1.0
aF0.875
aF0.875
aF0.875
aF1.0
aF1.0
aF1.0
aF0.875
aF0.75
aF0.875
aF1.0
aF1.0
aF0.875
aF1.0
aF1.0
aF0.875
aF0.75
aF0.875
aF0.375
aF0.625
aF0.875
aF0.75
aF0.75
aF0.875
aF0.875
aF1.0
aF0.875
aF0.75
aF0.875
aF0.5
aF0.875
aF0.875
aF1.0
aF0.875
aF0.875
aF0.75
aF0.625
aF1.0
aF0.75
aF1.0
aF1.0
aF0.875
aF0.875
aF0.875
aF0.875
aF0.75
aF1.0
aF0.875
aF0.875
aF1.0
aF0.875
aF0.75
aF0.625
aF0.625
aF1.0
aF0.875
aF1.0
aF1.0
aF0.75
aF1.0
aF0.625
aF0.75
aF1.0
aF0.875
aF0.875
aF1.0
aF0.875
aF0.875
aF0.875
aF0.75
aF1.0
aF1.0
aF0.875
aF0.875
aF0.875
aF0.75
aF1.0
aF1.0
aF0.625
aF1.0
aF1.0
aF0.875
aF1.0
aF0.875
aF0.875
aF0.875
aF1.0
aF0.75
aF1.0
aF0.875
aF1.0
aF1.0
aF0.75
aF0.625
aF0.875
aF0.75
aF0.875
aF0.875
aF1.0
aF1.0
aF1.0
aF0.75
aF0.75
aF0.875
aF0.75
aF0.875
aF1.0
aF0.75
aF0.875
aF0.875
aF0.875
aF1.0
aF0.875
aF0.75
aF1.0
aF0.875
aF1.0
aF0.875
aF0.875
aF0.75
aF0.875
aF0.5
aF1.0
aF1.0
aF0.875
aF0.75
aF0.875
aF1.0
aF0.875
aF1.0
aF0.875
aF0.875
aF1.0
aF1.0
aF0.875
aF1.0
aF0.875
aF0.875
aF0.875
aF0.875
aF1.0
aF0.75
aF1.0
aF0.875
aF0.75
aF0.75
aF0.875
aF0.75
aF0.75
aF0.875
aF0.875
aF0.625
aF0.625
aF0.875
aF0.875
aF0.75
aF0.875
aF0.875
aF0.75
aF0.875
aF0.875
aF0.875
aF0.625
aF1.0
aF0.875
aF0.875
aF0.875
aF1.0
aF1.0
aF0.875
aF0.875
aF1.0
aF1.0
aF1.0
aF0.875
aF0.75
aF1.0
aF0.875
aF0.75
aF0.875
aF0.875
aF1.0
aF1.0
aF0.75
aF0.75
aF1.0
aF0.875
aF0.75
aF1.0
aF0.875
aF1.0
aF1.0
aF0.75
aF1.0
aF1.0
aF1.0
aF0.75
aF1.0
aF0.75
aF1.0
aF0.875
aF0.875
aF1.0
aF0.875
aF1.0
aF0.875
aF0.875
aF0.875
aF0.75
aF1.0
aF1.0
aF1.0
aF0.875
aF0.875
aF0.875
aF0.875
aF0.875
aF0.625
aF1.0
aF0.875
aF0.875
aF0.875
aF1.0
aF1.0
aF1.0
aF1.0
aF0.75
aF1.0
aF0.875
aF0.875
aF0.75
aF1.0
aF0.75
aF0.875
aF0.75
aF0.875
aF0.625
aF0.875
aF0.875
aF0.625
aF1.0
aF1.0
aF0.875
aF1.0
aF0.875
aF0.875
aF0.875
aF1.0
aF0.875
aF0.625
aF1.0
aF1.0
aF0.875
aF0.875
aF0.875
aF1.0
aF1.0
aF1.0
aF0.875
aF0.75
aF0.875
aF0.875
aF0.875
aF1.0
aF1.0
aF0.75
aF0.75
aF0.875
aF1.0
aF0.75
aF1.0
aF0.875
aF0.875
aF0.625
aF0.875
aF0.875
aF0.875
aF1.0
aF1.0
aF1.0
aF0.875
aF0.75
aF0.875
aF1.0
aF0.875
aF0.875
aF0.75
aF1.0
aF0.625
aF0.875
aF0.875
aF0.875
aF1.0
aF0.875
aF1.0
aF0.625
aF1.0
aF0.875
aF1.0
aF0.875
aF1.0
aF0.75
aF1.0
aF1.0
aF1.0
aF1.0
aF0.75
aF0.875
aF0.625
aF0.875
aF1.0
aF1.0
aF0.875
aF1.0
aF0.875
aF0.875
aF0.875
aF1.0
aF0.625
aF0.75
aF1.0
aF0.75
aF0.875
aF0.75
aF0.75
aF0.75
aF0.875
aF0.875
aF1.0
aF1.0
aF0.875
aF1.0
aF1.0
aF1.0
aF0.875
aF0.875
aF0.875
aF1.0
aF1.0
aF0.875
aF0.875
aF1.0
aF1.0
aF0.75
aF1.0
aF0.875
aF1.0
aF0.875
aF0.75
aF1.0
aF0.875
aF1.0
aF1.0
aF0.75
aF0.875
aF1.0
aF0.875
aF1.0
aF1.0
aF0.875
aF1.0
aF0.875
aF1.0
aF0.875
aF0.75
aF1.0
aF0.875
aF1.0
aF1.0
aF0.625
aF1.0
aF0.875
aF1.0
aF0.625
aF1.0
aF1.0
aF0.75
aF0.625
aF0.75
aF1.0
aF0.875
aF0.75
aF1.0
aF1.0
aF0.75
aF0.625
aF0.875
aF1.0
aF1.0
aF1.0
aF1.0
aF0.75
aF1.0
aF1.0
aF1.0
aF1.0
aF0.75
aF1.0
aF0.75
aF1.0
aF1.0
aF0.75
aF0.875
aF0.75
aF0.875
aF1.0
aF1.0
aF0.875
aF0.875
aF1.0
aF1.0
aF0.875
aF0.75
aF0.875
aF0.875
aF1.0
aF0.75
aF0.875
aF1.0
aF1.0
aF0.875
aF1.0
aF0.75
aF0.875
aF1.0
aF1.0
aF0.75
aF1.0
aF1.0
aF0.75
aF1.0
aF1.0
aF1.0
aF0.875
aF1.0
aF0.875
aF0.875
aF0.875
aF0.875
aF1.0
aF0.875
aF0.625
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF0.625
aF0.875
aF1.0
aF1.0
aF0.75
aF0.875
aF1.0
aF0.75
aF0.875
aF0.875
aF1.0
aF1.0
aF1.0
aF0.875
aF1.0
aF1.0
aF0.875
aF1.0
aF0.875
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF0.875
aF0.875
aF0.875
aF0.875
aF1.0
aF1.0
aF1.0
aF1.0
aF0.75
aF0.875
aF0.875
aF0.75
aF0.875
aF0.75
aF1.0
aF1.0
aF0.75
aF0.875
aF0.875
aF1.0
aF0.875
aF0.875
aF0.75
aF0.875
aF1.0
aF0.875
aF1.0
aF1.0
aF0.75
aF0.875
aF1.0
aF1.0
aF1.0
aF0.75
aF1.0
aF0.875
aF0.875
aF1.0
aF1.0
aF1.0
aF1.0
aF0.875
aF1.0
aF1.0
aF1.0
aF0.875
aF0.875
aF1.0
aF0.875
aF1.0
aF1.0
aF1.0
aF0.875
aF0.75
aF1.0
aF1.0
aF1.0
aF0.875
aF1.0
aF0.875
aF0.875
aF1.0
aF0.875
aF1.0
aF1.0
aF0.75
aF0.875
aF0.875
aF0.875
aF1.0
aF0.875
aF1.0
aF1.0
aF0.875
aF0.875
aF1.0
aF0.875
aF0.75
aF0.875
aF0.875
aF1.0
aF0.875
aF0.75
aF1.0
aF0.875
aF0.875
aF0.875
aF1.0
aF0.75
aF0.875
aF0.875
aF0.875
aF1.0
aF1.0
aF1.0
aF0.875
aF0.875
aF1.0
aF1.0
aF0.875
aF1.0
aF1.0
aF0.875
aF1.0
aF1.0
aF1.0
aF1.0
aF0.875
aF0.75
aF0.75
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF0.625
aF1.0
aF0.75
aF1.0
aF1.0
aF0.75
aF0.75
aF1.0
aF1.0
aF0.75
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF0.875
aF0.625
aF0.875
aF0.875
aF0.875
aF0.875
aF0.625
aF1.0
aF0.875
aF1.0
aF0.625
aF0.75
aF1.0
aF0.875
aF1.0
aF1.0
aF0.75
aF1.0
aF1.0
aF0.75
aF0.875
aF0.75
aF0.75
aF0.875
aF1.0
aF1.0
aF1.0
aF0.875
aF1.0
aF0.875
aF0.75
aF1.0
aF1.0
aF1.0
aF0.875
aF0.625
aF0.75
aF1.0
aF1.0
aF1.0
aF1.0
aF0.75
aF1.0
aF0.875
aF1.0
aF1.0
aF1.0
aF0.75
aF1.0
aF1.0
aF0.75
aF0.875
aF1.0
aF0.875
aF1.0
aF1.0
aF1.0
aF0.875
aF1.0
aF1.0
aF1.0
aF1.0
aF0.75
aF0.875
aF0.875
aF1.0
aF1.0
aF1.0
aF0.75
aF1.0
aF0.875
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF0.75
aF0.875
aF0.875
aF0.75
aF0.75
aF1.0
aF1.0
aF0.875
aF1.0
aF0.875
aF0.875
aF1.0
aF0.875
aF0.875
aF1.0
aF0.875
aF0.875
aF1.0
aF1.0
aF1.0
aF0.875
aF0.875
aF0.875
aF0.875
aF1.0
aF0.875
aF1.0
aF0.875
aF0.875
aF0.875
aF1.0
aF1.0
aF0.875
aF0.875
aF1.0
aF0.75
aF0.875
aF0.875
aF0.875
aF1.0
aF1.0
aF0.875
aF1.0
aF0.875
aF0.875
aF1.0
aF1.0
aF0.875
aF0.875
aF1.0
aF1.0
aF0.75
aF0.875
aF0.75
aF0.875
aF0.75
aF0.75
aF1.0
aF1.0
aF1.0
aF0.875
aF1.0
aF1.0
aF0.875
aF1.0
aF1.0
aF0.875
aF0.875
aF1.0
aF1.0
aF0.875
aF1.0
aF1.0
aF0.875
aF1.0
aF1.0
aF0.75
aF1.0
aF0.875
aF1.0
aF0.75
aF0.625
aF0.875
aF1.0
aF0.875
aF0.625
aF1.0
aF0.875
aF1.0
aF1.0
aF1.0
aF0.75
aF1.0
aF0.875
aF1.0
aF0.875
aF1.0
aF0.75
aF1.0
aF1.0
aF0.875
aF0.75
aF0.875
aF0.875
aF1.0
aF1.0
aF1.0
aF0.875
aF0.875
aF1.0
aF0.75
aF1.0
aF0.875
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF0.75
aF0.875
aF0.875
aF0.875
aF0.875
aF1.0
aF1.0
aF1.0
aF0.875
aF0.875
aF0.875
aF1.0
aF0.75
aF0.875
aF1.0
aF0.625
aF0.875
aF1.0
aF0.875
aF0.875
aF0.875
aF1.0
aF0.875
aF1.0
aF1.0
aF1.0
aF0.875
aF1.0
aF0.75
aF1.0
aF0.875
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF0.875
aF0.875
aF1.0
aF1.0
aF0.5
aF1.0
aF0.875
aF1.0
aF1.0
aF0.75
aF1.0
aF1.0
aF0.875
aF0.875
aF0.875
aF0.875
aF1.0
aF1.0
aF1.0
aF1.0
aF0.75
aF0.875
aF1.0
aF1.0
aF1.0
aF0.75
aF1.0
aF0.875
aF1.0
aF0.875
aF1.0
aF0.875
aF1.0
aF0.875
aF0.875
aF0.625
aF0.875
aF0.875
aF0.875
aF0.75
aF1.0
aF0.75
aF1.0
aF0.875
aF0.875
aF1.0
aF0.875
aF0.75
aF1.0
aF1.0
aF1.0
aF0.75
aF1.0
aF0.875
aF0.875
aF1.0
aF1.0
aF1.0
aF0.75
aF1.0
aF0.75
aF1.0
aF1.0
aF1.0
aF0.75
aF1.0
aF1.0
aF1.0
aF1.0
aF0.875
aF0.875
aF0.875
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF0.75
aF1.0
aF0.875
aF0.875
aF1.0
aF0.625
aF0.875
aF0.75
aF0.875
aF1.0
aF1.0
aF0.875
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF0.625
aF1.0
aF1.0
aF0.875
aF0.875
aF0.625
aF0.875
aF0.875
aF0.875
aF0.75
aF0.875
aF1.0
aF1.0
aF1.0
aF1.0
aF0.875
aF1.0
aF1.0
aF1.0
aF0.875
aF1.0
aF1.0
aF1.0
aF0.875
aF1.0
aF1.0
aF0.875
aF1.0
aF0.875
aF1.0
aF1.0
aF0.875
aF1.0
aF1.0
aF0.625
aF0.875
aF1.0
aF1.0
aF0.75
aF1.0
aF1.0
aF0.875
aF0.875
aF0.875
aF0.75
aF0.875
aF1.0
aF0.875
aF0.875
aF1.0
aF0.875
aF0.875
aF0.875
aF1.0
aF0.75
aF0.875
aF0.875
aF1.0
aF0.875
aF1.0
aF0.75
aF1.0
aF1.0
aF0.75
aF0.625
aF0.75
aF0.75
aF0.875
aF0.5
aF0.875
aF0.875
aF1.0
aF1.0
aF0.75
aF0.875
aF0.875
aF1.0
aF1.0
aF1.0
aF1.0
aF0.75
aF0.875
aF1.0
aF0.875
aF1.0
aF0.75
aF0.875
aF0.75
aF0.75
aF0.875
aF0.75
aF1.0
aF0.875
aF1.0
aF1.0
aF0.875
aF0.75
aF0.875
aF0.875
aF1.0
aF1.0
aF1.0
aF1.0
aF0.875
aF0.875
aF0.75
aF1.0
aF0.875
aF1.0
aF0.875
aF1.0
aF1.0
aF0.875
aF0.75
aF1.0
aF1.0
aF0.875
aF0.875
aF1.0
aF1.0
aF1.0
aF0.875
aF0.875
aF0.875
aF1.0
aF0.75
aF0.875
aF1.0
aF1.0
aF1.0
aF1.0
aF0.75
aF0.625
aF1.0
aF0.75
aF0.875
aF1.0
aF0.875
aF1.0
aF0.875
aF0.875
aF1.0
aF1.0
aF0.875
aF0.875
aF0.875
aF0.875
aF1.0
aF1.0
aF1.0
aF1.0
aF0.875
aF1.0
aF0.875
aF1.0
aF0.875
aF1.0
aF1.0
aF1.0
aF0.875
aF0.875
aF1.0
aF0.875
aF0.875
aF1.0
aF0.875
aF1.0
aF0.875
aF0.875
aF1.0
aF0.75
aF0.75
aF1.0
aF0.875
aF1.0
aF0.875
aF1.0
aF0.75
aF1.0
aF1.0
aF0.875
aF1.0
aF0.75
aF1.0
aF1.0
aF0.875
aF1.0
aF0.875
aF0.875
aF1.0
aF0.875
aF1.0
aF1.0
aF0.875
aF0.875
aF0.875
aF0.875
aF0.875
aF0.875
aF1.0
aF1.0
aF0.875
aF1.0
aF1.0
aF1.0
aF1.0
aF0.875
aF1.0
aF1.0
aF0.875
aF1.0
aF0.875
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF0.875
aF0.875
aF1.0
aF0.875
aF0.875
aF1.0
aF1.0
aF0.875
aF1.0
aF1.0
aF1.0
aF1.0
aF0.875
aF1.0
aF0.875
aF0.875
aF1.0
aF1.0
aF0.875
aF0.875
aF1.0
aF0.875
aF0.875
aF1.0
aF1.0
aF1.0
aF0.75
aF0.625
aF0.875
aF1.0
aF1.0
aF1.0
aF0.875
aF1.0
aF0.875
aF1.0
aF1.0
aF0.875
aF1.0
aF0.875
aF1.0
aF1.0
aF1.0
aF1.0
aF0.875
aF1.0
aF1.0
aF0.875
aF0.875
aF1.0
aF0.875
aF0.625
aF0.875
aF1.0
aF1.0
aF0.75
aF1.0
aF1.0
aF0.875
aF1.0
aF0.875
aF0.875
aF0.875
aF0.625
aF0.875
aF1.0
aF0.875
aF1.0
aF1.0
aF1.0
aF1.0
aF0.75
aF0.875
aF1.0
aF0.875
aF1.0
aF1.0
aF1.0
aF0.875
aF0.75
aF0.875
aF1.0
aF0.875
aF0.875
aF0.75
aF1.0
aF1.0
aF1.0
aF1.0
aF0.75
aF1.0
aF1.0
aF0.625
aF1.0
aF0.75
aF0.75
aF1.0
aF1.0
aF1.0
aF0.875
aF1.0
aF1.0
aF1.0
aF0.875
aF1.0
aF1.0
aF1.0
aF1.0
aF0.875
aF1.0
aF1.0
aF0.75
aF0.875
aF0.875
aF1.0
aF1.0
aF1.0
aF1.0
aF0.875
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF0.875
aF1.0
aF0.875
aF0.75
aF0.875
aF0.875
aF1.0
aF1.0
aF0.875
aF1.0
aF0.75
aF0.75
aF1.0
aF0.875
aF1.0
aF0.875
aF0.875
aF0.75
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF0.875
aF0.875
aF0.875
aF0.875
aF1.0
aF0.875
aF1.0
aF0.875
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF0.875
aF1.0
aF0.75
aF0.875
aF0.875
aF1.0
aF0.75
aF1.0
aF1.0
aF0.875
aF0.75
aF1.0
aF1.0
aF1.0
aF1.0
aF0.75
aF1.0
aF0.875
aF1.0
aF0.5
aF0.875
aF0.75
aF1.0
aF0.875
aF1.0
aF1.0
aF1.0
aF0.875
aF0.75
aF1.0
aF1.0
aF1.0
aF0.875
aF1.0
aF0.875
aF1.0
aF0.75
aF1.0
aF1.0
aF0.875
aF0.875
aF1.0
aF0.875
aF1.0
aF0.875
aF0.875
aF1.0
aF1.0
aF0.875
aF0.875
aF0.875
aF1.0
aF0.75
aF1.0
aF0.875
aF0.875
aF0.875
aF0.875
aF0.875
aF0.875
aF0.875
aF1.0
aF0.875
aF1.0
aF1.0
aF1.0
aF0.875
aF1.0
aF0.875
aF1.0
aF1.0
aF0.75
aF1.0
aF0.875
aF1.0
aF0.875
aF1.0
aF0.875
aF0.875
aF1.0
aF0.875
aF1.0
aF0.875
aF0.75
aF0.875
aF1.0
aF1.0
aF1.0
aF0.875
aF1.0
aF0.875
aF1.0
aF1.0
aF0.875
aF0.875
aF0.875
aF0.875
aF0.875
aF0.875
aF1.0
aF0.875
aF0.875
aF1.0
aF0.875
aF1.0
aF0.75
aF0.75
aF1.0
aF1.0
aF1.0
aF0.875
aF1.0
aF0.875
aF1.0
aF1.0
aF1.0
aF0.875
aF1.0
aF0.75
aF0.75
aF0.875
aF0.875
aF0.875
aF1.0
aF1.0
aF0.75
aF1.0
aF0.875
aF1.0
aF1.0
aF1.0
aF0.875
aF0.875
aF1.0
aF0.875
aF1.0
aF1.0
aF1.0
aF0.875
aF1.0
aF0.875
aF1.0
aF1.0
aF0.875
aF0.875
aF0.875
aF1.0
aF0.875
aF1.0
aF1.0
aF0.75
aF0.75
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF0.875
aF0.875
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF0.75
aF1.0
aF0.875
aF1.0
aF0.875
aF0.875
aF0.875
aF1.0
aF1.0
aF1.0
aF0.875
aF0.875
aF0.875
aF0.875
aF0.875
aF0.875
aF0.875
aF0.625
aF0.875
aF0.875
aF1.0
aF1.0
aF1.0
aF0.75
aF0.875
aF1.0
aF0.75
aF0.875
aF1.0
aF0.75
aF0.875
aF1.0
aF1.0
aF1.0
aF0.75
aF1.0
aF1.0
aF0.875
aF1.0
aF1.0
aF0.375
aF0.875
aF1.0
aF0.75
aF1.0
aF0.875
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF0.875
aF0.75
aF1.0
aF1.0
aF0.875
aF0.875
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF0.875
aF0.75
aF1.0
aF0.875
aF0.875
aF0.875
aF0.625
aF1.0
aF0.875
aF1.0
aF1.0
aF0.875
aF1.0
aF0.75
aF1.0
aF1.0
aF1.0
aF0.75
aF1.0
aF1.0
aF0.875
aF1.0
aF0.875
aF0.875
aF1.0
aF1.0
aF0.875
aF0.75
aF1.0
aF1.0
aF1.0
aF0.75
aF1.0
aF0.875
aF1.0
aF1.0
aF0.875
aF1.0
aF1.0
aF0.875
aF0.75
aF1.0
aF0.875
aF1.0
aF1.0
aF0.75
aF0.875
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF1.0
aF0.875
aF1.0
aF1.0
aF0.875
aF1.0
aF0.875
aF1.0
aF0.875
aF0.875
aF1.0
aF0.875
aF1.0
aF1.0
aF0.875
aF1.0
aF0.75
aF1.0
aF0.875
aF1.0
aF1.0
aF0.875
aF0.75
aF1.0
aF0.875
aF1.0
aF1.0
aF1.0
aF1.0
aF0.75
aF1.0
aF1.0
aF0.875
aF1.0
aF1.0
aF0.875
aF0.875
aF1.0
aF1.0
aF0.875
aF0.625
aF0.75
aF0.75
aF1.0
aF1.0
aF1.0
aF0.875
aF1.0
aF1.0
aF0.75
aF1.0
aF0.75
aF0.875
aF1.0
aF1.0
aF1.0
aF1.0
aF0.875
aF0.875
aF1.0
aF0.875
aF0.75
aF0.875
aF0.75
aF0.875
aF0.875
aF1.0
aF1.0
aF0.875
aF0.875
aF0.875
aF0.875
aF0.875
aF1.0
aF0.875
aF0.875
aF1.0
aF1.0
aF1.0
aF0.875
aF1.0
aF0.875
aF0.875
aF1.0
aF1.0
aF1.0
aF0.875
aF1.0
aF0.875
aF0.875
aF1.0
aF0.875
aF1.0
aF1.0
aF0.875
aF0.75
aF0.625
aF1.0
aF1.0
aF1.0
aF0.75
aF1.0
aF1.0
aF1.0
aF1.0
aF0.875
aF1.0
aF1.0
aF1.0
aF0.875
aF1.0
aF1.0
aF1.0
aF0.875
aF0.875
aF1.0
aF1.0
aF1.0
aF1.0
aF0.75
aF0.875
aF1.0
aF1.0
aF1.0
aF0.875
aF0.75
aF0.875
aF1.0
aF0.875
aF1.0
aF1.0
aF0.75
aF1.0
aF1.0
aF1.0
aF0.875
aF0.625
aF0.875
aF0.75
aF1.0
aF0.75
aF1.0
aF0.875
aF0.875
aF1.0
aF0.875
aF1.0
aF1.0
aF1.0
aF0.875
aF0.625
aF0.75
aF0.875
aF0.875
aF0.75
aF1.0
aF1.0
aF0.875
aF0.875
aF0.75
aF1.0
aF0.875
aF0.75
aF0.875
aF1.0
aF0.875
aF1.0
aF0.75
aF1.0
aF1.0
aF0.875
aF1.0
aF0.875
aF0.875
aF0.75
atp35
tp36
Rp37
aa(lp38
S'loss'
p39
ag0
(g24
g25
(S'SoftmaxWithLoss'
p40
(lp41
F1.48131
aF1.3824
aF1.3892
aF1.63341
aF1.20765
aF2.13374
aF0.816915
aF1.2161
aF1.58413
aF0.675347
aF0.843468
aF0.592967
aF0.977963
aF0.750159
aF0.431212
aF0.832397
aF0.659158
aF0.536716
aF0.485298
aF0.728338
aF0.691784
aF0.367919
aF0.811112
aF0.625862
aF0.507761
aF0.723358
aF1.29238
aF0.708359
aF0.546374
aF0.737971
aF0.436196
aF0.687048
aF0.330094
aF0.642933
aF0.193362
aF0.526145
aF0.860942
aF0.709738
aF0.713683
aF0.227343
aF0.360479
aF0.844036
aF0.345884
aF0.418117
aF0.192986
aF0.366458
aF0.71524
aF0.727337
aF0.313086
aF0.665948
aF0.577509
aF0.625075
aF0.25207
aF0.353528
aF0.30478
aF0.253411
aF0.73703
aF0.32361
aF0.118638
aF0.718275
aF0.760856
aF0.92545
aF0.284792
aF0.169557
aF0.737892
aF0.358485
aF1.05961
aF0.635596
aF0.534225
aF0.88682
aF1.15503
aF0.469836
aF0.550338
aF0.436444
aF0.239305
aF0.770794
aF1.07415
aF0.27669
aF0.0921887
aF0.900206
aF0.149834
aF0.250253
aF0.467448
aF0.157738
aF0.694421
aF1.04629
aF0.191373
aF0.780563
aF0.863879
aF0.153486
aF0.904084
aF0.212969
aF0.446453
aF0.299106
aF0.219059
aF0.809388
aF0.209261
aF0.473023
aF0.484965
aF0.356629
aF0.286967
aF0.789485
aF0.747291
aF0.500948
aF0.853198
aF0.782663
aF0.51863
aF1.16703
aF0.341752
aF0.593041
aF0.96699
aF0.140431
aF0.26576
aF0.271329
aF0.146447
aF0.27053
aF0.484024
aF0.212047
aF0.0701329
aF0.174029
aF0.343727
aF0.309594
aF0.904061
aF0.431644
aF0.481144
aF0.492536
aF0.144736
aF0.193835
aF0.35315
aF0.371917
aF0.126921
aF0.255847
aF0.648677
aF0.50193
aF0.348635
aF0.875018
aF0.338355
aF0.204639
aF0.292419
aF0.141807
aF0.681849
aF0.32999
aF0.387989
aF0.468488
aF0.464051
aF0.819515
aF0.22548
aF0.262555
aF0.737862
aF0.469361
aF0.441475
aF0.627449
aF0.630612
aF0.220883
aF0.319477
aF0.985252
aF0.296406
aF0.301087
aF0.70383
aF0.854274
aF0.261397
aF0.195904
aF0.24477
aF0.227123
aF0.571261
aF0.0715402
aF0.496243
aF0.325128
aF0.128416
aF0.401054
aF0.608361
aF0.534926
aF0.081674
aF0.623678
aF0.253891
aF0.444945
aF0.651711
aF0.6587
aF0.145646
aF0.242685
aF0.496177
aF0.562042
aF1.04058
aF0.776541
aF0.0674987
aF0.370537
aF0.427024
aF0.529803
aF0.109016
aF0.290044
aF1.04361
aF0.0984626
aF0.59822
aF0.311106
aF0.641487
aF0.201923
aF0.446181
aF0.0894181
aF0.247976
aF0.291902
aF0.161044
aF1.46046
aF0.496375
aF0.589349
aF0.689666
aF0.151257
aF0.784401
aF0.229043
aF0.119648
aF0.339384
aF0.507527
aF0.320268
aF0.390736
aF0.629351
aF1.10165
aF0.548599
aF0.261092
aF0.223021
aF0.218586
aF0.345991
aF0.31286
aF0.136283
aF0.237842
aF0.161279
aF0.278255
aF0.66072
aF0.273505
aF0.11208
aF0.0969782
aF0.337778
aF0.118883
aF0.0891379
aF0.363308
aF0.922762
aF0.463402
aF1.14113
aF0.878451
aF0.375925
aF0.495275
aF0.509286
aF0.311461
aF0.470772
aF0.137821
aF0.282333
aF0.340114
aF0.313405
aF1.21366
aF0.176513
aF0.528469
aF0.156261
aF0.32289
aF0.37986
aF1.14469
aF0.557603
aF0.148303
aF0.871505
aF0.103539
aF0.148141
aF1.05791
aF0.584226
aF0.215495
aF0.407323
aF0.972027
aF0.211017
aF0.627692
aF0.199481
aF0.12167
aF0.376407
aF0.479388
aF0.552426
aF1.22644
aF0.102985
aF0.362324
aF0.106302
aF0.113326
aF0.523337
aF0.0986427
aF1.23798
aF0.434401
aF0.168592
aF0.302352
aF0.400067
aF0.0571366
aF0.26582
aF0.292611
aF0.239988
aF0.633591
aF0.0739725
aF0.133359
aF0.592796
aF0.284717
aF0.272327
aF0.753582
aF0.257839
aF0.0393024
aF1.68803
aF0.154017
aF0.136736
aF0.236299
aF0.0753045
aF0.268524
aF0.169715
aF0.208385
aF0.0890075
aF0.748454
aF0.097273
aF0.234612
aF0.0538339
aF0.078789
aF0.717917
aF0.858873
aF0.546755
aF0.463876
aF0.288986
aF0.222207
aF0.122776
aF0.145181
aF0.110385
aF0.827819
aF0.945363
aF0.424225
aF0.288563
aF0.607369
aF0.170982
aF0.902294
aF0.520293
aF0.270505
aF0.367461
aF0.0644515
aF0.311159
aF0.64252
aF0.0583334
aF0.236257
aF0.194166
aF0.143561
aF0.300142
aF0.432079
aF0.227754
aF1.24591
aF0.0399802
aF0.0833509
aF0.241766
aF0.34249
aF0.155156
aF0.0819547
aF0.442317
aF0.0546582
aF0.133669
aF0.311296
aF0.0837257
aF0.157787
aF0.391699
aF0.0844126
aF0.296597
aF0.399794
aF0.943412
aF0.371828
aF0.121052
aF0.368667
aF0.119534
aF0.166736
aF0.705676
aF0.525588
aF0.174211
aF0.343369
aF0.485397
aF0.249
aF0.122558
aF0.648796
aF0.418631
aF0.293691
aF0.372597
aF0.591975
aF0.344713
aF0.582845
aF0.437718
aF0.65541
aF0.468263
aF0.29254
aF1.65231
aF0.112115
aF0.322595
aF0.248297
aF0.573889
aF0.0936211
aF0.151154
aF0.230906
aF0.353703
aF0.0704857
aF0.126463
aF0.172379
aF0.170785
aF0.490554
aF0.239221
aF0.116123
aF0.970243
aF0.334593
aF0.291761
aF0.139388
aF0.122915
aF0.579825
aF0.426989
aF0.146816
aF0.191951
aF0.817472
aF0.171014
aF0.49393
aF0.193222
aF0.214952
aF0.722192
aF0.102357
aF0.0824588
aF0.0970152
aF0.751643
aF0.0987554
aF0.732246
aF0.113109
aF0.165808
aF0.54887
aF0.156552
aF0.444289
aF0.0984445
aF0.263368
aF0.407751
aF0.277435
aF0.797286
aF0.0856699
aF0.0663048
aF0.0900435
aF0.423915
aF0.336827
aF0.423676
aF0.425352
aF0.305627
aF0.664328
aF0.161549
aF0.23487
aF0.328941
aF0.590315
aF0.133317
aF0.0837907
aF0.145553
aF0.106391
aF0.375807
aF0.278727
aF0.257027
aF0.445584
aF0.498962
aF0.145422
aF0.708761
aF0.305284
aF0.527983
aF0.564528
aF0.594871
aF0.222165
aF0.261033
aF1.43199
aF0.135972
aF0.219176
aF0.526127
aF0.107105
aF0.22597
aF0.225393
aF0.252092
aF0.1724
aF0.358602
aF0.646023
aF0.135986
aF0.12941
aF0.315734
aF0.219659
aF0.460614
aF0.177215
aF0.071282
aF0.0837669
aF0.455916
aF0.424655
aF0.139796
aF0.301042
aF0.278935
aF0.0434804
aF0.128715
aF0.520352
aF0.460486
aF0.247098
aF0.191052
aF0.540629
aF0.199582
aF0.566343
aF0.279128
aF0.791597
aF0.248116
aF0.275216
aF0.446564
aF0.147966
aF0.109127
aF0.183467
aF0.305856
aF0.554056
aF0.465484
aF0.0723792
aF0.314704
aF0.270197
aF0.239174
aF0.26803
aF0.729194
aF0.15953
aF0.211302
aF0.371968
aF0.152437
aF0.178634
aF0.216428
aF0.614845
aF0.116316
aF0.280216
aF0.0420848
aF0.30577
aF0.0672324
aF0.945289
aF0.0777507
aF0.0769981
aF0.206
aF0.199062
aF0.965844
aF0.243788
aF0.948724
aF0.389774
aF0.122644
aF0.167062
aF0.491865
aF0.0553413
aF0.267274
aF0.286361
aF0.525239
aF0.179158
aF1.06827
aF0.341308
aF0.192751
aF0.432396
aF0.501593
aF0.589822
aF0.775564
aF0.446428
aF0.46059
aF0.399548
aF0.141129
aF0.0574178
aF0.152382
aF0.168988
aF0.122235
aF0.223018
aF0.525022
aF0.33049
aF0.554129
aF0.106401
aF0.079575
aF0.21187
aF0.389978
aF0.122457
aF0.084116
aF0.671081
aF0.0285294
aF0.341298
aF0.10504
aF0.641459
aF0.288517
aF0.127158
aF0.162525
aF0.0675201
aF0.123272
aF0.681115
aF0.239839
aF0.191631
aF0.396177
aF0.152999
aF0.13284
aF0.420167
aF0.0546455
aF0.335217
aF0.0225641
aF0.250786
aF1.2034
aF0.0450786
aF0.675665
aF0.148443
aF0.245649
aF0.720729
aF0.117176
aF0.638591
aF0.0594097
aF1.11414
aF0.106848
aF0.0943795
aF0.425946
aF0.877231
aF0.496718
aF0.0506384
aF0.499644
aF0.361295
aF0.101125
aF0.1358
aF0.544198
aF0.730272
aF0.311933
aF0.0195842
aF0.116192
aF0.181614
aF0.0981128
aF0.343534
aF0.0502262
aF0.0930861
aF0.0489401
aF0.0273974
aF0.256853
aF0.0951768
aF0.95257
aF0.0851894
aF0.0475531
aF0.559789
aF0.155401
aF0.57414
aF0.184258
aF0.109904
aF0.0813761
aF0.206671
aF0.280991
aF0.119499
aF0.272008
aF0.225897
aF0.553395
aF0.399122
aF0.324647
aF0.0522756
aF0.861092
aF0.376081
aF0.0450047
aF0.0347809
aF0.501424
aF0.0499464
aF0.626597
aF0.263666
aF0.0444516
aF0.0469043
aF0.326003
aF0.0458338
aF0.204078
aF0.744942
aF0.101616
aF0.15639
aF0.142564
aF0.25858
aF0.128209
aF0.445604
aF0.463769
aF0.236646
aF0.677026
aF0.128593
aF0.171693
aF0.421857
aF0.122179
aF0.0591241
aF0.0453381
aF0.0682817
aF0.0961906
aF0.715773
aF0.309447
aF0.148272
aF0.043156
aF0.430341
aF0.289645
aF0.073201
aF0.606289
aF0.194284
aF0.832267
aF0.198268
aF0.0510204
aF0.118938
aF0.216772
aF0.130504
aF0.0814986
aF0.414216
aF0.0179928
aF0.190221
aF0.119688
aF0.0894549
aF0.0424778
aF0.115045
aF0.0348469
aF0.148384
aF0.375353
aF0.424878
aF0.291517
aF0.139108
aF0.0781012
aF0.030271
aF0.0560846
aF0.381807
aF0.152248
aF0.14268
aF0.344377
aF0.397135
aF0.499163
aF0.0955027
aF0.0750111
aF0.773762
aF0.345103
aF0.139965
aF0.176785
aF0.18441
aF0.203864
aF0.611675
aF0.134079
aF0.115078
aF0.421436
aF0.0741684
aF0.0350738
aF0.855614
aF0.257161
aF0.0817636
aF0.0776374
aF0.0739678
aF0.368714
aF0.0784168
aF0.300828
aF0.204326
aF0.131808
aF0.0527801
aF0.0459435
aF0.0292369
aF0.384661
aF0.0918228
aF0.0372726
aF0.0820938
aF0.284212
aF0.391701
aF0.124697
aF0.643315
aF0.101173
aF0.0962586
aF0.039918
aF0.342127
aF0.471563
aF0.0654137
aF0.10292
aF0.105736
aF0.304348
aF0.0746547
aF0.307571
aF0.189857
aF0.0334789
aF0.668838
aF0.0530891
aF0.0171642
aF0.423036
aF0.833856
aF0.477773
aF0.414613
aF0.130849
aF0.462339
aF0.0808673
aF0.0308074
aF0.627855
aF0.198914
aF0.136167
aF0.299078
aF0.403263
aF0.264839
aF0.156082
aF0.0248938
aF0.130451
aF0.48663
aF0.111267
aF0.773456
aF0.509282
aF0.156531
aF0.196778
aF0.719777
aF0.14571
aF0.364366
aF0.201854
aF0.0637322
aF0.10779
aF0.188621
aF0.51094
aF0.30834
aF0.0892971
aF0.0396224
aF0.166704
aF0.0426416
aF0.0435415
aF0.182869
aF0.0543495
aF0.122071
aF0.084184
aF0.0830754
aF0.20846
aF0.908486
aF0.392332
aF0.0228354
aF0.0647222
aF0.0491463
aF0.141402
aF0.0591319
aF0.0451579
aF0.709004
aF0.031516
aF0.511167
aF0.127322
aF0.135797
aF0.468191
aF0.44315
aF0.163259
aF0.107812
aF0.719836
aF0.0630169
aF0.123136
aF0.0221702
aF0.203847
aF0.181625
aF0.014635
aF0.0492275
aF0.0938431
aF0.0683352
aF0.141837
aF1.01855
aF0.136654
aF0.159867
aF0.148797
aF0.302713
aF1.32088
aF0.263148
aF0.32245
aF0.0498864
aF1.01026
aF0.611959
aF0.0600686
aF0.286312
aF0.0298649
aF0.0517379
aF0.366824
aF0.165743
aF0.084458
aF0.454814
aF0.305249
aF0.322694
aF0.903255
aF0.400551
aF0.0389057
aF0.103387
aF0.0991327
aF0.217944
aF0.0299408
aF0.181232
aF0.838907
aF0.0352566
aF0.114364
aF0.0478785
aF0.224131
aF0.433594
aF0.514781
aF0.112344
aF0.0659749
aF0.0689067
aF0.0571583
aF0.562942
aF0.13712
aF0.18559
aF0.0577578
aF0.131624
aF0.0749735
aF0.637026
aF0.0584467
aF0.190228
aF0.339869
aF0.609782
aF0.173233
aF0.203979
aF0.208564
aF0.0750833
aF0.228676
aF0.282147
aF0.0586587
aF0.0720089
aF0.103932
aF0.0463578
aF0.48796
aF0.707119
aF0.316546
aF0.0531399
aF0.0501684
aF0.152905
aF0.461021
aF0.0441946
aF0.450266
aF0.0790537
aF0.0685837
aF0.0892069
aF0.11995
aF0.0860466
aF0.131589
aF0.543934
aF0.295638
aF0.153296
aF0.72721
aF1.25693
aF0.109614
aF0.112953
aF0.358855
aF0.108539
aF0.205331
aF0.361603
aF0.0691524
aF0.250586
aF0.39574
aF0.150606
aF0.233745
aF0.227512
aF0.0327532
aF0.0793052
aF0.0797306
aF0.141566
aF0.213914
aF0.444671
aF0.396834
aF0.101679
aF0.275055
aF0.0507335
aF0.175752
aF0.618976
aF0.21315
aF0.109218
aF0.0696456
aF0.289092
aF0.170017
aF0.139661
aF0.565412
aF0.276352
aF0.206438
aF0.285921
aF0.0967511
aF0.0694962
aF0.176815
aF0.103612
aF0.331742
aF0.491281
aF0.0516787
aF0.0789862
aF0.351817
aF0.505394
aF0.059618
aF0.151471
aF0.336501
aF0.166556
aF0.531991
aF0.182595
aF1.07647
aF0.427287
aF0.0347476
aF0.200486
aF0.0238464
aF0.144248
aF0.0564956
aF0.200438
aF0.184692
aF0.0654085
aF0.0341569
aF0.186141
aF0.306774
aF0.0771025
aF0.0755726
aF0.308066
aF0.134544
aF0.168144
aF0.298915
aF0.135936
aF0.0552154
aF0.280319
aF0.118023
aF0.41413
aF0.0535439
aF0.535691
aF0.888018
aF0.247464
aF0.0800051
aF0.232911
aF0.984649
aF0.0881342
aF0.290092
aF0.135227
aF0.0216669
aF0.0525057
aF0.519036
aF0.065681
aF0.303661
aF0.038284
aF0.174086
aF0.050812
aF0.609436
aF0.085046
aF0.0429854
aF0.199965
aF0.695841
aF0.395188
aF0.353431
aF0.0817666
aF0.0363934
aF0.0162484
aF0.164149
aF0.319919
aF0.0566295
aF0.950027
aF0.0260141
aF0.318424
aF0.103232
aF0.0242409
aF0.0308989
aF0.0673119
aF0.138002
aF0.0964124
aF0.0689581
aF0.0563548
aF0.114882
aF0.459696
aF0.317231
aF0.510166
aF0.153807
aF0.192589
aF0.0262113
aF0.0480873
aF0.0248683
aF0.124374
aF0.154678
aF0.174427
aF0.120067
aF0.562058
aF0.608088
aF0.100795
aF0.813129
aF0.579731
aF0.104667
aF0.410779
aF0.209247
aF0.292477
aF0.109363
aF0.283646
aF0.0154594
aF0.046076
aF0.243573
aF0.24302
aF0.263246
aF0.412932
aF0.14252
aF0.286507
aF0.142985
aF0.0414302
aF0.0454681
aF0.030629
aF0.0973704
aF0.153558
aF0.668225
aF0.121192
aF0.0222351
aF0.760221
aF0.109023
aF0.13263
aF0.0208669
aF0.058189
aF0.385053
aF0.093043
aF0.0908099
aF0.27274
aF0.282826
aF0.137291
aF0.15491
aF0.0409829
aF0.110123
aF0.0513275
aF0.163445
aF0.352397
aF0.262592
aF0.0281604
aF0.0397436
aF0.0541208
aF0.543874
aF0.165028
aF0.34999
aF0.113566
aF0.270838
aF0.11414
aF0.271021
aF0.0963112
aF0.240828
aF0.330309
aF1.29819
aF0.134002
aF0.251883
aF0.413254
aF0.612267
aF0.0797496
aF0.851255
aF0.110791
aF0.203128
aF0.250395
aF0.0332102
aF0.756973
aF0.556248
aF0.103778
aF0.0483592
aF0.274652
aF0.496589
aF0.116606
aF0.203578
aF0.503615
aF0.0854412
aF0.0494076
aF0.0417765
aF0.605884
aF0.0463525
aF0.658779
aF0.0274014
aF0.0289498
aF0.0373604
aF0.372517
aF0.0526842
aF0.0692984
aF0.0392406
aF0.174219
aF0.146376
aF0.41083
aF0.198875
aF0.185012
aF0.202241
aF0.0605165
aF0.0540905
aF0.121911
aF0.388391
aF0.0812432
aF0.184488
aF0.383652
aF0.0320549
aF1.3743
aF0.396468
aF0.51519
aF0.384166
aF0.059616
aF0.0732471
aF0.22583
aF0.104221
aF0.0924108
aF0.0605445
aF0.10164
aF0.0878428
aF0.0588686
aF0.625356
aF0.0202452
aF0.041201
aF0.269516
aF0.337483
aF0.65574
aF0.350733
aF0.25275
aF0.342304
aF0.735669
aF0.244519
aF0.0902994
aF0.136551
aF0.0791061
aF0.0419671
aF0.473759
aF0.0551437
aF0.0349311
aF0.108558
aF0.362571
aF0.112301
aF0.0165644
aF0.0747361
aF0.153302
aF0.0904756
aF0.0772921
aF0.213339
aF0.019726
aF0.29335
aF0.0296791
aF0.118647
aF0.216232
aF0.0449268
aF0.0601736
aF0.64343
aF0.262098
aF0.0206756
aF0.0357263
aF0.457602
aF0.15466
aF0.0137037
aF0.338796
aF0.232589
aF0.181138
aF0.331872
aF0.212773
aF0.0874476
aF0.136762
aF0.177648
aF0.0408126
aF0.23839
aF0.505836
aF0.359248
aF0.0377731
aF0.426875
aF0.147437
aF0.199958
aF0.0438255
aF0.181776
aF0.148374
aF0.570777
aF0.107624
aF0.0754826
aF0.645535
aF0.826916
aF0.258629
aF0.934861
aF0.256419
aF1.20168
aF0.246102
aF0.518028
aF0.106651
aF0.100905
aF0.514515
aF0.167334
aF0.564014
aF0.0495366
aF0.0607332
aF0.0537956
aF0.0474783
aF0.470723
aF0.249667
aF0.0363832
aF0.417117
aF0.1792
aF0.584545
aF0.120678
aF0.638052
aF0.793699
aF0.256199
aF0.517992
aF0.0762138
aF0.277784
aF0.124808
aF0.0284931
aF0.402546
aF0.585603
aF0.415374
aF0.25594
aF0.0260346
aF0.0598568
aF0.145908
aF0.157241
aF0.170257
aF0.203643
aF0.331191
aF0.141545
aF0.138937
aF0.0189212
aF0.943099
aF0.182311
aF0.112004
aF0.154463
aF0.746983
aF0.117222
aF0.0823515
aF0.353338
aF0.127438
aF0.216908
aF0.0774859
aF0.0468201
aF0.19904
aF0.252903
aF0.204831
aF0.0597553
aF0.355124
aF0.46138
aF0.0967944
aF0.0214346
aF0.0285586
aF0.0447381
aF0.268638
aF0.522754
aF0.071784
aF0.419488
aF0.340944
aF0.0348107
aF0.311612
aF0.0678393
aF0.394103
aF0.1242
aF0.127784
aF0.0854442
aF0.22197
aF0.141599
aF0.232434
aF0.340855
aF0.169638
aF0.0434349
aF0.0671882
aF0.0686852
aF0.193309
aF0.0194867
aF0.158136
aF0.0103742
aF0.251242
aF0.0360617
aF0.0613664
aF0.0888342
aF0.455391
aF0.166933
aF0.130457
aF0.243042
aF0.430666
aF0.0616932
aF0.131705
aF0.0442551
aF0.141824
aF0.154546
aF0.0249724
aF0.269569
aF0.288725
aF0.0249609
aF0.357963
aF0.0735178
aF0.55237
aF0.19852
aF0.339554
aF0.0156736
aF0.0618599
aF0.170215
aF0.151598
aF0.672317
aF0.0874223
aF0.117346
aF0.15272
aF0.0363764
aF0.174749
aF0.286069
aF0.145332
aF0.264253
aF0.0916498
aF0.0545931
aF0.125715
aF0.440814
aF0.256209
aF0.101616
aF0.366674
aF0.56986
aF0.130291
aF0.0504556
aF0.355413
aF0.137887
aF0.0306869
aF0.119558
aF0.110245
aF0.439372
aF0.162891
aF0.073156
aF0.289985
aF0.0263192
aF0.278442
aF0.136102
aF0.042262
aF0.0465908
aF0.0230888
aF0.147073
aF0.045327
aF0.158223
aF0.279322
aF0.019064
aF0.316417
aF0.384942
aF0.0848439
aF0.110136
aF0.284023
aF0.0492742
aF0.0603127
aF0.0488343
aF0.0947289
aF0.169256
aF0.112512
aF0.244446
aF0.18202
aF0.0580653
aF0.0664562
aF0.582075
aF0.243254
aF0.262441
aF0.322507
aF0.491091
aF0.0464194
aF0.067567
aF0.0317282
aF0.495969
aF0.536613
aF0.185609
aF0.129411
aF0.017659
aF0.0788421
aF0.218348
aF0.25644
aF0.149477
aF0.131422
aF0.0803306
aF0.474972
aF0.0941266
aF0.384214
aF0.12544
aF0.0549598
aF0.117267
aF0.025179
aF0.27104
aF0.104359
aF0.0385347
aF0.338118
aF0.246036
aF0.0307125
aF0.191914
aF0.796264
aF0.337734
aF0.0843455
aF0.0325138
aF0.652978
aF0.112825
aF0.0961955
aF0.318555
aF0.0376828
aF0.90406
aF0.229359
aF0.274649
aF1.22677
aF0.456811
aF0.246939
aF0.465882
aF0.0298917
aF0.0570634
aF0.0313518
aF0.0223635
aF0.852617
aF0.235112
aF0.156215
aF0.175683
aF0.0474784
aF0.0623357
aF0.125497
aF0.221739
aF0.638392
aF0.215157
aF0.0908367
aF0.351464
aF0.304651
aF0.351105
aF0.0982204
aF0.118168
aF0.0616482
aF0.127412
aF0.527265
aF0.101404
aF0.0203762
aF1.01663
aF0.151658
aF0.582504
aF0.51132
aF0.0594867
aF0.0815692
aF0.0947146
aF0.131346
aF0.0276266
aF0.0273511
aF0.0490676
aF0.320111
aF0.0180461
aF0.0874564
aF0.0362731
aF0.0821483
aF0.361242
aF0.103874
aF0.0595004
aF0.352275
aF0.705967
aF0.171522
aF0.141689
aF0.0863092
aF0.0375659
aF0.0444443
aF0.215213
aF0.039921
aF0.0837441
aF0.05826
aF0.161507
aF0.137938
aF0.0889375
aF0.0939432
aF0.123863
aF0.0350802
aF0.282905
aF0.249822
aF0.440911
aF0.310886
aF0.0743823
aF0.0547643
aF0.261015
aF0.0397203
aF0.530984
aF0.438478
aF0.0887049
aF0.163765
aF0.0733073
aF0.737841
aF0.294023
aF0.941284
aF0.092521
aF0.0385731
aF0.0900486
aF0.054751
aF0.108982
aF0.409832
aF0.182275
aF0.454473
aF0.505308
aF0.131048
aF0.591549
aF0.0656057
aF0.241863
aF0.213445
aF0.0848804
aF0.0590973
aF0.0468378
aF0.0584165
aF0.233369
aF0.0644484
aF0.394897
aF0.210304
aF0.139373
aF0.0993978
aF0.458695
aF0.0244488
aF0.0344481
aF0.281949
aF0.594517
aF0.0636532
aF0.0313544
aF0.0134726
aF0.0366691
aF0.404789
aF0.0389201
aF0.260808
aF0.163283
aF1.28456
aF0.117693
aF0.2717
aF0.0764344
aF0.227795
aF0.0259896
aF0.029552
aF0.0455123
aF0.287933
aF0.614376
aF0.156368
aF0.100112
aF0.0731098
aF0.158191
aF0.0739746
aF0.297855
aF0.0195873
aF0.455478
aF0.0782685
aF0.129311
aF0.160563
aF0.501476
aF0.0167473
aF0.317759
aF0.0536972
aF0.397153
aF0.227954
aF0.0683147
aF0.0331738
aF0.214035
aF0.196336
aF0.116052
aF0.0331314
aF0.691683
aF0.0815136
aF0.180433
aF0.41441
aF0.355246
aF0.222807
aF0.358907
aF0.297317
aF0.196468
aF0.0509626
aF0.595282
aF0.108428
aF0.0161103
aF0.0547154
aF0.25205
aF0.0153397
aF0.210135
aF0.162274
aF0.0783899
aF0.621465
aF0.109569
aF0.217976
aF0.0825081
aF0.829408
aF0.0914029
aF0.517621
aF0.444464
aF0.0713863
aF0.312902
aF0.141051
aF0.192553
aF0.660368
aF0.118996
aF0.236084
aF0.0470043
aF0.0769532
aF0.129725
aF0.311082
aF0.270442
aF0.0468205
aF0.0128673
aF0.483316
aF0.296229
aF0.271322
aF0.483119
aF0.275398
aF0.404979
aF0.114911
aF0.311156
aF0.336001
aF0.0330651
aF0.228044
aF0.0174693
aF0.673089
aF0.565641
aF0.0455216
aF0.0089714
aF0.0900632
aF0.353457
aF0.0286918
aF0.476343
aF0.0492265
aF0.157813
aF0.0724839
aF0.44577
aF0.0640108
aF0.97081
aF0.580571
aF0.315836
aF0.214071
aF0.499661
aF0.145688
aF0.0942782
aF0.734869
aF0.0558973
aF0.17569
aF0.176231
aF0.0947945
aF0.145709
aF0.590569
aF0.202952
aF0.169577
aF0.2042
aF0.0972194
aF0.217162
aF0.110324
aF0.334026
aF0.137395
aF0.121491
aF0.177942
aF0.0616696
aF0.358651
aF0.213654
aF0.236955
aF0.127779
aF0.525109
aF0.0424409
aF0.063472
aF0.734652
aF0.641128
aF0.106454
aF0.0385297
aF0.0483146
aF0.073946
aF0.00779668
aF0.0668729
aF0.0665242
aF0.083878
aF0.412938
aF0.381182
aF0.0771623
aF0.0425103
aF0.0753442
aF0.0952567
aF0.05812
aF0.0640139
aF0.228256
aF0.0430081
aF0.092284
aF0.880518
aF0.0302757
aF0.344976
aF0.046296
aF0.274506
aF0.507441
aF0.274417
aF0.0975319
aF0.0450798
aF0.0389461
aF0.37824
aF0.236109
aF0.204379
aF0.701975
aF0.197872
aF0.343057
aF0.187172
aF1.01905
aF0.316318
aF0.222206
aF0.0291627
aF0.13226
aF0.0254704
aF0.797088
aF0.256813
aF0.0791869
aF0.467301
aF0.476424
aF0.0859004
aF0.74044
aF0.253224
aF0.0569964
aF0.0716724
aF0.100255
aF0.377169
aF0.0261297
aF0.0437603
aF0.222543
aF0.0390223
aF0.080189
aF1.30031
aF0.225922
aF0.0336552
aF0.755309
aF0.0601744
aF0.225836
aF0.0734392
aF0.0286957
aF0.0562953
aF0.142578
aF0.0200894
aF0.0526132
aF0.0587472
aF0.495836
aF0.635046
aF0.0311668
aF0.169179
aF0.543258
aF0.140312
aF0.0906247
aF0.120769
aF0.101225
aF0.100382
aF0.0663017
aF0.543418
aF0.497213
aF0.0286474
aF0.146247
aF0.274951
aF0.191707
aF0.533054
aF0.079272
aF0.28068
aF0.0993535
aF0.0419064
aF0.211504
aF0.0263864
aF0.630306
aF0.161848
aF0.136807
aF0.0320775
aF0.838929
aF0.0658141
aF0.0928958
aF0.185394
aF0.0696422
aF0.221003
aF0.223297
aF0.0122367
aF0.0613417
aF0.130033
aF0.49983
aF0.203906
aF0.111715
aF0.0237714
aF0.735909
aF0.0689035
aF0.214297
aF0.0551949
aF0.0493654
aF0.247769
aF0.0200672
aF0.201601
aF0.471813
aF1.21653
aF0.129268
aF0.324543
aF0.0187789
aF0.0994941
aF0.482544
aF0.223673
aF0.153555
aF0.0192659
aF0.0874152
aF0.0290786
aF0.0370732
aF0.0200534
aF0.0682101
aF0.270993
aF0.0315824
aF0.0233546
aF0.360143
aF0.00904707
aF0.513696
aF0.0594907
aF0.313611
aF0.209791
aF0.0222544
aF0.306295
aF0.0725252
aF0.0277507
aF0.197164
aF0.0152782
aF0.642962
aF0.060452
aF0.555847
aF0.0645247
aF0.0231121
aF0.204696
aF0.723944
aF0.0282487
aF0.28508
aF0.135728
aF0.0784603
aF0.0246688
aF0.0893682
aF0.24836
aF0.0901683
aF0.0215076
aF0.420812
aF0.0295775
aF0.116813
aF0.179812
aF0.395667
aF0.0173249
aF0.0309815
aF0.555773
aF0.473441
aF0.585773
aF0.703901
aF0.020648
aF0.137698
aF0.0302222
aF0.342158
aF0.0952014
aF0.049789
aF0.323626
aF0.052712
aF0.417005
aF1.0371
aF0.0576066
aF0.0862781
aF0.156832
aF0.108429
aF0.196795
aF0.369441
aF0.0496106
aF0.491621
aF1.0846
aF0.248029
aF0.461451
aF0.226361
aF0.472295
aF0.035772
aF0.038821
aF0.204183
aF0.319899
aF0.149781
aF0.794781
aF0.138939
aF0.065723
aF0.204665
aF1.06509
aF0.0183802
aF0.0280353
aF0.0340776
aF0.193855
aF0.0179618
aF0.154759
aF0.12997
aF0.121245
aF0.048037
aF0.0517332
aF0.175705
aF0.0472363
aF0.301078
aF1.04381
aF0.0425865
aF0.146938
aF0.0407033
aF0.163443
aF0.610415
aF0.447541
aF0.680122
aF0.0399223
aF0.0995664
aF0.0576907
aF0.474933
aF0.0206284
aF0.0495206
aF0.018082
aF0.0294661
aF0.301119
aF0.0789812
aF0.0764886
aF0.0595731
aF0.403574
aF0.0386627
aF0.120065
aF0.0535416
aF0.225544
aF0.345387
aF0.0380695
aF0.143163
aF0.0247435
aF0.0418447
aF0.842919
aF0.235647
aF0.038093
aF0.0341169
aF0.0960361
aF0.18881
aF0.277494
aF0.46967
aF0.074231
aF0.177058
aF0.0658334
aF0.018372
aF0.433639
aF0.0584434
aF0.0850722
aF0.197736
aF0.219684
aF0.491597
aF0.699131
aF0.882188
aF0.0779144
aF0.71781
aF0.058028
aF0.248173
aF0.272848
aF0.0355193
aF0.27172
aF0.0203066
aF0.220345
aF0.0807978
aF0.209712
aF1.40487
aF0.306268
aF0.281635
aF0.243982
aF0.786004
aF0.0377588
aF0.0606058
aF0.195599
aF0.179607
aF0.429885
aF0.152338
aF0.204438
aF0.330168
aF0.635099
aF0.113284
aF0.239194
aF0.0587632
aF0.260958
aF0.0446119
aF0.0211338
aF0.772633
aF0.151849
aF0.13616
aF0.200703
aF0.317392
atp42
tp43
Rp44
aa(lp45
S'learning_rate'
p46
ag0
(g24
g25
(S'LearningRate'
p47
(lp48
F0.01
aNaNaF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.01
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF1e-05
aF1e-05
aF1e-05
aF1e-05
aF1e-05
aF1e-05
aF1e-05
aF1e-05
aF1e-05
aF1e-05
aF1e-05
aF1e-05
aF1e-05
aF1e-05
aF1e-05
aF1e-05
aF1e-05
aF1e-05
aF1e-05
aF1e-05
atp49
tp50
Rp51
aatp52
Rp53
sS'val_outputs'
p54
g20
((lp55
(lp56
g23
ag0
(g24
g25
(g26
(lp57
I0
aF1.0
aF2.0
aF3.0
aF4.0
aF5.0
aF6.0
aF7.0
aF8.0
aF9.0
aF10.0
aF11.0
aF12.0
aF13.0
aF14.0
aF15.0
aF16.0
aF17.0
aF18.0
aF19.0
aF20.0
aF21.0
aF22.0
aF23.0
aF24.0
aF25.0
aF26.0
aF27.0
aF28.0
aF29.0
aF30.0
atp58
tp59
Rp60
aa(lp61
S'accuracy'
p62
ag0
(g24
g25
(S'Accuracy'
p63
(lp64
F0.5
aF0.763889
aF0.798611
aF0.8125
aF0.805556
aF0.819444
aF0.805556
aF0.847222
aF0.819444
aF0.854167
aF0.854167
aF0.875
aF0.868056
aF0.861111
aF0.847222
aF0.868056
aF0.854167
aF0.868056
aF0.854167
aF0.875
aF0.854167
aF0.861111
aF0.868056
aF0.861111
aF0.868056
aF0.868056
aF0.868056
aF0.868056
aF0.861111
aF0.868056
aF0.868056
atp65
tp66
Rp67
aa(lp68
S'loss'
p69
ag0
(g24
g25
(S'SoftmaxWithLoss'
p70
(lp71
F87.3365
aF0.666139
aF0.499721
aF0.517228
aF0.49572
aF0.516059
aF0.439145
aF0.50365
aF0.430273
aF0.386714
aF0.374929
aF0.358816
aF0.375713
aF0.366517
aF0.391699
aF0.340489
aF0.370673
aF0.370698
aF0.403393
aF0.370169
aF0.367309
aF0.354577
aF0.354666
aF0.36756
aF0.359541
aF0.357039
aF0.361674
aF0.35758
aF0.376048
aF0.356995
aF0.377573
atp72
tp73
Rp74
aatp75
Rp76
sS'loaded_snapshot_epoch'
p77
NsS'pretrained_model'
p78
V
p79
sS'deploy_file'
p80
S'deploy.prototxt'
p81
sS'current_iteration'
p82
I157380
sS'gpu_count'
p83
I1
sS'rms_decay'
p84
F0.99
sS'train_epochs'
p85
I30
sS'network'
p86
ccaffe_pb2
NetParameter
p87
(tRp88
(dp89
S'serialized'
p90
S'\n\x10DENSENET_cifar10\xa2\x064\n\ntrain-data\x12\x04Data"\x04data"\x05labelB\x07"\x05train\xa2\x06\x04\x10\x01\x18 \xda\x06\x02  \xa2\x060\n\x08val-data\x12\x04Data"\x04data"\x05labelB\x05"\x03val\xa2\x06\x04\x10\x00\x18 \xda\x06\x02  \xa2\x06P\n\x0cConvolution1\x12\x0bConvolution\x1a\x04data"\x0cConvolution1\xd2\x06\x1e\x08\x10\x10\x00\x18\x01 \x030\x01:\x06\n\x04msraB\n\n\x08constant\xa2\x06U\n\nBatchNorm1\x12\tBatchNorm\x1a\x0cConvolution1"\nBatchNorm12\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x00\xa2\x06:\n\x06Scale1\x12\x05Scale\x1a\nBatchNorm1"\nBatchNorm1\xf2\x08\x10\x1a\x05\x15\x00\x00\x80? \x01*\x05\x15\x00\x00\x00\x00\xa2\x06%\n\x05ReLU1\x12\x04ReLU\x1a\nBatchNorm1"\nBatchNorm1\xa2\x06V\n\x0cConvolution2\x12\x0bConvolution\x1a\nBatchNorm1"\x0cConvolution2\xd2\x06\x1e\x08\x0c\x10\x00\x18\x01 \x030\x01:\x06\n\x04msraB\n\n\x08constant\xa2\x063\n\x08Dropout1\x12\x07Dropout\x1a\x0cConvolution2"\x08Dropout1\xe2\x06\x05\r\xcd\xccL>\xa2\x067\n\x07Concat1\x12\x06Concat\x1a\x0cConvolution1\x1a\x08Dropout1"\x07Concat1\xc2\x06\x02\x10\x01\xa2\x06P\n\nBatchNorm2\x12\tBatchNorm\x1a\x07Concat1"\nBatchNorm22\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x00\xa2\x06:\n\x06Scale2\x12\x05Scale\x1a\nBatchNorm2"\nBatchNorm2\xf2\x08\x10\x1a\x05\x15\x00\x00\x80? \x01*\x05\x15\x00\x00\x00\x00\xa2\x06%\n\x05ReLU2\x12\x04ReLU\x1a\nBatchNorm2"\nBatchNorm2\xa2\x06V\n\x0cConvolution3\x12\x0bConvolution\x1a\nBatchNorm2"\x0cConvolution3\xd2\x06\x1e\x08\x0c\x10\x00\x18\x01 \x030\x01:\x06\n\x04msraB\n\n\x08constant\xa2\x063\n\x08Dropout2\x12\x07Dropout\x1a\x0cConvolution3"\x08Dropout2\xe2\x06\x05\r\xcd\xccL>\xa2\x062\n\x07Concat2\x12\x06Concat\x1a\x07Concat1\x1a\x08Dropout2"\x07Concat2\xc2\x06\x02\x10\x01\xa2\x06P\n\nBatchNorm3\x12\tBatchNorm\x1a\x07Concat2"\nBatchNorm32\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x00\xa2\x06:\n\x06Scale3\x12\x05Scale\x1a\nBatchNorm3"\nBatchNorm3\xf2\x08\x10\x1a\x05\x15\x00\x00\x80? \x01*\x05\x15\x00\x00\x00\x00\xa2\x06%\n\x05ReLU3\x12\x04ReLU\x1a\nBatchNorm3"\nBatchNorm3\xa2\x06V\n\x0cConvolution4\x12\x0bConvolution\x1a\nBatchNorm3"\x0cConvolution4\xd2\x06\x1e\x08\x0c\x10\x00\x18\x01 \x030\x01:\x06\n\x04msraB\n\n\x08constant\xa2\x063\n\x08Dropout3\x12\x07Dropout\x1a\x0cConvolution4"\x08Dropout3\xe2\x06\x05\r\xcd\xccL>\xa2\x062\n\x07Concat3\x12\x06Concat\x1a\x07Concat2\x1a\x08Dropout3"\x07Concat3\xc2\x06\x02\x10\x01\xa2\x06P\n\nBatchNorm4\x12\tBatchNorm\x1a\x07Concat3"\nBatchNorm42\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x00\xa2\x06:\n\x06Scale4\x12\x05Scale\x1a\nBatchNorm4"\nBatchNorm4\xf2\x08\x10\x1a\x05\x15\x00\x00\x80? \x01*\x05\x15\x00\x00\x00\x00\xa2\x06%\n\x05ReLU4\x12\x04ReLU\x1a\nBatchNorm4"\nBatchNorm4\xa2\x06V\n\x0cConvolution5\x12\x0bConvolution\x1a\nBatchNorm4"\x0cConvolution5\xd2\x06\x1e\x08\x0c\x10\x00\x18\x01 \x030\x01:\x06\n\x04msraB\n\n\x08constant\xa2\x063\n\x08Dropout4\x12\x07Dropout\x1a\x0cConvolution5"\x08Dropout4\xe2\x06\x05\r\xcd\xccL>\xa2\x062\n\x07Concat4\x12\x06Concat\x1a\x07Concat3\x1a\x08Dropout4"\x07Concat4\xc2\x06\x02\x10\x01\xa2\x06P\n\nBatchNorm5\x12\tBatchNorm\x1a\x07Concat4"\nBatchNorm52\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x00\xa2\x06:\n\x06Scale5\x12\x05Scale\x1a\nBatchNorm5"\nBatchNorm5\xf2\x08\x10\x1a\x05\x15\x00\x00\x80? \x01*\x05\x15\x00\x00\x00\x00\xa2\x06%\n\x05ReLU5\x12\x04ReLU\x1a\nBatchNorm5"\nBatchNorm5\xa2\x06V\n\x0cConvolution6\x12\x0bConvolution\x1a\nBatchNorm5"\x0cConvolution6\xd2\x06\x1e\x08\x0c\x10\x00\x18\x01 \x030\x01:\x06\n\x04msraB\n\n\x08constant\xa2\x063\n\x08Dropout5\x12\x07Dropout\x1a\x0cConvolution6"\x08Dropout5\xe2\x06\x05\r\xcd\xccL>\xa2\x062\n\x07Concat5\x12\x06Concat\x1a\x07Concat4\x1a\x08Dropout5"\x07Concat5\xc2\x06\x02\x10\x01\xa2\x06P\n\nBatchNorm6\x12\tBatchNorm\x1a\x07Concat5"\nBatchNorm62\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x00\xa2\x06:\n\x06Scale6\x12\x05Scale\x1a\nBatchNorm6"\nBatchNorm6\xf2\x08\x10\x1a\x05\x15\x00\x00\x80? \x01*\x05\x15\x00\x00\x00\x00\xa2\x06%\n\x05ReLU6\x12\x04ReLU\x1a\nBatchNorm6"\nBatchNorm6\xa2\x06V\n\x0cConvolution7\x12\x0bConvolution\x1a\nBatchNorm6"\x0cConvolution7\xd2\x06\x1e\x08\x0c\x10\x00\x18\x01 \x030\x01:\x06\n\x04msraB\n\n\x08constant\xa2\x063\n\x08Dropout6\x12\x07Dropout\x1a\x0cConvolution7"\x08Dropout6\xe2\x06\x05\r\xcd\xccL>\xa2\x062\n\x07Concat6\x12\x06Concat\x1a\x07Concat5\x1a\x08Dropout6"\x07Concat6\xc2\x06\x02\x10\x01\xa2\x06P\n\nBatchNorm7\x12\tBatchNorm\x1a\x07Concat6"\nBatchNorm72\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x00\xa2\x06:\n\x06Scale7\x12\x05Scale\x1a\nBatchNorm7"\nBatchNorm7\xf2\x08\x10\x1a\x05\x15\x00\x00\x80? \x01*\x05\x15\x00\x00\x00\x00\xa2\x06%\n\x05ReLU7\x12\x04ReLU\x1a\nBatchNorm7"\nBatchNorm7\xa2\x06V\n\x0cConvolution8\x12\x0bConvolution\x1a\nBatchNorm7"\x0cConvolution8\xd2\x06\x1e\x08\x0c\x10\x00\x18\x01 \x030\x01:\x06\n\x04msraB\n\n\x08constant\xa2\x063\n\x08Dropout7\x12\x07Dropout\x1a\x0cConvolution8"\x08Dropout7\xe2\x06\x05\r\xcd\xccL>\xa2\x062\n\x07Concat7\x12\x06Concat\x1a\x07Concat6\x1a\x08Dropout7"\x07Concat7\xc2\x06\x02\x10\x01\xa2\x06P\n\nBatchNorm8\x12\tBatchNorm\x1a\x07Concat7"\nBatchNorm82\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x00\xa2\x06:\n\x06Scale8\x12\x05Scale\x1a\nBatchNorm8"\nBatchNorm8\xf2\x08\x10\x1a\x05\x15\x00\x00\x80? \x01*\x05\x15\x00\x00\x00\x00\xa2\x06%\n\x05ReLU8\x12\x04ReLU\x1a\nBatchNorm8"\nBatchNorm8\xa2\x06V\n\x0cConvolution9\x12\x0bConvolution\x1a\nBatchNorm8"\x0cConvolution9\xd2\x06\x1e\x08\x0c\x10\x00\x18\x01 \x030\x01:\x06\n\x04msraB\n\n\x08constant\xa2\x063\n\x08Dropout8\x12\x07Dropout\x1a\x0cConvolution9"\x08Dropout8\xe2\x06\x05\r\xcd\xccL>\xa2\x062\n\x07Concat8\x12\x06Concat\x1a\x07Concat7\x1a\x08Dropout8"\x07Concat8\xc2\x06\x02\x10\x01\xa2\x06P\n\nBatchNorm9\x12\tBatchNorm\x1a\x07Concat8"\nBatchNorm92\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x00\xa2\x06:\n\x06Scale9\x12\x05Scale\x1a\nBatchNorm9"\nBatchNorm9\xf2\x08\x10\x1a\x05\x15\x00\x00\x80? \x01*\x05\x15\x00\x00\x00\x00\xa2\x06%\n\x05ReLU9\x12\x04ReLU\x1a\nBatchNorm9"\nBatchNorm9\xa2\x06X\n\rConvolution10\x12\x0bConvolution\x1a\nBatchNorm9"\rConvolution10\xd2\x06\x1e\x08\x0c\x10\x00\x18\x01 \x030\x01:\x06\n\x04msraB\n\n\x08constant\xa2\x064\n\x08Dropout9\x12\x07Dropout\x1a\rConvolution10"\x08Dropout9\xe2\x06\x05\r\xcd\xccL>\xa2\x062\n\x07Concat9\x12\x06Concat\x1a\x07Concat8\x1a\x08Dropout9"\x07Concat9\xc2\x06\x02\x10\x01\xa2\x06R\n\x0bBatchNorm10\x12\tBatchNorm\x1a\x07Concat9"\x0bBatchNorm102\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x00\xa2\x06=\n\x07Scale10\x12\x05Scale\x1a\x0bBatchNorm10"\x0bBatchNorm10\xf2\x08\x10\x1a\x05\x15\x00\x00\x80? \x01*\x05\x15\x00\x00\x00\x00\xa2\x06(\n\x06ReLU10\x12\x04ReLU\x1a\x0bBatchNorm10"\x0bBatchNorm10\xa2\x06Y\n\rConvolution11\x12\x0bConvolution\x1a\x0bBatchNorm10"\rConvolution11\xd2\x06\x1e\x08\x0c\x10\x00\x18\x01 \x030\x01:\x06\n\x04msraB\n\n\x08constant\xa2\x066\n\tDropout10\x12\x07Dropout\x1a\rConvolution11"\tDropout10\xe2\x06\x05\r\xcd\xccL>\xa2\x065\n\x08Concat10\x12\x06Concat\x1a\x07Concat9\x1a\tDropout10"\x08Concat10\xc2\x06\x02\x10\x01\xa2\x06S\n\x0bBatchNorm11\x12\tBatchNorm\x1a\x08Concat10"\x0bBatchNorm112\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x00\xa2\x06=\n\x07Scale11\x12\x05Scale\x1a\x0bBatchNorm11"\x0bBatchNorm11\xf2\x08\x10\x1a\x05\x15\x00\x00\x80? \x01*\x05\x15\x00\x00\x00\x00\xa2\x06(\n\x06ReLU11\x12\x04ReLU\x1a\x0bBatchNorm11"\x0bBatchNorm11\xa2\x06Y\n\rConvolution12\x12\x0bConvolution\x1a\x0bBatchNorm11"\rConvolution12\xd2\x06\x1e\x08\x0c\x10\x00\x18\x01 \x030\x01:\x06\n\x04msraB\n\n\x08constant\xa2\x066\n\tDropout11\x12\x07Dropout\x1a\rConvolution12"\tDropout11\xe2\x06\x05\r\xcd\xccL>\xa2\x066\n\x08Concat11\x12\x06Concat\x1a\x08Concat10\x1a\tDropout11"\x08Concat11\xc2\x06\x02\x10\x01\xa2\x06S\n\x0bBatchNorm12\x12\tBatchNorm\x1a\x08Concat11"\x0bBatchNorm122\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x00\xa2\x06=\n\x07Scale12\x12\x05Scale\x1a\x0bBatchNorm12"\x0bBatchNorm12\xf2\x08\x10\x1a\x05\x15\x00\x00\x80? \x01*\x05\x15\x00\x00\x00\x00\xa2\x06(\n\x06ReLU12\x12\x04ReLU\x1a\x0bBatchNorm12"\x0bBatchNorm12\xa2\x06Y\n\rConvolution13\x12\x0bConvolution\x1a\x0bBatchNorm12"\rConvolution13\xd2\x06\x1e\x08\x0c\x10\x00\x18\x01 \x030\x01:\x06\n\x04msraB\n\n\x08constant\xa2\x066\n\tDropout12\x12\x07Dropout\x1a\rConvolution13"\tDropout12\xe2\x06\x05\r\xcd\xccL>\xa2\x066\n\x08Concat12\x12\x06Concat\x1a\x08Concat11\x1a\tDropout12"\x08Concat12\xc2\x06\x02\x10\x01\xa2\x06S\n\x0bBatchNorm13\x12\tBatchNorm\x1a\x08Concat12"\x0bBatchNorm132\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x00\xa2\x06=\n\x07Scale13\x12\x05Scale\x1a\x0bBatchNorm13"\x0bBatchNorm13\xf2\x08\x10\x1a\x05\x15\x00\x00\x80? \x01*\x05\x15\x00\x00\x00\x00\xa2\x06(\n\x06ReLU13\x12\x04ReLU\x1a\x0bBatchNorm13"\x0bBatchNorm13\xa2\x06Z\n\rConvolution14\x12\x0bConvolution\x1a\x0bBatchNorm13"\rConvolution14\xd2\x06\x1f\x08\xa0\x01\x10\x00\x18\x00 \x010\x01:\x06\n\x04msraB\n\n\x08constant\xa2\x066\n\tDropout13\x12\x07Dropout\x1a\rConvolution14"\tDropout13\xe2\x06\x05\r\xcd\xccL>\xa2\x061\n\x08Pooling1\x12\x07Pooling\x1a\tDropout13"\x08Pooling1\xca\x07\x06\x08\x01\x10\x02\x18\x02\xa2\x06S\n\x0bBatchNorm14\x12\tBatchNorm\x1a\x08Pooling1"\x0bBatchNorm142\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x00\xa2\x06=\n\x07Scale14\x12\x05Scale\x1a\x0bBatchNorm14"\x0bBatchNorm14\xf2\x08\x10\x1a\x05\x15\x00\x00\x80? \x01*\x05\x15\x00\x00\x00\x00\xa2\x06(\n\x06ReLU14\x12\x04ReLU\x1a\x0bBatchNorm14"\x0bBatchNorm14\xa2\x06Y\n\rConvolution15\x12\x0bConvolution\x1a\x0bBatchNorm14"\rConvolution15\xd2\x06\x1e\x08\x0c\x10\x00\x18\x01 \x030\x01:\x06\n\x04msraB\n\n\x08constant\xa2\x066\n\tDropout14\x12\x07Dropout\x1a\rConvolution15"\tDropout14\xe2\x06\x05\r\xcd\xccL>\xa2\x066\n\x08Concat13\x12\x06Concat\x1a\x08Pooling1\x1a\tDropout14"\x08Concat13\xc2\x06\x02\x10\x01\xa2\x06S\n\x0bBatchNorm15\x12\tBatchNorm\x1a\x08Concat13"\x0bBatchNorm152\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x00\xa2\x06=\n\x07Scale15\x12\x05Scale\x1a\x0bBatchNorm15"\x0bBatchNorm15\xf2\x08\x10\x1a\x05\x15\x00\x00\x80? \x01*\x05\x15\x00\x00\x00\x00\xa2\x06(\n\x06ReLU15\x12\x04ReLU\x1a\x0bBatchNorm15"\x0bBatchNorm15\xa2\x06Y\n\rConvolution16\x12\x0bConvolution\x1a\x0bBatchNorm15"\rConvolution16\xd2\x06\x1e\x08\x0c\x10\x00\x18\x01 \x030\x01:\x06\n\x04msraB\n\n\x08constant\xa2\x066\n\tDropout15\x12\x07Dropout\x1a\rConvolution16"\tDropout15\xe2\x06\x05\r\xcd\xccL>\xa2\x066\n\x08Concat14\x12\x06Concat\x1a\x08Concat13\x1a\tDropout15"\x08Concat14\xc2\x06\x02\x10\x01\xa2\x06S\n\x0bBatchNorm16\x12\tBatchNorm\x1a\x08Concat14"\x0bBatchNorm162\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x00\xa2\x06=\n\x07Scale16\x12\x05Scale\x1a\x0bBatchNorm16"\x0bBatchNorm16\xf2\x08\x10\x1a\x05\x15\x00\x00\x80? \x01*\x05\x15\x00\x00\x00\x00\xa2\x06(\n\x06ReLU16\x12\x04ReLU\x1a\x0bBatchNorm16"\x0bBatchNorm16\xa2\x06Y\n\rConvolution17\x12\x0bConvolution\x1a\x0bBatchNorm16"\rConvolution17\xd2\x06\x1e\x08\x0c\x10\x00\x18\x01 \x030\x01:\x06\n\x04msraB\n\n\x08constant\xa2\x066\n\tDropout16\x12\x07Dropout\x1a\rConvolution17"\tDropout16\xe2\x06\x05\r\xcd\xccL>\xa2\x066\n\x08Concat15\x12\x06Concat\x1a\x08Concat14\x1a\tDropout16"\x08Concat15\xc2\x06\x02\x10\x01\xa2\x06S\n\x0bBatchNorm17\x12\tBatchNorm\x1a\x08Concat15"\x0bBatchNorm172\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x00\xa2\x06=\n\x07Scale17\x12\x05Scale\x1a\x0bBatchNorm17"\x0bBatchNorm17\xf2\x08\x10\x1a\x05\x15\x00\x00\x80? \x01*\x05\x15\x00\x00\x00\x00\xa2\x06(\n\x06ReLU17\x12\x04ReLU\x1a\x0bBatchNorm17"\x0bBatchNorm17\xa2\x06Y\n\rConvolution18\x12\x0bConvolution\x1a\x0bBatchNorm17"\rConvolution18\xd2\x06\x1e\x08\x0c\x10\x00\x18\x01 \x030\x01:\x06\n\x04msraB\n\n\x08constant\xa2\x066\n\tDropout17\x12\x07Dropout\x1a\rConvolution18"\tDropout17\xe2\x06\x05\r\xcd\xccL>\xa2\x066\n\x08Concat16\x12\x06Concat\x1a\x08Concat15\x1a\tDropout17"\x08Concat16\xc2\x06\x02\x10\x01\xa2\x06S\n\x0bBatchNorm18\x12\tBatchNorm\x1a\x08Concat16"\x0bBatchNorm182\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x00\xa2\x06=\n\x07Scale18\x12\x05Scale\x1a\x0bBatchNorm18"\x0bBatchNorm18\xf2\x08\x10\x1a\x05\x15\x00\x00\x80? \x01*\x05\x15\x00\x00\x00\x00\xa2\x06(\n\x06ReLU18\x12\x04ReLU\x1a\x0bBatchNorm18"\x0bBatchNorm18\xa2\x06Y\n\rConvolution19\x12\x0bConvolution\x1a\x0bBatchNorm18"\rConvolution19\xd2\x06\x1e\x08\x0c\x10\x00\x18\x01 \x030\x01:\x06\n\x04msraB\n\n\x08constant\xa2\x066\n\tDropout18\x12\x07Dropout\x1a\rConvolution19"\tDropout18\xe2\x06\x05\r\xcd\xccL>\xa2\x066\n\x08Concat17\x12\x06Concat\x1a\x08Concat16\x1a\tDropout18"\x08Concat17\xc2\x06\x02\x10\x01\xa2\x06S\n\x0bBatchNorm19\x12\tBatchNorm\x1a\x08Concat17"\x0bBatchNorm192\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x00\xa2\x06=\n\x07Scale19\x12\x05Scale\x1a\x0bBatchNorm19"\x0bBatchNorm19\xf2\x08\x10\x1a\x05\x15\x00\x00\x80? \x01*\x05\x15\x00\x00\x00\x00\xa2\x06(\n\x06ReLU19\x12\x04ReLU\x1a\x0bBatchNorm19"\x0bBatchNorm19\xa2\x06Y\n\rConvolution20\x12\x0bConvolution\x1a\x0bBatchNorm19"\rConvolution20\xd2\x06\x1e\x08\x0c\x10\x00\x18\x01 \x030\x01:\x06\n\x04msraB\n\n\x08constant\xa2\x066\n\tDropout19\x12\x07Dropout\x1a\rConvolution20"\tDropout19\xe2\x06\x05\r\xcd\xccL>\xa2\x066\n\x08Concat18\x12\x06Concat\x1a\x08Concat17\x1a\tDropout19"\x08Concat18\xc2\x06\x02\x10\x01\xa2\x06S\n\x0bBatchNorm20\x12\tBatchNorm\x1a\x08Concat18"\x0bBatchNorm202\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x00\xa2\x06=\n\x07Scale20\x12\x05Scale\x1a\x0bBatchNorm20"\x0bBatchNorm20\xf2\x08\x10\x1a\x05\x15\x00\x00\x80? \x01*\x05\x15\x00\x00\x00\x00\xa2\x06(\n\x06ReLU20\x12\x04ReLU\x1a\x0bBatchNorm20"\x0bBatchNorm20\xa2\x06Y\n\rConvolution21\x12\x0bConvolution\x1a\x0bBatchNorm20"\rConvolution21\xd2\x06\x1e\x08\x0c\x10\x00\x18\x01 \x030\x01:\x06\n\x04msraB\n\n\x08constant\xa2\x066\n\tDropout20\x12\x07Dropout\x1a\rConvolution21"\tDropout20\xe2\x06\x05\r\xcd\xccL>\xa2\x066\n\x08Concat19\x12\x06Concat\x1a\x08Concat18\x1a\tDropout20"\x08Concat19\xc2\x06\x02\x10\x01\xa2\x06S\n\x0bBatchNorm21\x12\tBatchNorm\x1a\x08Concat19"\x0bBatchNorm212\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x00\xa2\x06=\n\x07Scale21\x12\x05Scale\x1a\x0bBatchNorm21"\x0bBatchNorm21\xf2\x08\x10\x1a\x05\x15\x00\x00\x80? \x01*\x05\x15\x00\x00\x00\x00\xa2\x06(\n\x06ReLU21\x12\x04ReLU\x1a\x0bBatchNorm21"\x0bBatchNorm21\xa2\x06Y\n\rConvolution22\x12\x0bConvolution\x1a\x0bBatchNorm21"\rConvolution22\xd2\x06\x1e\x08\x0c\x10\x00\x18\x01 \x030\x01:\x06\n\x04msraB\n\n\x08constant\xa2\x066\n\tDropout21\x12\x07Dropout\x1a\rConvolution22"\tDropout21\xe2\x06\x05\r\xcd\xccL>\xa2\x066\n\x08Concat20\x12\x06Concat\x1a\x08Concat19\x1a\tDropout21"\x08Concat20\xc2\x06\x02\x10\x01\xa2\x06S\n\x0bBatchNorm22\x12\tBatchNorm\x1a\x08Concat20"\x0bBatchNorm222\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x00\xa2\x06=\n\x07Scale22\x12\x05Scale\x1a\x0bBatchNorm22"\x0bBatchNorm22\xf2\x08\x10\x1a\x05\x15\x00\x00\x80? \x01*\x05\x15\x00\x00\x00\x00\xa2\x06(\n\x06ReLU22\x12\x04ReLU\x1a\x0bBatchNorm22"\x0bBatchNorm22\xa2\x06Y\n\rConvolution23\x12\x0bConvolution\x1a\x0bBatchNorm22"\rConvolution23\xd2\x06\x1e\x08\x0c\x10\x00\x18\x01 \x030\x01:\x06\n\x04msraB\n\n\x08constant\xa2\x066\n\tDropout22\x12\x07Dropout\x1a\rConvolution23"\tDropout22\xe2\x06\x05\r\xcd\xccL>\xa2\x066\n\x08Concat21\x12\x06Concat\x1a\x08Concat20\x1a\tDropout22"\x08Concat21\xc2\x06\x02\x10\x01\xa2\x06S\n\x0bBatchNorm23\x12\tBatchNorm\x1a\x08Concat21"\x0bBatchNorm232\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x00\xa2\x06=\n\x07Scale23\x12\x05Scale\x1a\x0bBatchNorm23"\x0bBatchNorm23\xf2\x08\x10\x1a\x05\x15\x00\x00\x80? \x01*\x05\x15\x00\x00\x00\x00\xa2\x06(\n\x06ReLU23\x12\x04ReLU\x1a\x0bBatchNorm23"\x0bBatchNorm23\xa2\x06Y\n\rConvolution24\x12\x0bConvolution\x1a\x0bBatchNorm23"\rConvolution24\xd2\x06\x1e\x08\x0c\x10\x00\x18\x01 \x030\x01:\x06\n\x04msraB\n\n\x08constant\xa2\x066\n\tDropout23\x12\x07Dropout\x1a\rConvolution24"\tDropout23\xe2\x06\x05\r\xcd\xccL>\xa2\x066\n\x08Concat22\x12\x06Concat\x1a\x08Concat21\x1a\tDropout23"\x08Concat22\xc2\x06\x02\x10\x01\xa2\x06S\n\x0bBatchNorm24\x12\tBatchNorm\x1a\x08Concat22"\x0bBatchNorm242\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x00\xa2\x06=\n\x07Scale24\x12\x05Scale\x1a\x0bBatchNorm24"\x0bBatchNorm24\xf2\x08\x10\x1a\x05\x15\x00\x00\x80? \x01*\x05\x15\x00\x00\x00\x00\xa2\x06(\n\x06ReLU24\x12\x04ReLU\x1a\x0bBatchNorm24"\x0bBatchNorm24\xa2\x06Y\n\rConvolution25\x12\x0bConvolution\x1a\x0bBatchNorm24"\rConvolution25\xd2\x06\x1e\x08\x0c\x10\x00\x18\x01 \x030\x01:\x06\n\x04msraB\n\n\x08constant\xa2\x066\n\tDropout24\x12\x07Dropout\x1a\rConvolution25"\tDropout24\xe2\x06\x05\r\xcd\xccL>\xa2\x066\n\x08Concat23\x12\x06Concat\x1a\x08Concat22\x1a\tDropout24"\x08Concat23\xc2\x06\x02\x10\x01\xa2\x06S\n\x0bBatchNorm25\x12\tBatchNorm\x1a\x08Concat23"\x0bBatchNorm252\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x00\xa2\x06=\n\x07Scale25\x12\x05Scale\x1a\x0bBatchNorm25"\x0bBatchNorm25\xf2\x08\x10\x1a\x05\x15\x00\x00\x80? \x01*\x05\x15\x00\x00\x00\x00\xa2\x06(\n\x06ReLU25\x12\x04ReLU\x1a\x0bBatchNorm25"\x0bBatchNorm25\xa2\x06Y\n\rConvolution26\x12\x0bConvolution\x1a\x0bBatchNorm25"\rConvolution26\xd2\x06\x1e\x08\x0c\x10\x00\x18\x01 \x030\x01:\x06\n\x04msraB\n\n\x08constant\xa2\x066\n\tDropout25\x12\x07Dropout\x1a\rConvolution26"\tDropout25\xe2\x06\x05\r\xcd\xccL>\xa2\x066\n\x08Concat24\x12\x06Concat\x1a\x08Concat23\x1a\tDropout25"\x08Concat24\xc2\x06\x02\x10\x01\xa2\x06S\n\x0bBatchNorm26\x12\tBatchNorm\x1a\x08Concat24"\x0bBatchNorm262\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x00\xa2\x06=\n\x07Scale26\x12\x05Scale\x1a\x0bBatchNorm26"\x0bBatchNorm26\xf2\x08\x10\x1a\x05\x15\x00\x00\x80? \x01*\x05\x15\x00\x00\x00\x00\xa2\x06(\n\x06ReLU26\x12\x04ReLU\x1a\x0bBatchNorm26"\x0bBatchNorm26\xa2\x06Z\n\rConvolution27\x12\x0bConvolution\x1a\x0bBatchNorm26"\rConvolution27\xd2\x06\x1f\x08\xb0\x02\x10\x00\x18\x00 \x010\x01:\x06\n\x04msraB\n\n\x08constant\xa2\x066\n\tDropout26\x12\x07Dropout\x1a\rConvolution27"\tDropout26\xe2\x06\x05\r\xcd\xccL>\xa2\x061\n\x08Pooling2\x12\x07Pooling\x1a\tDropout26"\x08Pooling2\xca\x07\x06\x08\x01\x10\x02\x18\x02\xa2\x06S\n\x0bBatchNorm27\x12\tBatchNorm\x1a\x08Pooling2"\x0bBatchNorm272\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x00\xa2\x06=\n\x07Scale27\x12\x05Scale\x1a\x0bBatchNorm27"\x0bBatchNorm27\xf2\x08\x10\x1a\x05\x15\x00\x00\x80? \x01*\x05\x15\x00\x00\x00\x00\xa2\x06(\n\x06ReLU27\x12\x04ReLU\x1a\x0bBatchNorm27"\x0bBatchNorm27\xa2\x06Y\n\rConvolution28\x12\x0bConvolution\x1a\x0bBatchNorm27"\rConvolution28\xd2\x06\x1e\x08\x0c\x10\x00\x18\x01 \x030\x01:\x06\n\x04msraB\n\n\x08constant\xa2\x066\n\tDropout27\x12\x07Dropout\x1a\rConvolution28"\tDropout27\xe2\x06\x05\r\xcd\xccL>\xa2\x066\n\x08Concat25\x12\x06Concat\x1a\x08Pooling2\x1a\tDropout27"\x08Concat25\xc2\x06\x02\x10\x01\xa2\x06S\n\x0bBatchNorm28\x12\tBatchNorm\x1a\x08Concat25"\x0bBatchNorm282\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x00\xa2\x06=\n\x07Scale28\x12\x05Scale\x1a\x0bBatchNorm28"\x0bBatchNorm28\xf2\x08\x10\x1a\x05\x15\x00\x00\x80? \x01*\x05\x15\x00\x00\x00\x00\xa2\x06(\n\x06ReLU28\x12\x04ReLU\x1a\x0bBatchNorm28"\x0bBatchNorm28\xa2\x06Y\n\rConvolution29\x12\x0bConvolution\x1a\x0bBatchNorm28"\rConvolution29\xd2\x06\x1e\x08\x0c\x10\x00\x18\x01 \x030\x01:\x06\n\x04msraB\n\n\x08constant\xa2\x066\n\tDropout28\x12\x07Dropout\x1a\rConvolution29"\tDropout28\xe2\x06\x05\r\xcd\xccL>\xa2\x066\n\x08Concat26\x12\x06Concat\x1a\x08Concat25\x1a\tDropout28"\x08Concat26\xc2\x06\x02\x10\x01\xa2\x06S\n\x0bBatchNorm29\x12\tBatchNorm\x1a\x08Concat26"\x0bBatchNorm292\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x00\xa2\x06=\n\x07Scale29\x12\x05Scale\x1a\x0bBatchNorm29"\x0bBatchNorm29\xf2\x08\x10\x1a\x05\x15\x00\x00\x80? \x01*\x05\x15\x00\x00\x00\x00\xa2\x06(\n\x06ReLU29\x12\x04ReLU\x1a\x0bBatchNorm29"\x0bBatchNorm29\xa2\x06Y\n\rConvolution30\x12\x0bConvolution\x1a\x0bBatchNorm29"\rConvolution30\xd2\x06\x1e\x08\x0c\x10\x00\x18\x01 \x030\x01:\x06\n\x04msraB\n\n\x08constant\xa2\x066\n\tDropout29\x12\x07Dropout\x1a\rConvolution30"\tDropout29\xe2\x06\x05\r\xcd\xccL>\xa2\x066\n\x08Concat27\x12\x06Concat\x1a\x08Concat26\x1a\tDropout29"\x08Concat27\xc2\x06\x02\x10\x01\xa2\x06S\n\x0bBatchNorm30\x12\tBatchNorm\x1a\x08Concat27"\x0bBatchNorm302\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x00\xa2\x06=\n\x07Scale30\x12\x05Scale\x1a\x0bBatchNorm30"\x0bBatchNorm30\xf2\x08\x10\x1a\x05\x15\x00\x00\x80? \x01*\x05\x15\x00\x00\x00\x00\xa2\x06(\n\x06ReLU30\x12\x04ReLU\x1a\x0bBatchNorm30"\x0bBatchNorm30\xa2\x06Y\n\rConvolution31\x12\x0bConvolution\x1a\x0bBatchNorm30"\rConvolution31\xd2\x06\x1e\x08\x0c\x10\x00\x18\x01 \x030\x01:\x06\n\x04msraB\n\n\x08constant\xa2\x066\n\tDropout30\x12\x07Dropout\x1a\rConvolution31"\tDropout30\xe2\x06\x05\r\xcd\xccL>\xa2\x066\n\x08Concat28\x12\x06Concat\x1a\x08Concat27\x1a\tDropout30"\x08Concat28\xc2\x06\x02\x10\x01\xa2\x06S\n\x0bBatchNorm31\x12\tBatchNorm\x1a\x08Concat28"\x0bBatchNorm312\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x00\xa2\x06=\n\x07Scale31\x12\x05Scale\x1a\x0bBatchNorm31"\x0bBatchNorm31\xf2\x08\x10\x1a\x05\x15\x00\x00\x80? \x01*\x05\x15\x00\x00\x00\x00\xa2\x06(\n\x06ReLU31\x12\x04ReLU\x1a\x0bBatchNorm31"\x0bBatchNorm31\xa2\x06Y\n\rConvolution32\x12\x0bConvolution\x1a\x0bBatchNorm31"\rConvolution32\xd2\x06\x1e\x08\x0c\x10\x00\x18\x01 \x030\x01:\x06\n\x04msraB\n\n\x08constant\xa2\x066\n\tDropout31\x12\x07Dropout\x1a\rConvolution32"\tDropout31\xe2\x06\x05\r\xcd\xccL>\xa2\x066\n\x08Concat29\x12\x06Concat\x1a\x08Concat28\x1a\tDropout31"\x08Concat29\xc2\x06\x02\x10\x01\xa2\x06S\n\x0bBatchNorm32\x12\tBatchNorm\x1a\x08Concat29"\x0bBatchNorm322\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x00\xa2\x06=\n\x07Scale32\x12\x05Scale\x1a\x0bBatchNorm32"\x0bBatchNorm32\xf2\x08\x10\x1a\x05\x15\x00\x00\x80? \x01*\x05\x15\x00\x00\x00\x00\xa2\x06(\n\x06ReLU32\x12\x04ReLU\x1a\x0bBatchNorm32"\x0bBatchNorm32\xa2\x06Y\n\rConvolution33\x12\x0bConvolution\x1a\x0bBatchNorm32"\rConvolution33\xd2\x06\x1e\x08\x0c\x10\x00\x18\x01 \x030\x01:\x06\n\x04msraB\n\n\x08constant\xa2\x066\n\tDropout32\x12\x07Dropout\x1a\rConvolution33"\tDropout32\xe2\x06\x05\r\xcd\xccL>\xa2\x066\n\x08Concat30\x12\x06Concat\x1a\x08Concat29\x1a\tDropout32"\x08Concat30\xc2\x06\x02\x10\x01\xa2\x06S\n\x0bBatchNorm33\x12\tBatchNorm\x1a\x08Concat30"\x0bBatchNorm332\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x00\xa2\x06=\n\x07Scale33\x12\x05Scale\x1a\x0bBatchNorm33"\x0bBatchNorm33\xf2\x08\x10\x1a\x05\x15\x00\x00\x80? \x01*\x05\x15\x00\x00\x00\x00\xa2\x06(\n\x06ReLU33\x12\x04ReLU\x1a\x0bBatchNorm33"\x0bBatchNorm33\xa2\x06Y\n\rConvolution34\x12\x0bConvolution\x1a\x0bBatchNorm33"\rConvolution34\xd2\x06\x1e\x08\x0c\x10\x00\x18\x01 \x030\x01:\x06\n\x04msraB\n\n\x08constant\xa2\x066\n\tDropout33\x12\x07Dropout\x1a\rConvolution34"\tDropout33\xe2\x06\x05\r\xcd\xccL>\xa2\x066\n\x08Concat31\x12\x06Concat\x1a\x08Concat30\x1a\tDropout33"\x08Concat31\xc2\x06\x02\x10\x01\xa2\x06S\n\x0bBatchNorm34\x12\tBatchNorm\x1a\x08Concat31"\x0bBatchNorm342\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x00\xa2\x06=\n\x07Scale34\x12\x05Scale\x1a\x0bBatchNorm34"\x0bBatchNorm34\xf2\x08\x10\x1a\x05\x15\x00\x00\x80? \x01*\x05\x15\x00\x00\x00\x00\xa2\x06(\n\x06ReLU34\x12\x04ReLU\x1a\x0bBatchNorm34"\x0bBatchNorm34\xa2\x06Y\n\rConvolution35\x12\x0bConvolution\x1a\x0bBatchNorm34"\rConvolution35\xd2\x06\x1e\x08\x0c\x10\x00\x18\x01 \x030\x01:\x06\n\x04msraB\n\n\x08constant\xa2\x066\n\tDropout34\x12\x07Dropout\x1a\rConvolution35"\tDropout34\xe2\x06\x05\r\xcd\xccL>\xa2\x066\n\x08Concat32\x12\x06Concat\x1a\x08Concat31\x1a\tDropout34"\x08Concat32\xc2\x06\x02\x10\x01\xa2\x06S\n\x0bBatchNorm35\x12\tBatchNorm\x1a\x08Concat32"\x0bBatchNorm352\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x00\xa2\x06=\n\x07Scale35\x12\x05Scale\x1a\x0bBatchNorm35"\x0bBatchNorm35\xf2\x08\x10\x1a\x05\x15\x00\x00\x80? \x01*\x05\x15\x00\x00\x00\x00\xa2\x06(\n\x06ReLU35\x12\x04ReLU\x1a\x0bBatchNorm35"\x0bBatchNorm35\xa2\x06Y\n\rConvolution36\x12\x0bConvolution\x1a\x0bBatchNorm35"\rConvolution36\xd2\x06\x1e\x08\x0c\x10\x00\x18\x01 \x030\x01:\x06\n\x04msraB\n\n\x08constant\xa2\x066\n\tDropout35\x12\x07Dropout\x1a\rConvolution36"\tDropout35\xe2\x06\x05\r\xcd\xccL>\xa2\x066\n\x08Concat33\x12\x06Concat\x1a\x08Concat32\x1a\tDropout35"\x08Concat33\xc2\x06\x02\x10\x01\xa2\x06S\n\x0bBatchNorm36\x12\tBatchNorm\x1a\x08Concat33"\x0bBatchNorm362\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x00\xa2\x06=\n\x07Scale36\x12\x05Scale\x1a\x0bBatchNorm36"\x0bBatchNorm36\xf2\x08\x10\x1a\x05\x15\x00\x00\x80? \x01*\x05\x15\x00\x00\x00\x00\xa2\x06(\n\x06ReLU36\x12\x04ReLU\x1a\x0bBatchNorm36"\x0bBatchNorm36\xa2\x06Y\n\rConvolution37\x12\x0bConvolution\x1a\x0bBatchNorm36"\rConvolution37\xd2\x06\x1e\x08\x0c\x10\x00\x18\x01 \x030\x01:\x06\n\x04msraB\n\n\x08constant\xa2\x066\n\tDropout36\x12\x07Dropout\x1a\rConvolution37"\tDropout36\xe2\x06\x05\r\xcd\xccL>\xa2\x066\n\x08Concat34\x12\x06Concat\x1a\x08Concat33\x1a\tDropout36"\x08Concat34\xc2\x06\x02\x10\x01\xa2\x06S\n\x0bBatchNorm37\x12\tBatchNorm\x1a\x08Concat34"\x0bBatchNorm372\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x00\xa2\x06=\n\x07Scale37\x12\x05Scale\x1a\x0bBatchNorm37"\x0bBatchNorm37\xf2\x08\x10\x1a\x05\x15\x00\x00\x80? \x01*\x05\x15\x00\x00\x00\x00\xa2\x06(\n\x06ReLU37\x12\x04ReLU\x1a\x0bBatchNorm37"\x0bBatchNorm37\xa2\x06Y\n\rConvolution38\x12\x0bConvolution\x1a\x0bBatchNorm37"\rConvolution38\xd2\x06\x1e\x08\x0c\x10\x00\x18\x01 \x030\x01:\x06\n\x04msraB\n\n\x08constant\xa2\x066\n\tDropout37\x12\x07Dropout\x1a\rConvolution38"\tDropout37\xe2\x06\x05\r\xcd\xccL>\xa2\x066\n\x08Concat35\x12\x06Concat\x1a\x08Concat34\x1a\tDropout37"\x08Concat35\xc2\x06\x02\x10\x01\xa2\x06S\n\x0bBatchNorm38\x12\tBatchNorm\x1a\x08Concat35"\x0bBatchNorm382\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x00\xa2\x06=\n\x07Scale38\x12\x05Scale\x1a\x0bBatchNorm38"\x0bBatchNorm38\xf2\x08\x10\x1a\x05\x15\x00\x00\x80? \x01*\x05\x15\x00\x00\x00\x00\xa2\x06(\n\x06ReLU38\x12\x04ReLU\x1a\x0bBatchNorm38"\x0bBatchNorm38\xa2\x06Y\n\rConvolution39\x12\x0bConvolution\x1a\x0bBatchNorm38"\rConvolution39\xd2\x06\x1e\x08\x0c\x10\x00\x18\x01 \x030\x01:\x06\n\x04msraB\n\n\x08constant\xa2\x066\n\tDropout38\x12\x07Dropout\x1a\rConvolution39"\tDropout38\xe2\x06\x05\r\xcd\xccL>\xa2\x066\n\x08Concat36\x12\x06Concat\x1a\x08Concat35\x1a\tDropout38"\x08Concat36\xc2\x06\x02\x10\x01\xa2\x06S\n\x0bBatchNorm39\x12\tBatchNorm\x1a\x08Concat36"\x0bBatchNorm392\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x002\n\x1d\x00\x00\x00\x00%\x00\x00\x00\x00\xa2\x06=\n\x07Scale39\x12\x05Scale\x1a\x0bBatchNorm39"\x0bBatchNorm39\xf2\x08\x10\x1a\x05\x15\x00\x00\x80? \x01*\x05\x15\x00\x00\x00\x00\xa2\x06(\n\x06ReLU39\x12\x04ReLU\x1a\x0bBatchNorm39"\x0bBatchNorm39\xa2\x061\n\x08Pooling3\x12\x07Pooling\x1a\x0bBatchNorm39"\x08Pooling3\xca\x07\x04\x08\x01`\x01\xa2\x06S\n\rInnerProduct1\x12\x0cInnerProduct\x1a\x08Pooling3"\rInnerProduct1\xaa\x07\x1a\x08\x04\x10\x01\x1a\x08\n\x06xavier"\n\n\x08constant\xa2\x06B\n\x04loss\x12\x0fSoftmaxWithLoss\x1a\rInnerProduct1\x1a\x05label"\x04loss-\x00\x00\x80?J\x08"\x06deploy\xa2\x06D\n\x08Accuracy\x12\x08Accuracy\x1a\rInnerProduct1\x1a\x05label"\x08accuracyB\x05"\x03valB\x07"\x05train\xa2\x064\n\x07softmax\x12\x07Softmax\x1a\rInnerProduct1"\x07softmaxB\x08"\x06deploy'
p91
sbsS'use_mean'
p92
Vimage
p93
sS'model_file'
p94
S'original.prototxt'
p95
sS'job_dir'
p96
S'/home/azh2/digits/digits/jobs/20180308-215226-c8ec'
p97
sS'receiving_val_output'
p98
I00
sS'parents'
p99
NsS'caffe_flavor'
p100
S'NVIDIA'
p101
sS'lr_policy'
p102
(dp103
S'policy'
p104
Vstep
p105
sS'stepsize'
p106
F33.0
sS'gamma'
p107
F0.1
ssS'crop_size'
p108
I32
sS'receiving_train_output'
p109
I00
sS'val_interval'
p110
F1.0
sS'current_epoch'
p111
F30.0
sS'random_seed'
p112
NsS'saving_snapshot'
p113
I00
sS'pickver_task_caffe_train'
p114
I5
sg46
F0.01
sS'loaded_snapshot_file'
p115
NsS'batch_size'
p116
I8
sS'digits_version'
p117
S'5.1-dev'
p118
sS'job'
p119
g4
sS'pickver_task_train'
p120
I2
sS'current_resources'
p121
NsS'pickver_task'
p122
I1
sS'job_id'
p123
S'20180308-215226-c8ec'
p124
sS'exception'
p125
NsS'data_aug'
p126
(dp127
S'hsv_h'
p128
F0.02
sS'hsv_use'
p129
I00
sS'flip'
p130
Vnone
p131
sS'quad_rot'
p132
Vnone
p133
sS'scale'
p134
F0.0
sS'noise'
p135
F0.0
sS'hsv_s'
p136
F0.04
sS'rot'
p137
I0
sS'hsv_v'
p138
F0.06
ssS'selected_gpus'
p139
NsS'solver'
p140
ccaffe_pb2
SolverParameter
p141
(tRp142
(dp143
g90
S'\x18\x12 \xfe(-\n\xd7#<0O8\xc4\xcd\tB\x04stepM\xcd\xcc\xcc=]fff?e\x17\xb7\xd18h\xe0\x95\x03p\xfe(z\x08snapshot\x88\x01\x01\xc2\x01\x12train_val.prototxt\xf0\x01\x00\xa0\x02\x08'
p144
sbsS'snapshot_prefix'
p145
S'snapshot'
p146
sS'traceback'
p147
NsS'framework_id'
p148
S'caffe'
p149
sS'status_history'
p150
(lp151
((idigits.status
Status
p152
S'I'
p154
bF1520545946.861403
tp155
a((idigits.status
Status
p156
S'R'
p157
bF1520545948.501535
tp158
a((idigits.status
Status
p159
S'D'
p160
bF1520582954.162946
tp161
asS'last_train_update'
p162
F1520582948.672793
sS'train_val_file'
p163
S'train_val.prototxt'
p164
sS'image_mean'
p165
NsS'batch_accumulation'
p166
I8
sS'progress'
p167
F1.0
sS'log_file'
p168
S'caffe_output.log'
p169
sS'solver_file'
p170
S'solver.prototxt'
p171
sS'solver_type'
p172
VSGD
p173
sbasS'pickver_job'
p174
I2
sS'dataset_id'
p175
S'20180308-173033-5771'
p176
sS'pickver_job_model_image'
p177
I1
sg125
NsS'group'
p178
VPlant classification
p179
sS'persistent'
p180
I01
sS'_name'
p181
Vplant 4 class using DenseNet from scratch run 2
p182
sS'form_data'
p183
(dp184
S'form.standard_networks.data'
p185
VNone
p186
sS'form.learning_rate.data'
p187
(lp188
F0.01
asS'form.model_name.data'
p189
g182
sS'form.aug_flip.data'
p190
g131
sS'form.select_gpus.data'
p191
(lp192
sS'form.framework.data'
p193
Vcaffe
p194
sS'form.dataset.data'
p195
V20180308-173033-5771
p196
sS'form.python_layer_from_client.data'
p197
I00
sS'form.lr_multistep_values.data'
p198
V50,85
p199
sS'form.pretrained_networks.data'
p200
VNone
p201
sS'form.aug_hsv_s.data'
p202
F0.04
sS'form.lr_sigmoid_gamma.data'
p203
F0.1
sS'form.lr_policy.data'
p204
g105
sS'form.random_seed.data'
p205
NsS'form.previous_networks.data'
p206
VNone
p207
sS'form.val_interval.data'
p208
F1.0
sS'form.aug_quad_rot.data'
p209
g133
sS'form.rms_decay.data'
p210
F0.99
sS'form.method.data'
p211
Vcustom
p212
sS'form.lr_sigmoid_step.data'
p213
F50.0
sS'form.crop_size.data'
p214
NsS'form.select_gpu_count.data'
p215
I1
sS'form.python_layer_server_file.data'
p216
g79
sS'form.aug_hsv_v.data'
p217
F0.06
sS'form.solver_type.data'
p218
g173
sS'form.aug_noise.data'
p219
F0.0
sS'form.select_gpus_list.data'
p220
g79
sS'form.shuffle.data'
p221
I01
sS'form.aug_scale.data'
p222
F0.0
sS'form.batch_accumulation.data'
p223
I8
sS'form.custom_network.data'
p224
Vname: "DENSENET_cifar10"\u000alayer {\u000a  name: "train-data"\u000a  type: "Data"\u000a  top: "data"\u000a  top: "label"\u000a  transform_param {\u000a    mirror: true\u000a    crop_size: 32\u000a  }\u000a  data_param {\u000a  batch_size: 32\u000a  }\u000a  include { stage: "train" }\u000a}\u000alayer {\u000a  name: "val-data"\u000a  type: "Data"\u000a  top: "data"\u000a  top: "label"\u000a  transform_param {\u000a    mirror: false\u000a    crop_size: 32\u000a  }\u000a  data_param {\u000a    batch_size: 32\u000a  }\u000a  include { stage: "val" }\u000a}\u000alayer {\u000a  name: "Convolution1"\u000a  type: "Convolution"\u000a  bottom: "data"\u000a  top: "Convolution1"\u000a  convolution_param {\u000a    num_output: 16\u000a    bias_term: false\u000a    pad: 1\u000a    kernel_size: 3\u000a    stride: 1\u000a    weight_filler {\u000a      type: "msra"\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "BatchNorm1"\u000a  type: "BatchNorm"\u000a  bottom: "Convolution1"\u000a  top: "BatchNorm1"\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a}\u000alayer {\u000a  name: "Scale1"\u000a  type: "Scale"\u000a  bottom: "BatchNorm1"\u000a  top: "BatchNorm1"\u000a  scale_param {\u000a    filler {\u000a      value: 1\u000a    }\u000a    bias_term: true\u000a    bias_filler {\u000a      value: 0\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "ReLU1"\u000a  type: "ReLU"\u000a  bottom: "BatchNorm1"\u000a  top: "BatchNorm1"\u000a}\u000alayer {\u000a  name: "Convolution2"\u000a  type: "Convolution"\u000a  bottom: "BatchNorm1"\u000a  top: "Convolution2"\u000a  convolution_param {\u000a    num_output: 12\u000a    bias_term: false\u000a    pad: 1\u000a    kernel_size: 3\u000a    stride: 1\u000a    weight_filler {\u000a      type: "msra"\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "Dropout1"\u000a  type: "Dropout"\u000a  bottom: "Convolution2"\u000a  top: "Dropout1"\u000a  dropout_param {\u000a    dropout_ratio: 0.2\u000a  }\u000a}\u000alayer {\u000a  name: "Concat1"\u000a  type: "Concat"\u000a  bottom: "Convolution1"\u000a  bottom: "Dropout1"\u000a  top: "Concat1"\u000a  concat_param {\u000a    axis: 1\u000a  }\u000a}\u000alayer {\u000a  name: "BatchNorm2"\u000a  type: "BatchNorm"\u000a  bottom: "Concat1"\u000a  top: "BatchNorm2"\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a}\u000alayer {\u000a  name: "Scale2"\u000a  type: "Scale"\u000a  bottom: "BatchNorm2"\u000a  top: "BatchNorm2"\u000a  scale_param {\u000a    filler {\u000a      value: 1\u000a    }\u000a    bias_term: true\u000a    bias_filler {\u000a      value: 0\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "ReLU2"\u000a  type: "ReLU"\u000a  bottom: "BatchNorm2"\u000a  top: "BatchNorm2"\u000a}\u000alayer {\u000a  name: "Convolution3"\u000a  type: "Convolution"\u000a  bottom: "BatchNorm2"\u000a  top: "Convolution3"\u000a  convolution_param {\u000a    num_output: 12\u000a    bias_term: false\u000a    pad: 1\u000a    kernel_size: 3\u000a    stride: 1\u000a    weight_filler {\u000a      type: "msra"\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "Dropout2"\u000a  type: "Dropout"\u000a  bottom: "Convolution3"\u000a  top: "Dropout2"\u000a  dropout_param {\u000a    dropout_ratio: 0.2\u000a  }\u000a}\u000alayer {\u000a  name: "Concat2"\u000a  type: "Concat"\u000a  bottom: "Concat1"\u000a  bottom: "Dropout2"\u000a  top: "Concat2"\u000a  concat_param {\u000a    axis: 1\u000a  }\u000a}\u000alayer {\u000a  name: "BatchNorm3"\u000a  type: "BatchNorm"\u000a  bottom: "Concat2"\u000a  top: "BatchNorm3"\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a}\u000alayer {\u000a  name: "Scale3"\u000a  type: "Scale"\u000a  bottom: "BatchNorm3"\u000a  top: "BatchNorm3"\u000a  scale_param {\u000a    filler {\u000a      value: 1\u000a    }\u000a    bias_term: true\u000a    bias_filler {\u000a      value: 0\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "ReLU3"\u000a  type: "ReLU"\u000a  bottom: "BatchNorm3"\u000a  top: "BatchNorm3"\u000a}\u000alayer {\u000a  name: "Convolution4"\u000a  type: "Convolution"\u000a  bottom: "BatchNorm3"\u000a  top: "Convolution4"\u000a  convolution_param {\u000a    num_output: 12\u000a    bias_term: false\u000a    pad: 1\u000a    kernel_size: 3\u000a    stride: 1\u000a    weight_filler {\u000a      type: "msra"\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "Dropout3"\u000a  type: "Dropout"\u000a  bottom: "Convolution4"\u000a  top: "Dropout3"\u000a  dropout_param {\u000a    dropout_ratio: 0.2\u000a  }\u000a}\u000alayer {\u000a  name: "Concat3"\u000a  type: "Concat"\u000a  bottom: "Concat2"\u000a  bottom: "Dropout3"\u000a  top: "Concat3"\u000a  concat_param {\u000a    axis: 1\u000a  }\u000a}\u000alayer {\u000a  name: "BatchNorm4"\u000a  type: "BatchNorm"\u000a  bottom: "Concat3"\u000a  top: "BatchNorm4"\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a}\u000alayer {\u000a  name: "Scale4"\u000a  type: "Scale"\u000a  bottom: "BatchNorm4"\u000a  top: "BatchNorm4"\u000a  scale_param {\u000a    filler {\u000a      value: 1\u000a    }\u000a    bias_term: true\u000a    bias_filler {\u000a      value: 0\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "ReLU4"\u000a  type: "ReLU"\u000a  bottom: "BatchNorm4"\u000a  top: "BatchNorm4"\u000a}\u000alayer {\u000a  name: "Convolution5"\u000a  type: "Convolution"\u000a  bottom: "BatchNorm4"\u000a  top: "Convolution5"\u000a  convolution_param {\u000a    num_output: 12\u000a    bias_term: false\u000a    pad: 1\u000a    kernel_size: 3\u000a    stride: 1\u000a    weight_filler {\u000a      type: "msra"\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "Dropout4"\u000a  type: "Dropout"\u000a  bottom: "Convolution5"\u000a  top: "Dropout4"\u000a  dropout_param {\u000a    dropout_ratio: 0.2\u000a  }\u000a}\u000alayer {\u000a  name: "Concat4"\u000a  type: "Concat"\u000a  bottom: "Concat3"\u000a  bottom: "Dropout4"\u000a  top: "Concat4"\u000a  concat_param {\u000a    axis: 1\u000a  }\u000a}\u000alayer {\u000a  name: "BatchNorm5"\u000a  type: "BatchNorm"\u000a  bottom: "Concat4"\u000a  top: "BatchNorm5"\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a}\u000alayer {\u000a  name: "Scale5"\u000a  type: "Scale"\u000a  bottom: "BatchNorm5"\u000a  top: "BatchNorm5"\u000a  scale_param {\u000a    filler {\u000a      value: 1\u000a    }\u000a    bias_term: true\u000a    bias_filler {\u000a      value: 0\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "ReLU5"\u000a  type: "ReLU"\u000a  bottom: "BatchNorm5"\u000a  top: "BatchNorm5"\u000a}\u000alayer {\u000a  name: "Convolution6"\u000a  type: "Convolution"\u000a  bottom: "BatchNorm5"\u000a  top: "Convolution6"\u000a  convolution_param {\u000a    num_output: 12\u000a    bias_term: false\u000a    pad: 1\u000a    kernel_size: 3\u000a    stride: 1\u000a    weight_filler {\u000a      type: "msra"\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "Dropout5"\u000a  type: "Dropout"\u000a  bottom: "Convolution6"\u000a  top: "Dropout5"\u000a  dropout_param {\u000a    dropout_ratio: 0.2\u000a  }\u000a}\u000alayer {\u000a  name: "Concat5"\u000a  type: "Concat"\u000a  bottom: "Concat4"\u000a  bottom: "Dropout5"\u000a  top: "Concat5"\u000a  concat_param {\u000a    axis: 1\u000a  }\u000a}\u000alayer {\u000a  name: "BatchNorm6"\u000a  type: "BatchNorm"\u000a  bottom: "Concat5"\u000a  top: "BatchNorm6"\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a}\u000alayer {\u000a  name: "Scale6"\u000a  type: "Scale"\u000a  bottom: "BatchNorm6"\u000a  top: "BatchNorm6"\u000a  scale_param {\u000a    filler {\u000a      value: 1\u000a    }\u000a    bias_term: true\u000a    bias_filler {\u000a      value: 0\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "ReLU6"\u000a  type: "ReLU"\u000a  bottom: "BatchNorm6"\u000a  top: "BatchNorm6"\u000a}\u000alayer {\u000a  name: "Convolution7"\u000a  type: "Convolution"\u000a  bottom: "BatchNorm6"\u000a  top: "Convolution7"\u000a  convolution_param {\u000a    num_output: 12\u000a    bias_term: false\u000a    pad: 1\u000a    kernel_size: 3\u000a    stride: 1\u000a    weight_filler {\u000a      type: "msra"\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "Dropout6"\u000a  type: "Dropout"\u000a  bottom: "Convolution7"\u000a  top: "Dropout6"\u000a  dropout_param {\u000a    dropout_ratio: 0.2\u000a  }\u000a}\u000alayer {\u000a  name: "Concat6"\u000a  type: "Concat"\u000a  bottom: "Concat5"\u000a  bottom: "Dropout6"\u000a  top: "Concat6"\u000a  concat_param {\u000a    axis: 1\u000a  }\u000a}\u000alayer {\u000a  name: "BatchNorm7"\u000a  type: "BatchNorm"\u000a  bottom: "Concat6"\u000a  top: "BatchNorm7"\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a}\u000alayer {\u000a  name: "Scale7"\u000a  type: "Scale"\u000a  bottom: "BatchNorm7"\u000a  top: "BatchNorm7"\u000a  scale_param {\u000a    filler {\u000a      value: 1\u000a    }\u000a    bias_term: true\u000a    bias_filler {\u000a      value: 0\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "ReLU7"\u000a  type: "ReLU"\u000a  bottom: "BatchNorm7"\u000a  top: "BatchNorm7"\u000a}\u000alayer {\u000a  name: "Convolution8"\u000a  type: "Convolution"\u000a  bottom: "BatchNorm7"\u000a  top: "Convolution8"\u000a  convolution_param {\u000a    num_output: 12\u000a    bias_term: false\u000a    pad: 1\u000a    kernel_size: 3\u000a    stride: 1\u000a    weight_filler {\u000a      type: "msra"\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "Dropout7"\u000a  type: "Dropout"\u000a  bottom: "Convolution8"\u000a  top: "Dropout7"\u000a  dropout_param {\u000a    dropout_ratio: 0.2\u000a  }\u000a}\u000alayer {\u000a  name: "Concat7"\u000a  type: "Concat"\u000a  bottom: "Concat6"\u000a  bottom: "Dropout7"\u000a  top: "Concat7"\u000a  concat_param {\u000a    axis: 1\u000a  }\u000a}\u000alayer {\u000a  name: "BatchNorm8"\u000a  type: "BatchNorm"\u000a  bottom: "Concat7"\u000a  top: "BatchNorm8"\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a}\u000alayer {\u000a  name: "Scale8"\u000a  type: "Scale"\u000a  bottom: "BatchNorm8"\u000a  top: "BatchNorm8"\u000a  scale_param {\u000a    filler {\u000a      value: 1\u000a    }\u000a    bias_term: true\u000a    bias_filler {\u000a      value: 0\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "ReLU8"\u000a  type: "ReLU"\u000a  bottom: "BatchNorm8"\u000a  top: "BatchNorm8"\u000a}\u000alayer {\u000a  name: "Convolution9"\u000a  type: "Convolution"\u000a  bottom: "BatchNorm8"\u000a  top: "Convolution9"\u000a  convolution_param {\u000a    num_output: 12\u000a    bias_term: false\u000a    pad: 1\u000a    kernel_size: 3\u000a    stride: 1\u000a    weight_filler {\u000a      type: "msra"\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "Dropout8"\u000a  type: "Dropout"\u000a  bottom: "Convolution9"\u000a  top: "Dropout8"\u000a  dropout_param {\u000a    dropout_ratio: 0.2\u000a  }\u000a}\u000alayer {\u000a  name: "Concat8"\u000a  type: "Concat"\u000a  bottom: "Concat7"\u000a  bottom: "Dropout8"\u000a  top: "Concat8"\u000a  concat_param {\u000a    axis: 1\u000a  }\u000a}\u000alayer {\u000a  name: "BatchNorm9"\u000a  type: "BatchNorm"\u000a  bottom: "Concat8"\u000a  top: "BatchNorm9"\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a}\u000alayer {\u000a  name: "Scale9"\u000a  type: "Scale"\u000a  bottom: "BatchNorm9"\u000a  top: "BatchNorm9"\u000a  scale_param {\u000a    filler {\u000a      value: 1\u000a    }\u000a    bias_term: true\u000a    bias_filler {\u000a      value: 0\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "ReLU9"\u000a  type: "ReLU"\u000a  bottom: "BatchNorm9"\u000a  top: "BatchNorm9"\u000a}\u000alayer {\u000a  name: "Convolution10"\u000a  type: "Convolution"\u000a  bottom: "BatchNorm9"\u000a  top: "Convolution10"\u000a  convolution_param {\u000a    num_output: 12\u000a    bias_term: false\u000a    pad: 1\u000a    kernel_size: 3\u000a    stride: 1\u000a    weight_filler {\u000a      type: "msra"\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "Dropout9"\u000a  type: "Dropout"\u000a  bottom: "Convolution10"\u000a  top: "Dropout9"\u000a  dropout_param {\u000a    dropout_ratio: 0.2\u000a  }\u000a}\u000alayer {\u000a  name: "Concat9"\u000a  type: "Concat"\u000a  bottom: "Concat8"\u000a  bottom: "Dropout9"\u000a  top: "Concat9"\u000a  concat_param {\u000a    axis: 1\u000a  }\u000a}\u000alayer {\u000a  name: "BatchNorm10"\u000a  type: "BatchNorm"\u000a  bottom: "Concat9"\u000a  top: "BatchNorm10"\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a}\u000alayer {\u000a  name: "Scale10"\u000a  type: "Scale"\u000a  bottom: "BatchNorm10"\u000a  top: "BatchNorm10"\u000a  scale_param {\u000a    filler {\u000a      value: 1\u000a    }\u000a    bias_term: true\u000a    bias_filler {\u000a      value: 0\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "ReLU10"\u000a  type: "ReLU"\u000a  bottom: "BatchNorm10"\u000a  top: "BatchNorm10"\u000a}\u000alayer {\u000a  name: "Convolution11"\u000a  type: "Convolution"\u000a  bottom: "BatchNorm10"\u000a  top: "Convolution11"\u000a  convolution_param {\u000a    num_output: 12\u000a    bias_term: false\u000a    pad: 1\u000a    kernel_size: 3\u000a    stride: 1\u000a    weight_filler {\u000a      type: "msra"\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "Dropout10"\u000a  type: "Dropout"\u000a  bottom: "Convolution11"\u000a  top: "Dropout10"\u000a  dropout_param {\u000a    dropout_ratio: 0.2\u000a  }\u000a}\u000alayer {\u000a  name: "Concat10"\u000a  type: "Concat"\u000a  bottom: "Concat9"\u000a  bottom: "Dropout10"\u000a  top: "Concat10"\u000a  concat_param {\u000a    axis: 1\u000a  }\u000a}\u000alayer {\u000a  name: "BatchNorm11"\u000a  type: "BatchNorm"\u000a  bottom: "Concat10"\u000a  top: "BatchNorm11"\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a}\u000alayer {\u000a  name: "Scale11"\u000a  type: "Scale"\u000a  bottom: "BatchNorm11"\u000a  top: "BatchNorm11"\u000a  scale_param {\u000a    filler {\u000a      value: 1\u000a    }\u000a    bias_term: true\u000a    bias_filler {\u000a      value: 0\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "ReLU11"\u000a  type: "ReLU"\u000a  bottom: "BatchNorm11"\u000a  top: "BatchNorm11"\u000a}\u000alayer {\u000a  name: "Convolution12"\u000a  type: "Convolution"\u000a  bottom: "BatchNorm11"\u000a  top: "Convolution12"\u000a  convolution_param {\u000a    num_output: 12\u000a    bias_term: false\u000a    pad: 1\u000a    kernel_size: 3\u000a    stride: 1\u000a    weight_filler {\u000a      type: "msra"\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "Dropout11"\u000a  type: "Dropout"\u000a  bottom: "Convolution12"\u000a  top: "Dropout11"\u000a  dropout_param {\u000a    dropout_ratio: 0.2\u000a  }\u000a}\u000alayer {\u000a  name: "Concat11"\u000a  type: "Concat"\u000a  bottom: "Concat10"\u000a  bottom: "Dropout11"\u000a  top: "Concat11"\u000a  concat_param {\u000a    axis: 1\u000a  }\u000a}\u000alayer {\u000a  name: "BatchNorm12"\u000a  type: "BatchNorm"\u000a  bottom: "Concat11"\u000a  top: "BatchNorm12"\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a}\u000alayer {\u000a  name: "Scale12"\u000a  type: "Scale"\u000a  bottom: "BatchNorm12"\u000a  top: "BatchNorm12"\u000a  scale_param {\u000a    filler {\u000a      value: 1\u000a    }\u000a    bias_term: true\u000a    bias_filler {\u000a      value: 0\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "ReLU12"\u000a  type: "ReLU"\u000a  bottom: "BatchNorm12"\u000a  top: "BatchNorm12"\u000a}\u000alayer {\u000a  name: "Convolution13"\u000a  type: "Convolution"\u000a  bottom: "BatchNorm12"\u000a  top: "Convolution13"\u000a  convolution_param {\u000a    num_output: 12\u000a    bias_term: false\u000a    pad: 1\u000a    kernel_size: 3\u000a    stride: 1\u000a    weight_filler {\u000a      type: "msra"\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "Dropout12"\u000a  type: "Dropout"\u000a  bottom: "Convolution13"\u000a  top: "Dropout12"\u000a  dropout_param {\u000a    dropout_ratio: 0.2\u000a  }\u000a}\u000alayer {\u000a  name: "Concat12"\u000a  type: "Concat"\u000a  bottom: "Concat11"\u000a  bottom: "Dropout12"\u000a  top: "Concat12"\u000a  concat_param {\u000a    axis: 1\u000a  }\u000a}\u000alayer {\u000a  name: "BatchNorm13"\u000a  type: "BatchNorm"\u000a  bottom: "Concat12"\u000a  top: "BatchNorm13"\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a}\u000alayer {\u000a  name: "Scale13"\u000a  type: "Scale"\u000a  bottom: "BatchNorm13"\u000a  top: "BatchNorm13"\u000a  scale_param {\u000a    filler {\u000a      value: 1\u000a    }\u000a    bias_term: true\u000a    bias_filler {\u000a      value: 0\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "ReLU13"\u000a  type: "ReLU"\u000a  bottom: "BatchNorm13"\u000a  top: "BatchNorm13"\u000a}\u000alayer {\u000a  name: "Convolution14"\u000a  type: "Convolution"\u000a  bottom: "BatchNorm13"\u000a  top: "Convolution14"\u000a  convolution_param {\u000a    num_output: 160\u000a    bias_term: false\u000a    pad: 0\u000a    kernel_size: 1\u000a    stride: 1\u000a    weight_filler {\u000a      type: "msra"\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "Dropout13"\u000a  type: "Dropout"\u000a  bottom: "Convolution14"\u000a  top: "Dropout13"\u000a  dropout_param {\u000a    dropout_ratio: 0.2\u000a  }\u000a}\u000alayer {\u000a  name: "Pooling1"\u000a  type: "Pooling"\u000a  bottom: "Dropout13"\u000a  top: "Pooling1"\u000a  pooling_param {\u000a    pool: AVE\u000a    kernel_size: 2\u000a    stride: 2\u000a  }\u000a}\u000alayer {\u000a  name: "BatchNorm14"\u000a  type: "BatchNorm"\u000a  bottom: "Pooling1"\u000a  top: "BatchNorm14"\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a}\u000alayer {\u000a  name: "Scale14"\u000a  type: "Scale"\u000a  bottom: "BatchNorm14"\u000a  top: "BatchNorm14"\u000a  scale_param {\u000a    filler {\u000a      value: 1\u000a    }\u000a    bias_term: true\u000a    bias_filler {\u000a      value: 0\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "ReLU14"\u000a  type: "ReLU"\u000a  bottom: "BatchNorm14"\u000a  top: "BatchNorm14"\u000a}\u000alayer {\u000a  name: "Convolution15"\u000a  type: "Convolution"\u000a  bottom: "BatchNorm14"\u000a  top: "Convolution15"\u000a  convolution_param {\u000a    num_output: 12\u000a    bias_term: false\u000a    pad: 1\u000a    kernel_size: 3\u000a    stride: 1\u000a    weight_filler {\u000a      type: "msra"\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "Dropout14"\u000a  type: "Dropout"\u000a  bottom: "Convolution15"\u000a  top: "Dropout14"\u000a  dropout_param {\u000a    dropout_ratio: 0.2\u000a  }\u000a}\u000alayer {\u000a  name: "Concat13"\u000a  type: "Concat"\u000a  bottom: "Pooling1"\u000a  bottom: "Dropout14"\u000a  top: "Concat13"\u000a  concat_param {\u000a    axis: 1\u000a  }\u000a}\u000alayer {\u000a  name: "BatchNorm15"\u000a  type: "BatchNorm"\u000a  bottom: "Concat13"\u000a  top: "BatchNorm15"\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a}\u000alayer {\u000a  name: "Scale15"\u000a  type: "Scale"\u000a  bottom: "BatchNorm15"\u000a  top: "BatchNorm15"\u000a  scale_param {\u000a    filler {\u000a      value: 1\u000a    }\u000a    bias_term: true\u000a    bias_filler {\u000a      value: 0\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "ReLU15"\u000a  type: "ReLU"\u000a  bottom: "BatchNorm15"\u000a  top: "BatchNorm15"\u000a}\u000alayer {\u000a  name: "Convolution16"\u000a  type: "Convolution"\u000a  bottom: "BatchNorm15"\u000a  top: "Convolution16"\u000a  convolution_param {\u000a    num_output: 12\u000a    bias_term: false\u000a    pad: 1\u000a    kernel_size: 3\u000a    stride: 1\u000a    weight_filler {\u000a      type: "msra"\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "Dropout15"\u000a  type: "Dropout"\u000a  bottom: "Convolution16"\u000a  top: "Dropout15"\u000a  dropout_param {\u000a    dropout_ratio: 0.2\u000a  }\u000a}\u000alayer {\u000a  name: "Concat14"\u000a  type: "Concat"\u000a  bottom: "Concat13"\u000a  bottom: "Dropout15"\u000a  top: "Concat14"\u000a  concat_param {\u000a    axis: 1\u000a  }\u000a}\u000alayer {\u000a  name: "BatchNorm16"\u000a  type: "BatchNorm"\u000a  bottom: "Concat14"\u000a  top: "BatchNorm16"\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a}\u000alayer {\u000a  name: "Scale16"\u000a  type: "Scale"\u000a  bottom: "BatchNorm16"\u000a  top: "BatchNorm16"\u000a  scale_param {\u000a    filler {\u000a      value: 1\u000a    }\u000a    bias_term: true\u000a    bias_filler {\u000a      value: 0\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "ReLU16"\u000a  type: "ReLU"\u000a  bottom: "BatchNorm16"\u000a  top: "BatchNorm16"\u000a}\u000alayer {\u000a  name: "Convolution17"\u000a  type: "Convolution"\u000a  bottom: "BatchNorm16"\u000a  top: "Convolution17"\u000a  convolution_param {\u000a    num_output: 12\u000a    bias_term: false\u000a    pad: 1\u000a    kernel_size: 3\u000a    stride: 1\u000a    weight_filler {\u000a      type: "msra"\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "Dropout16"\u000a  type: "Dropout"\u000a  bottom: "Convolution17"\u000a  top: "Dropout16"\u000a  dropout_param {\u000a    dropout_ratio: 0.2\u000a  }\u000a}\u000alayer {\u000a  name: "Concat15"\u000a  type: "Concat"\u000a  bottom: "Concat14"\u000a  bottom: "Dropout16"\u000a  top: "Concat15"\u000a  concat_param {\u000a    axis: 1\u000a  }\u000a}\u000alayer {\u000a  name: "BatchNorm17"\u000a  type: "BatchNorm"\u000a  bottom: "Concat15"\u000a  top: "BatchNorm17"\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a}\u000alayer {\u000a  name: "Scale17"\u000a  type: "Scale"\u000a  bottom: "BatchNorm17"\u000a  top: "BatchNorm17"\u000a  scale_param {\u000a    filler {\u000a      value: 1\u000a    }\u000a    bias_term: true\u000a    bias_filler {\u000a      value: 0\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "ReLU17"\u000a  type: "ReLU"\u000a  bottom: "BatchNorm17"\u000a  top: "BatchNorm17"\u000a}\u000alayer {\u000a  name: "Convolution18"\u000a  type: "Convolution"\u000a  bottom: "BatchNorm17"\u000a  top: "Convolution18"\u000a  convolution_param {\u000a    num_output: 12\u000a    bias_term: false\u000a    pad: 1\u000a    kernel_size: 3\u000a    stride: 1\u000a    weight_filler {\u000a      type: "msra"\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "Dropout17"\u000a  type: "Dropout"\u000a  bottom: "Convolution18"\u000a  top: "Dropout17"\u000a  dropout_param {\u000a    dropout_ratio: 0.2\u000a  }\u000a}\u000alayer {\u000a  name: "Concat16"\u000a  type: "Concat"\u000a  bottom: "Concat15"\u000a  bottom: "Dropout17"\u000a  top: "Concat16"\u000a  concat_param {\u000a    axis: 1\u000a  }\u000a}\u000alayer {\u000a  name: "BatchNorm18"\u000a  type: "BatchNorm"\u000a  bottom: "Concat16"\u000a  top: "BatchNorm18"\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a}\u000alayer {\u000a  name: "Scale18"\u000a  type: "Scale"\u000a  bottom: "BatchNorm18"\u000a  top: "BatchNorm18"\u000a  scale_param {\u000a    filler {\u000a      value: 1\u000a    }\u000a    bias_term: true\u000a    bias_filler {\u000a      value: 0\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "ReLU18"\u000a  type: "ReLU"\u000a  bottom: "BatchNorm18"\u000a  top: "BatchNorm18"\u000a}\u000alayer {\u000a  name: "Convolution19"\u000a  type: "Convolution"\u000a  bottom: "BatchNorm18"\u000a  top: "Convolution19"\u000a  convolution_param {\u000a    num_output: 12\u000a    bias_term: false\u000a    pad: 1\u000a    kernel_size: 3\u000a    stride: 1\u000a    weight_filler {\u000a      type: "msra"\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "Dropout18"\u000a  type: "Dropout"\u000a  bottom: "Convolution19"\u000a  top: "Dropout18"\u000a  dropout_param {\u000a    dropout_ratio: 0.2\u000a  }\u000a}\u000alayer {\u000a  name: "Concat17"\u000a  type: "Concat"\u000a  bottom: "Concat16"\u000a  bottom: "Dropout18"\u000a  top: "Concat17"\u000a  concat_param {\u000a    axis: 1\u000a  }\u000a}\u000alayer {\u000a  name: "BatchNorm19"\u000a  type: "BatchNorm"\u000a  bottom: "Concat17"\u000a  top: "BatchNorm19"\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a}\u000alayer {\u000a  name: "Scale19"\u000a  type: "Scale"\u000a  bottom: "BatchNorm19"\u000a  top: "BatchNorm19"\u000a  scale_param {\u000a    filler {\u000a      value: 1\u000a    }\u000a    bias_term: true\u000a    bias_filler {\u000a      value: 0\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "ReLU19"\u000a  type: "ReLU"\u000a  bottom: "BatchNorm19"\u000a  top: "BatchNorm19"\u000a}\u000alayer {\u000a  name: "Convolution20"\u000a  type: "Convolution"\u000a  bottom: "BatchNorm19"\u000a  top: "Convolution20"\u000a  convolution_param {\u000a    num_output: 12\u000a    bias_term: false\u000a    pad: 1\u000a    kernel_size: 3\u000a    stride: 1\u000a    weight_filler {\u000a      type: "msra"\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "Dropout19"\u000a  type: "Dropout"\u000a  bottom: "Convolution20"\u000a  top: "Dropout19"\u000a  dropout_param {\u000a    dropout_ratio: 0.2\u000a  }\u000a}\u000alayer {\u000a  name: "Concat18"\u000a  type: "Concat"\u000a  bottom: "Concat17"\u000a  bottom: "Dropout19"\u000a  top: "Concat18"\u000a  concat_param {\u000a    axis: 1\u000a  }\u000a}\u000alayer {\u000a  name: "BatchNorm20"\u000a  type: "BatchNorm"\u000a  bottom: "Concat18"\u000a  top: "BatchNorm20"\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a}\u000alayer {\u000a  name: "Scale20"\u000a  type: "Scale"\u000a  bottom: "BatchNorm20"\u000a  top: "BatchNorm20"\u000a  scale_param {\u000a    filler {\u000a      value: 1\u000a    }\u000a    bias_term: true\u000a    bias_filler {\u000a      value: 0\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "ReLU20"\u000a  type: "ReLU"\u000a  bottom: "BatchNorm20"\u000a  top: "BatchNorm20"\u000a}\u000alayer {\u000a  name: "Convolution21"\u000a  type: "Convolution"\u000a  bottom: "BatchNorm20"\u000a  top: "Convolution21"\u000a  convolution_param {\u000a    num_output: 12\u000a    bias_term: false\u000a    pad: 1\u000a    kernel_size: 3\u000a    stride: 1\u000a    weight_filler {\u000a      type: "msra"\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "Dropout20"\u000a  type: "Dropout"\u000a  bottom: "Convolution21"\u000a  top: "Dropout20"\u000a  dropout_param {\u000a    dropout_ratio: 0.2\u000a  }\u000a}\u000alayer {\u000a  name: "Concat19"\u000a  type: "Concat"\u000a  bottom: "Concat18"\u000a  bottom: "Dropout20"\u000a  top: "Concat19"\u000a  concat_param {\u000a    axis: 1\u000a  }\u000a}\u000alayer {\u000a  name: "BatchNorm21"\u000a  type: "BatchNorm"\u000a  bottom: "Concat19"\u000a  top: "BatchNorm21"\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a}\u000alayer {\u000a  name: "Scale21"\u000a  type: "Scale"\u000a  bottom: "BatchNorm21"\u000a  top: "BatchNorm21"\u000a  scale_param {\u000a    filler {\u000a      value: 1\u000a    }\u000a    bias_term: true\u000a    bias_filler {\u000a      value: 0\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "ReLU21"\u000a  type: "ReLU"\u000a  bottom: "BatchNorm21"\u000a  top: "BatchNorm21"\u000a}\u000alayer {\u000a  name: "Convolution22"\u000a  type: "Convolution"\u000a  bottom: "BatchNorm21"\u000a  top: "Convolution22"\u000a  convolution_param {\u000a    num_output: 12\u000a    bias_term: false\u000a    pad: 1\u000a    kernel_size: 3\u000a    stride: 1\u000a    weight_filler {\u000a      type: "msra"\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "Dropout21"\u000a  type: "Dropout"\u000a  bottom: "Convolution22"\u000a  top: "Dropout21"\u000a  dropout_param {\u000a    dropout_ratio: 0.2\u000a  }\u000a}\u000alayer {\u000a  name: "Concat20"\u000a  type: "Concat"\u000a  bottom: "Concat19"\u000a  bottom: "Dropout21"\u000a  top: "Concat20"\u000a  concat_param {\u000a    axis: 1\u000a  }\u000a}\u000alayer {\u000a  name: "BatchNorm22"\u000a  type: "BatchNorm"\u000a  bottom: "Concat20"\u000a  top: "BatchNorm22"\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a}\u000alayer {\u000a  name: "Scale22"\u000a  type: "Scale"\u000a  bottom: "BatchNorm22"\u000a  top: "BatchNorm22"\u000a  scale_param {\u000a    filler {\u000a      value: 1\u000a    }\u000a    bias_term: true\u000a    bias_filler {\u000a      value: 0\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "ReLU22"\u000a  type: "ReLU"\u000a  bottom: "BatchNorm22"\u000a  top: "BatchNorm22"\u000a}\u000alayer {\u000a  name: "Convolution23"\u000a  type: "Convolution"\u000a  bottom: "BatchNorm22"\u000a  top: "Convolution23"\u000a  convolution_param {\u000a    num_output: 12\u000a    bias_term: false\u000a    pad: 1\u000a    kernel_size: 3\u000a    stride: 1\u000a    weight_filler {\u000a      type: "msra"\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "Dropout22"\u000a  type: "Dropout"\u000a  bottom: "Convolution23"\u000a  top: "Dropout22"\u000a  dropout_param {\u000a    dropout_ratio: 0.2\u000a  }\u000a}\u000alayer {\u000a  name: "Concat21"\u000a  type: "Concat"\u000a  bottom: "Concat20"\u000a  bottom: "Dropout22"\u000a  top: "Concat21"\u000a  concat_param {\u000a    axis: 1\u000a  }\u000a}\u000alayer {\u000a  name: "BatchNorm23"\u000a  type: "BatchNorm"\u000a  bottom: "Concat21"\u000a  top: "BatchNorm23"\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a}\u000alayer {\u000a  name: "Scale23"\u000a  type: "Scale"\u000a  bottom: "BatchNorm23"\u000a  top: "BatchNorm23"\u000a  scale_param {\u000a    filler {\u000a      value: 1\u000a    }\u000a    bias_term: true\u000a    bias_filler {\u000a      value: 0\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "ReLU23"\u000a  type: "ReLU"\u000a  bottom: "BatchNorm23"\u000a  top: "BatchNorm23"\u000a}\u000alayer {\u000a  name: "Convolution24"\u000a  type: "Convolution"\u000a  bottom: "BatchNorm23"\u000a  top: "Convolution24"\u000a  convolution_param {\u000a    num_output: 12\u000a    bias_term: false\u000a    pad: 1\u000a    kernel_size: 3\u000a    stride: 1\u000a    weight_filler {\u000a      type: "msra"\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "Dropout23"\u000a  type: "Dropout"\u000a  bottom: "Convolution24"\u000a  top: "Dropout23"\u000a  dropout_param {\u000a    dropout_ratio: 0.2\u000a  }\u000a}\u000alayer {\u000a  name: "Concat22"\u000a  type: "Concat"\u000a  bottom: "Concat21"\u000a  bottom: "Dropout23"\u000a  top: "Concat22"\u000a  concat_param {\u000a    axis: 1\u000a  }\u000a}\u000alayer {\u000a  name: "BatchNorm24"\u000a  type: "BatchNorm"\u000a  bottom: "Concat22"\u000a  top: "BatchNorm24"\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a}\u000alayer {\u000a  name: "Scale24"\u000a  type: "Scale"\u000a  bottom: "BatchNorm24"\u000a  top: "BatchNorm24"\u000a  scale_param {\u000a    filler {\u000a      value: 1\u000a    }\u000a    bias_term: true\u000a    bias_filler {\u000a      value: 0\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "ReLU24"\u000a  type: "ReLU"\u000a  bottom: "BatchNorm24"\u000a  top: "BatchNorm24"\u000a}\u000alayer {\u000a  name: "Convolution25"\u000a  type: "Convolution"\u000a  bottom: "BatchNorm24"\u000a  top: "Convolution25"\u000a  convolution_param {\u000a    num_output: 12\u000a    bias_term: false\u000a    pad: 1\u000a    kernel_size: 3\u000a    stride: 1\u000a    weight_filler {\u000a      type: "msra"\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "Dropout24"\u000a  type: "Dropout"\u000a  bottom: "Convolution25"\u000a  top: "Dropout24"\u000a  dropout_param {\u000a    dropout_ratio: 0.2\u000a  }\u000a}\u000alayer {\u000a  name: "Concat23"\u000a  type: "Concat"\u000a  bottom: "Concat22"\u000a  bottom: "Dropout24"\u000a  top: "Concat23"\u000a  concat_param {\u000a    axis: 1\u000a  }\u000a}\u000alayer {\u000a  name: "BatchNorm25"\u000a  type: "BatchNorm"\u000a  bottom: "Concat23"\u000a  top: "BatchNorm25"\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a}\u000alayer {\u000a  name: "Scale25"\u000a  type: "Scale"\u000a  bottom: "BatchNorm25"\u000a  top: "BatchNorm25"\u000a  scale_param {\u000a    filler {\u000a      value: 1\u000a    }\u000a    bias_term: true\u000a    bias_filler {\u000a      value: 0\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "ReLU25"\u000a  type: "ReLU"\u000a  bottom: "BatchNorm25"\u000a  top: "BatchNorm25"\u000a}\u000alayer {\u000a  name: "Convolution26"\u000a  type: "Convolution"\u000a  bottom: "BatchNorm25"\u000a  top: "Convolution26"\u000a  convolution_param {\u000a    num_output: 12\u000a    bias_term: false\u000a    pad: 1\u000a    kernel_size: 3\u000a    stride: 1\u000a    weight_filler {\u000a      type: "msra"\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "Dropout25"\u000a  type: "Dropout"\u000a  bottom: "Convolution26"\u000a  top: "Dropout25"\u000a  dropout_param {\u000a    dropout_ratio: 0.2\u000a  }\u000a}\u000alayer {\u000a  name: "Concat24"\u000a  type: "Concat"\u000a  bottom: "Concat23"\u000a  bottom: "Dropout25"\u000a  top: "Concat24"\u000a  concat_param {\u000a    axis: 1\u000a  }\u000a}\u000alayer {\u000a  name: "BatchNorm26"\u000a  type: "BatchNorm"\u000a  bottom: "Concat24"\u000a  top: "BatchNorm26"\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a}\u000alayer {\u000a  name: "Scale26"\u000a  type: "Scale"\u000a  bottom: "BatchNorm26"\u000a  top: "BatchNorm26"\u000a  scale_param {\u000a    filler {\u000a      value: 1\u000a    }\u000a    bias_term: true\u000a    bias_filler {\u000a      value: 0\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "ReLU26"\u000a  type: "ReLU"\u000a  bottom: "BatchNorm26"\u000a  top: "BatchNorm26"\u000a}\u000alayer {\u000a  name: "Convolution27"\u000a  type: "Convolution"\u000a  bottom: "BatchNorm26"\u000a  top: "Convolution27"\u000a  convolution_param {\u000a    num_output: 304\u000a    bias_term: false\u000a    pad: 0\u000a    kernel_size: 1\u000a    stride: 1\u000a    weight_filler {\u000a      type: "msra"\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "Dropout26"\u000a  type: "Dropout"\u000a  bottom: "Convolution27"\u000a  top: "Dropout26"\u000a  dropout_param {\u000a    dropout_ratio: 0.2\u000a  }\u000a}\u000alayer {\u000a  name: "Pooling2"\u000a  type: "Pooling"\u000a  bottom: "Dropout26"\u000a  top: "Pooling2"\u000a  pooling_param {\u000a    pool: AVE\u000a    kernel_size: 2\u000a    stride: 2\u000a  }\u000a}\u000alayer {\u000a  name: "BatchNorm27"\u000a  type: "BatchNorm"\u000a  bottom: "Pooling2"\u000a  top: "BatchNorm27"\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a}\u000alayer {\u000a  name: "Scale27"\u000a  type: "Scale"\u000a  bottom: "BatchNorm27"\u000a  top: "BatchNorm27"\u000a  scale_param {\u000a    filler {\u000a      value: 1\u000a    }\u000a    bias_term: true\u000a    bias_filler {\u000a      value: 0\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "ReLU27"\u000a  type: "ReLU"\u000a  bottom: "BatchNorm27"\u000a  top: "BatchNorm27"\u000a}\u000alayer {\u000a  name: "Convolution28"\u000a  type: "Convolution"\u000a  bottom: "BatchNorm27"\u000a  top: "Convolution28"\u000a  convolution_param {\u000a    num_output: 12\u000a    bias_term: false\u000a    pad: 1\u000a    kernel_size: 3\u000a    stride: 1\u000a    weight_filler {\u000a      type: "msra"\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "Dropout27"\u000a  type: "Dropout"\u000a  bottom: "Convolution28"\u000a  top: "Dropout27"\u000a  dropout_param {\u000a    dropout_ratio: 0.2\u000a  }\u000a}\u000alayer {\u000a  name: "Concat25"\u000a  type: "Concat"\u000a  bottom: "Pooling2"\u000a  bottom: "Dropout27"\u000a  top: "Concat25"\u000a  concat_param {\u000a    axis: 1\u000a  }\u000a}\u000alayer {\u000a  name: "BatchNorm28"\u000a  type: "BatchNorm"\u000a  bottom: "Concat25"\u000a  top: "BatchNorm28"\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a}\u000alayer {\u000a  name: "Scale28"\u000a  type: "Scale"\u000a  bottom: "BatchNorm28"\u000a  top: "BatchNorm28"\u000a  scale_param {\u000a    filler {\u000a      value: 1\u000a    }\u000a    bias_term: true\u000a    bias_filler {\u000a      value: 0\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "ReLU28"\u000a  type: "ReLU"\u000a  bottom: "BatchNorm28"\u000a  top: "BatchNorm28"\u000a}\u000alayer {\u000a  name: "Convolution29"\u000a  type: "Convolution"\u000a  bottom: "BatchNorm28"\u000a  top: "Convolution29"\u000a  convolution_param {\u000a    num_output: 12\u000a    bias_term: false\u000a    pad: 1\u000a    kernel_size: 3\u000a    stride: 1\u000a    weight_filler {\u000a      type: "msra"\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "Dropout28"\u000a  type: "Dropout"\u000a  bottom: "Convolution29"\u000a  top: "Dropout28"\u000a  dropout_param {\u000a    dropout_ratio: 0.2\u000a  }\u000a}\u000alayer {\u000a  name: "Concat26"\u000a  type: "Concat"\u000a  bottom: "Concat25"\u000a  bottom: "Dropout28"\u000a  top: "Concat26"\u000a  concat_param {\u000a    axis: 1\u000a  }\u000a}\u000alayer {\u000a  name: "BatchNorm29"\u000a  type: "BatchNorm"\u000a  bottom: "Concat26"\u000a  top: "BatchNorm29"\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a}\u000alayer {\u000a  name: "Scale29"\u000a  type: "Scale"\u000a  bottom: "BatchNorm29"\u000a  top: "BatchNorm29"\u000a  scale_param {\u000a    filler {\u000a      value: 1\u000a    }\u000a    bias_term: true\u000a    bias_filler {\u000a      value: 0\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "ReLU29"\u000a  type: "ReLU"\u000a  bottom: "BatchNorm29"\u000a  top: "BatchNorm29"\u000a}\u000alayer {\u000a  name: "Convolution30"\u000a  type: "Convolution"\u000a  bottom: "BatchNorm29"\u000a  top: "Convolution30"\u000a  convolution_param {\u000a    num_output: 12\u000a    bias_term: false\u000a    pad: 1\u000a    kernel_size: 3\u000a    stride: 1\u000a    weight_filler {\u000a      type: "msra"\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "Dropout29"\u000a  type: "Dropout"\u000a  bottom: "Convolution30"\u000a  top: "Dropout29"\u000a  dropout_param {\u000a    dropout_ratio: 0.2\u000a  }\u000a}\u000alayer {\u000a  name: "Concat27"\u000a  type: "Concat"\u000a  bottom: "Concat26"\u000a  bottom: "Dropout29"\u000a  top: "Concat27"\u000a  concat_param {\u000a    axis: 1\u000a  }\u000a}\u000alayer {\u000a  name: "BatchNorm30"\u000a  type: "BatchNorm"\u000a  bottom: "Concat27"\u000a  top: "BatchNorm30"\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a}\u000alayer {\u000a  name: "Scale30"\u000a  type: "Scale"\u000a  bottom: "BatchNorm30"\u000a  top: "BatchNorm30"\u000a  scale_param {\u000a    filler {\u000a      value: 1\u000a    }\u000a    bias_term: true\u000a    bias_filler {\u000a      value: 0\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "ReLU30"\u000a  type: "ReLU"\u000a  bottom: "BatchNorm30"\u000a  top: "BatchNorm30"\u000a}\u000alayer {\u000a  name: "Convolution31"\u000a  type: "Convolution"\u000a  bottom: "BatchNorm30"\u000a  top: "Convolution31"\u000a  convolution_param {\u000a    num_output: 12\u000a    bias_term: false\u000a    pad: 1\u000a    kernel_size: 3\u000a    stride: 1\u000a    weight_filler {\u000a      type: "msra"\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "Dropout30"\u000a  type: "Dropout"\u000a  bottom: "Convolution31"\u000a  top: "Dropout30"\u000a  dropout_param {\u000a    dropout_ratio: 0.2\u000a  }\u000a}\u000alayer {\u000a  name: "Concat28"\u000a  type: "Concat"\u000a  bottom: "Concat27"\u000a  bottom: "Dropout30"\u000a  top: "Concat28"\u000a  concat_param {\u000a    axis: 1\u000a  }\u000a}\u000alayer {\u000a  name: "BatchNorm31"\u000a  type: "BatchNorm"\u000a  bottom: "Concat28"\u000a  top: "BatchNorm31"\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a}\u000alayer {\u000a  name: "Scale31"\u000a  type: "Scale"\u000a  bottom: "BatchNorm31"\u000a  top: "BatchNorm31"\u000a  scale_param {\u000a    filler {\u000a      value: 1\u000a    }\u000a    bias_term: true\u000a    bias_filler {\u000a      value: 0\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "ReLU31"\u000a  type: "ReLU"\u000a  bottom: "BatchNorm31"\u000a  top: "BatchNorm31"\u000a}\u000alayer {\u000a  name: "Convolution32"\u000a  type: "Convolution"\u000a  bottom: "BatchNorm31"\u000a  top: "Convolution32"\u000a  convolution_param {\u000a    num_output: 12\u000a    bias_term: false\u000a    pad: 1\u000a    kernel_size: 3\u000a    stride: 1\u000a    weight_filler {\u000a      type: "msra"\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "Dropout31"\u000a  type: "Dropout"\u000a  bottom: "Convolution32"\u000a  top: "Dropout31"\u000a  dropout_param {\u000a    dropout_ratio: 0.2\u000a  }\u000a}\u000alayer {\u000a  name: "Concat29"\u000a  type: "Concat"\u000a  bottom: "Concat28"\u000a  bottom: "Dropout31"\u000a  top: "Concat29"\u000a  concat_param {\u000a    axis: 1\u000a  }\u000a}\u000alayer {\u000a  name: "BatchNorm32"\u000a  type: "BatchNorm"\u000a  bottom: "Concat29"\u000a  top: "BatchNorm32"\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a}\u000alayer {\u000a  name: "Scale32"\u000a  type: "Scale"\u000a  bottom: "BatchNorm32"\u000a  top: "BatchNorm32"\u000a  scale_param {\u000a    filler {\u000a      value: 1\u000a    }\u000a    bias_term: true\u000a    bias_filler {\u000a      value: 0\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "ReLU32"\u000a  type: "ReLU"\u000a  bottom: "BatchNorm32"\u000a  top: "BatchNorm32"\u000a}\u000alayer {\u000a  name: "Convolution33"\u000a  type: "Convolution"\u000a  bottom: "BatchNorm32"\u000a  top: "Convolution33"\u000a  convolution_param {\u000a    num_output: 12\u000a    bias_term: false\u000a    pad: 1\u000a    kernel_size: 3\u000a    stride: 1\u000a    weight_filler {\u000a      type: "msra"\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "Dropout32"\u000a  type: "Dropout"\u000a  bottom: "Convolution33"\u000a  top: "Dropout32"\u000a  dropout_param {\u000a    dropout_ratio: 0.2\u000a  }\u000a}\u000alayer {\u000a  name: "Concat30"\u000a  type: "Concat"\u000a  bottom: "Concat29"\u000a  bottom: "Dropout32"\u000a  top: "Concat30"\u000a  concat_param {\u000a    axis: 1\u000a  }\u000a}\u000alayer {\u000a  name: "BatchNorm33"\u000a  type: "BatchNorm"\u000a  bottom: "Concat30"\u000a  top: "BatchNorm33"\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a}\u000alayer {\u000a  name: "Scale33"\u000a  type: "Scale"\u000a  bottom: "BatchNorm33"\u000a  top: "BatchNorm33"\u000a  scale_param {\u000a    filler {\u000a      value: 1\u000a    }\u000a    bias_term: true\u000a    bias_filler {\u000a      value: 0\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "ReLU33"\u000a  type: "ReLU"\u000a  bottom: "BatchNorm33"\u000a  top: "BatchNorm33"\u000a}\u000alayer {\u000a  name: "Convolution34"\u000a  type: "Convolution"\u000a  bottom: "BatchNorm33"\u000a  top: "Convolution34"\u000a  convolution_param {\u000a    num_output: 12\u000a    bias_term: false\u000a    pad: 1\u000a    kernel_size: 3\u000a    stride: 1\u000a    weight_filler {\u000a      type: "msra"\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "Dropout33"\u000a  type: "Dropout"\u000a  bottom: "Convolution34"\u000a  top: "Dropout33"\u000a  dropout_param {\u000a    dropout_ratio: 0.2\u000a  }\u000a}\u000alayer {\u000a  name: "Concat31"\u000a  type: "Concat"\u000a  bottom: "Concat30"\u000a  bottom: "Dropout33"\u000a  top: "Concat31"\u000a  concat_param {\u000a    axis: 1\u000a  }\u000a}\u000alayer {\u000a  name: "BatchNorm34"\u000a  type: "BatchNorm"\u000a  bottom: "Concat31"\u000a  top: "BatchNorm34"\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a}\u000alayer {\u000a  name: "Scale34"\u000a  type: "Scale"\u000a  bottom: "BatchNorm34"\u000a  top: "BatchNorm34"\u000a  scale_param {\u000a    filler {\u000a      value: 1\u000a    }\u000a    bias_term: true\u000a    bias_filler {\u000a      value: 0\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "ReLU34"\u000a  type: "ReLU"\u000a  bottom: "BatchNorm34"\u000a  top: "BatchNorm34"\u000a}\u000alayer {\u000a  name: "Convolution35"\u000a  type: "Convolution"\u000a  bottom: "BatchNorm34"\u000a  top: "Convolution35"\u000a  convolution_param {\u000a    num_output: 12\u000a    bias_term: false\u000a    pad: 1\u000a    kernel_size: 3\u000a    stride: 1\u000a    weight_filler {\u000a      type: "msra"\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "Dropout34"\u000a  type: "Dropout"\u000a  bottom: "Convolution35"\u000a  top: "Dropout34"\u000a  dropout_param {\u000a    dropout_ratio: 0.2\u000a  }\u000a}\u000alayer {\u000a  name: "Concat32"\u000a  type: "Concat"\u000a  bottom: "Concat31"\u000a  bottom: "Dropout34"\u000a  top: "Concat32"\u000a  concat_param {\u000a    axis: 1\u000a  }\u000a}\u000alayer {\u000a  name: "BatchNorm35"\u000a  type: "BatchNorm"\u000a  bottom: "Concat32"\u000a  top: "BatchNorm35"\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a}\u000alayer {\u000a  name: "Scale35"\u000a  type: "Scale"\u000a  bottom: "BatchNorm35"\u000a  top: "BatchNorm35"\u000a  scale_param {\u000a    filler {\u000a      value: 1\u000a    }\u000a    bias_term: true\u000a    bias_filler {\u000a      value: 0\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "ReLU35"\u000a  type: "ReLU"\u000a  bottom: "BatchNorm35"\u000a  top: "BatchNorm35"\u000a}\u000alayer {\u000a  name: "Convolution36"\u000a  type: "Convolution"\u000a  bottom: "BatchNorm35"\u000a  top: "Convolution36"\u000a  convolution_param {\u000a    num_output: 12\u000a    bias_term: false\u000a    pad: 1\u000a    kernel_size: 3\u000a    stride: 1\u000a    weight_filler {\u000a      type: "msra"\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "Dropout35"\u000a  type: "Dropout"\u000a  bottom: "Convolution36"\u000a  top: "Dropout35"\u000a  dropout_param {\u000a    dropout_ratio: 0.2\u000a  }\u000a}\u000alayer {\u000a  name: "Concat33"\u000a  type: "Concat"\u000a  bottom: "Concat32"\u000a  bottom: "Dropout35"\u000a  top: "Concat33"\u000a  concat_param {\u000a    axis: 1\u000a  }\u000a}\u000alayer {\u000a  name: "BatchNorm36"\u000a  type: "BatchNorm"\u000a  bottom: "Concat33"\u000a  top: "BatchNorm36"\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a}\u000alayer {\u000a  name: "Scale36"\u000a  type: "Scale"\u000a  bottom: "BatchNorm36"\u000a  top: "BatchNorm36"\u000a  scale_param {\u000a    filler {\u000a      value: 1\u000a    }\u000a    bias_term: true\u000a    bias_filler {\u000a      value: 0\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "ReLU36"\u000a  type: "ReLU"\u000a  bottom: "BatchNorm36"\u000a  top: "BatchNorm36"\u000a}\u000alayer {\u000a  name: "Convolution37"\u000a  type: "Convolution"\u000a  bottom: "BatchNorm36"\u000a  top: "Convolution37"\u000a  convolution_param {\u000a    num_output: 12\u000a    bias_term: false\u000a    pad: 1\u000a    kernel_size: 3\u000a    stride: 1\u000a    weight_filler {\u000a      type: "msra"\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "Dropout36"\u000a  type: "Dropout"\u000a  bottom: "Convolution37"\u000a  top: "Dropout36"\u000a  dropout_param {\u000a    dropout_ratio: 0.2\u000a  }\u000a}\u000alayer {\u000a  name: "Concat34"\u000a  type: "Concat"\u000a  bottom: "Concat33"\u000a  bottom: "Dropout36"\u000a  top: "Concat34"\u000a  concat_param {\u000a    axis: 1\u000a  }\u000a}\u000alayer {\u000a  name: "BatchNorm37"\u000a  type: "BatchNorm"\u000a  bottom: "Concat34"\u000a  top: "BatchNorm37"\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a}\u000alayer {\u000a  name: "Scale37"\u000a  type: "Scale"\u000a  bottom: "BatchNorm37"\u000a  top: "BatchNorm37"\u000a  scale_param {\u000a    filler {\u000a      value: 1\u000a    }\u000a    bias_term: true\u000a    bias_filler {\u000a      value: 0\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "ReLU37"\u000a  type: "ReLU"\u000a  bottom: "BatchNorm37"\u000a  top: "BatchNorm37"\u000a}\u000alayer {\u000a  name: "Convolution38"\u000a  type: "Convolution"\u000a  bottom: "BatchNorm37"\u000a  top: "Convolution38"\u000a  convolution_param {\u000a    num_output: 12\u000a    bias_term: false\u000a    pad: 1\u000a    kernel_size: 3\u000a    stride: 1\u000a    weight_filler {\u000a      type: "msra"\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "Dropout37"\u000a  type: "Dropout"\u000a  bottom: "Convolution38"\u000a  top: "Dropout37"\u000a  dropout_param {\u000a    dropout_ratio: 0.2\u000a  }\u000a}\u000alayer {\u000a  name: "Concat35"\u000a  type: "Concat"\u000a  bottom: "Concat34"\u000a  bottom: "Dropout37"\u000a  top: "Concat35"\u000a  concat_param {\u000a    axis: 1\u000a  }\u000a}\u000alayer {\u000a  name: "BatchNorm38"\u000a  type: "BatchNorm"\u000a  bottom: "Concat35"\u000a  top: "BatchNorm38"\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a}\u000alayer {\u000a  name: "Scale38"\u000a  type: "Scale"\u000a  bottom: "BatchNorm38"\u000a  top: "BatchNorm38"\u000a  scale_param {\u000a    filler {\u000a      value: 1\u000a    }\u000a    bias_term: true\u000a    bias_filler {\u000a      value: 0\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "ReLU38"\u000a  type: "ReLU"\u000a  bottom: "BatchNorm38"\u000a  top: "BatchNorm38"\u000a}\u000alayer {\u000a  name: "Convolution39"\u000a  type: "Convolution"\u000a  bottom: "BatchNorm38"\u000a  top: "Convolution39"\u000a  convolution_param {\u000a    num_output: 12\u000a    bias_term: false\u000a    pad: 1\u000a    kernel_size: 3\u000a    stride: 1\u000a    weight_filler {\u000a      type: "msra"\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "Dropout38"\u000a  type: "Dropout"\u000a  bottom: "Convolution39"\u000a  top: "Dropout38"\u000a  dropout_param {\u000a    dropout_ratio: 0.2\u000a  }\u000a}\u000alayer {\u000a  name: "Concat36"\u000a  type: "Concat"\u000a  bottom: "Concat35"\u000a  bottom: "Dropout38"\u000a  top: "Concat36"\u000a  concat_param {\u000a    axis: 1\u000a  }\u000a}\u000alayer {\u000a  name: "BatchNorm39"\u000a  type: "BatchNorm"\u000a  bottom: "Concat36"\u000a  top: "BatchNorm39"\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a  param {\u000a    lr_mult: 0\u000a    decay_mult: 0\u000a  }\u000a}\u000alayer {\u000a  name: "Scale39"\u000a  type: "Scale"\u000a  bottom: "BatchNorm39"\u000a  top: "BatchNorm39"\u000a  scale_param {\u000a    filler {\u000a      value: 1\u000a    }\u000a    bias_term: true\u000a    bias_filler {\u000a      value: 0\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "ReLU39"\u000a  type: "ReLU"\u000a  bottom: "BatchNorm39"\u000a  top: "BatchNorm39"\u000a}\u000alayer {\u000a  name: "Pooling3"\u000a  type: "Pooling"\u000a  bottom: "BatchNorm39"\u000a  top: "Pooling3"\u000a  pooling_param {\u000a    pool: AVE\u000a    global_pooling: true\u000a  }\u000a}\u000alayer {\u000a  name: "InnerProduct1"\u000a  type: "InnerProduct"\u000a  bottom: "Pooling3"\u000a  top: "InnerProduct1"\u000a  inner_product_param {\u000a    num_output: 4\u000a    bias_term: true\u000a    weight_filler {\u000a      type: "xavier"\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "loss"\u000a  type: "SoftmaxWithLoss"\u000a  bottom: "InnerProduct1"\u000a  bottom: "label"\u000a  top: "loss"\u000a  loss_weight: 1\u000a  exclude { stage: "deploy" }\u000a}\u000alayer {\u000a  name: "Accuracy"\u000a  type: "Accuracy"\u000a  bottom: "InnerProduct1"\u000a  bottom: "label"\u000a  top: "accuracy"\u000a  include { stage: "val" }\u000a  include { stage: "train" }\u000a}\u000alayer {\u000a  name: "softmax"\u000a  type: "Softmax"\u000a  bottom: "InnerProduct1"\u000a  top: "softmax"\u000a  include { stage: "deploy" }\u000a}
p225
sS'form.group_name.data'
p226
g179
sS'form.use_mean.data'
p227
g93
sS'form.lr_inv_gamma.data'
p228
F0.1
sS'form.lr_poly_power.data'
p229
F3.0
sS'form.snapshot_interval.data'
p230
F1.0
sS'form.lr_step_size.data'
p231
F33.0
sS'form.aug_hsv_use.data'
p232
I00
sS'form.aug_hsv_h.data'
p233
F0.02
sS'form.custom_network_snapshot.data'
p234
g79
sS'form.lr_step_gamma.data'
p235
F0.1
sS'form.lr_multistep_gamma.data'
p236
F0.5
sS'form.select_gpu.data'
p237
Vnext
p238
sS'form.lr_exp_gamma.data'
p239
F0.95
sS'form.lr_inv_power.data'
p240
F0.5
sS'form.aug_rot.data'
p241
I0
sS'form.train_epochs.data'
p242
I30
sS'form.batch_size.data'
p243
(lp244
I8
assg167
F1.0
sS'_id'
p245
S'20180308-215226-c8ec'
p246
sS'pickver_job_dataset'
p247
I1
sg150
(lp248
((idigits.status
Status
p249
g154
bF1520545946.682174
tp250
a((idigits.status
Status
p251
g157
bF1520545947.924147
tp252
a((idigits.status
Status
p253
g160
bF1520582954.204783
tp254
asS'pickver_job_model_image_classification'
p255
I1
sb.